[
  {
    "objectID": "docs/lesson05-binning-functional-annotation/02-binning_quality.html",
    "href": "docs/lesson05-binning-functional-annotation/02-binning_quality.html",
    "title": "QC of metagenome bins",
    "section": "",
    "text": "The quality of a metagenome-assembled genome (MAG) or bin is highly dependent on several things:\n\nthe depth of sequencing\nthe abundance of the organism in the community\nhow successful the assembly was\nhow successful the polishing (if used) was\n\nIn order to determine the quality of a MAG we can look at two different metrics. These are:\n\ncompleteness (i.e. how much of the genome is captured in the MAG?) and\ncontamination (i.e. do all the sequences in the MAG belong to the same organism?).\n\nWe can use the program CheckM to determine the quality of MAGs. CheckM uses a collection of domain and lineage-specific markers to estimate completeness and contamination of a MAG. This short YouTube video by Dr Robert Edwards explains how CheckM uses a hidden Markov model to calculate the level of contamination and completeness of bins, based on marker gene sets.\nCheckM has multiple different workflows available which are appropriate for different datasets. See CheckM documentation on Workflows for more information.\nWe will be using the lineage-specific workflow here. lineage_wf places your bins in a reference tree to determine which lineage it corresponds to. This allows it to use the appropriate marker genes to estimate quality parameters.\nFirst let’s move into our cs_course folder and make a directory for our CheckM results.\n\n\nCode\n\ncd ~/cs_course\nmkdir results/checkm\n\nCheckM has been pre-installed on the instance so we can check the help documentation for the lineage-specific workflow using the -h flag/option.\n\n\nCode\n\ncheckm lineage_wf -h\n\n\n\n\n\n\n\nOutput — CheckM help documentation\n\n\n\n\n\nusage: checkm lineage_wf [-h] [-r] [--ali] [--nt] [-g] [-u UNIQUE] [-m MULTI]\n                         [--force_domain] [--no_refinement]\n                         [--individual_markers] [--skip_adj_correction]\n                         [--skip_pseudogene_correction]\n                         [--aai_strain AAI_STRAIN] [-a ALIGNMENT_FILE]\n                         [--ignore_thresholds] [-e E_VALUE] [-l LENGTH]\n                         [-f FILE] [--tab_table] [-x EXTENSION] [-t THREADS]\n                         [--pplacer_threads PPLACER_THREADS] [-q]\n                         [--tmpdir TMPDIR]\n                         bin_input output_dir\n\nRuns tree, lineage_set, analyze, qa\n\npositional arguments:\n  bin_input             directory containing bins (fasta format) or path to file describing genomes/genes - tab separated in 2 or 3 columns [genome ID, genome fna, genome translation file (pep)]\n  output_dir            directory to write output files\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -r, --reduced_tree    use reduced tree (requires &lt;16GB of memory) for determining lineage of each bin\n  --ali                 generate HMMER alignment file for each bin\n  --nt                  generate nucleotide gene sequences for each bin\n  -g, --genes           bins contain genes as amino acids instead of nucleotide contigs\n  -u, --unique UNIQUE   minimum number of unique phylogenetic markers required to use lineage-specific marker set (default: 10)\n  -m, --multi MULTI     maximum number of multi-copy phylogenetic markers before defaulting to domain-level marker set (default: 10)\n  --force_domain        use domain-level sets for all bins\n  --no_refinement       do not perform lineage-specific marker set refinement\n  --individual_markers  treat marker as independent (i.e., ignore co-located set structure)\n  --skip_adj_correction\n                        do not exclude adjacent marker genes when estimating contamination\n  --skip_pseudogene_correction\n                        skip identification and filtering of pseudogenes\n  --aai_strain AAI_STRAIN\n                        AAI threshold used to identify strain heterogeneity (default: 0.9)\n  -a, --alignment_file ALIGNMENT_FILE\n                        produce file showing alignment of multi-copy genes and their AAI identity\n  --ignore_thresholds   ignore model-specific score thresholds\n  -e, --e_value E_VALUE\n                        e-value cut off (default: 1e-10)\n  -l, --length LENGTH   percent overlap between target and query (default: 0.7)\n  -f, --file FILE       print results to file (default: stdout)\n  --tab_table           print tab-separated values table\n  -x, --extension EXTENSION\n                        extension of bins (other files in directory are ignored) (default: fna)\n  -t, --threads THREADS\n                        number of threads (default: 1)\n  --pplacer_threads PPLACER_THREADS\n                        number of threads used by pplacer (memory usage increases linearly with additional threads) (default: 1)\n  -q, --quiet           suppress console output\n  --tmpdir TMPDIR       specify an alternative directory for temporary files\n\nExample: checkm lineage_wf ./bins ./output\n\n\n\nThis readout tells us what we need to include in the command:\n\nthe x flag telling CheckM the format of our bins (fa)\nthe directory that contains the bins (assembly_ERR5000342.fasta.metabat-bins1500-YYYYMMDD_HHMMSS/)\nthe directory that we want the output to be saved in (checkm/)\nthe --reduced_tree flag to limit the memory requirements\nthe -f flag to specify an output file name/format\nthe --tab_table flag so the output is in a tab-separated format\nthe -t flag to set the number of threads used to eight, which is the number we have on our instance\n\nAs a result our command looks like this:\n\n\nCode\n\ncheckm lineage_wf -x fa results/binning/assembly_ERR5000342.fasta.metabat-bins1500-YYYYMMDD_HHMMSS/ results/checkm/ --reduced_tree -t 8 --tab_table -f results/checkm/MAGs_checkm.tsv &&gt; results/checkm/checkm.out &\n\n(Don’t forget to change YYYYMMDD-HHMMSS to match your directory’s name.)\nAs always you can check the command’s progress by looking inside the checkm.out file or using jobs (as long as you haven’t logged out of your instance since starting the command running).\nWhen the run ends (it should take around 20 minutes) we can open our results file.\n\n\nCode\n\ncd results/checkm\nless MAGs_checkm.tsv\n\n\n\nOutput\n\n| Bin Id | Marker lineage       | # genomes | # markers | # marker sets | 0   | 1  | 2 | 3 | 4 | 5+ | Completeness | Contamination | Strain heterogeneity |\n|--------|----------------------|-----------|-----------|---------------|-----|----|---|---|---|----|--------------|---------------|---------|\n| bin.1  | k__Bacteria (UID203) | 5449      | 104       | 58            | 95  | 9  | 0 | 0 | 0 | 0  | 1.79         | 0.00          | 0.00    |\n| bin.10 | k__Bacteria (UID203) | 5449      | 104       | 58            | 100 | 4  | 0 | 0 | 0 | 0  | 3.45         | 0.00          | 0.00    |\n| bin.11 | root (UID1)          | 5656      | 56        | 24            | 56  | 0  | 0 | 0 | 0 | 0  | 0.00         | 0.00          | 0.00    |\n| bin.12 | k__Bacteria (UID203) | 5449      | 102       | 57            | 93  | 9  | 0 | 0 | 0 | 0  | 12.28        | 0.00          | 0.00    |\n| bin.13 | root (UID1)          | 5656      | 56        | 24            | 56  | 0  | 0 | 0 | 0 | 0  | 0.00         | 0.00          | 0.00    |\n| bin.14 | k__Bacteria (UID203) | 5449      | 104       | 58            | 92  | 12 | 0 | 0 | 0 | 0  | 10.11        | 0.00          | 0.00    |\n| bin.15 | root (UID1)          | 5656      | 56        | 24            | 55  | 1  | 0 | 0 | 0 | 0  | 4.17         | 0.00          | 0.00    |\n\nRunning this workflow is equivalent to running six separate CheckM commands. The CheckM documentation explains this is more detail.\n\n\n\n\n\n\nExercise 1: Downloading the tsv file\n\n\n\nFill in the blanks to complete the code you need to download the MAGs_checkm.tsv to your local computer using SCP:\n\n\nCode\n\nscp -i ___ csuser@instanceNNN.cloud-span.aws.york.ac.uk.:___/cs_course/results/checkm/MAGs_checkm.tsv ____\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn a terminal logged into your local machine type:\n\n\nCode\n\nscp -i login-key-instanceNNN.pem csuser@instanceNNN.cloud-span.aws.york.ac.uk:~/cs_course/results/checkm/MAGs_checkm.tsv &lt;the destination directory of your choice&gt;\n\n\n\n\n\n\nHow much contamination we can tolerate and how much completeness we need depends on the scientific question being tackled.\nTo help us, we can use a standard called Minimum Information about a Metagenome-Assembled Genome (MIMAG), developed by the Genomics Standard Consortium. You can read more about MIMAG in this 2017 paper.\nAs part of the standard, a framework to determine MAG quality from statistics is outlined. A MAG can be assigned one of three different metrics: High, Medium or Low quality draft metagenome assembled genomes.\nSee the table below for an overview of each category.\n\n\n\n\n\n\n\n\n\nQuality Category\nCompleteness\nContamination\nrRNA/tRNA encoded\n\n\n\n\nHigh\n&gt; 90%\n≤ 5%\nYes (≥ 18 tRNA and all rRNA)\n\n\nMedium\n≥ 50%\n≤ 10%\nNo\n\n\nLow\n&lt; 50%\n≤ 10%\nNo\n\n\n\nWe have already determined the completeness and contamination of each of our MAGs using CheckM. Next we will use a program to determine which rRNA and tRNAs are present in each MAG.\nNote that due to the difficulty in the assembly of short-read metagenomes, often just a completeness of &gt;90% and a contamination of ≤ 5% is treated as a good quality MAG.\nTo best examine your bins, you might want to import it into a spreadsheet software program (you should be able to directly copy and paste the contents of your MAGS_checkm.tsv text file into a spreadsheet without needing to do any further formatting or splitting into columns). Then, you can use “filter” (Google Sheets) or “format as table” (Excel) to sort your bins by completeness and/or contamination.\n\n\n\nSelect the entire top row (containing your headers) of the sheet\nClick the symbol which looks like a funnel in the toolbar at the top (second from the right) or go to Data &gt; Create a filter\nYou can now click the inverted pyramid in the header cell of each column to sort/filter the data by the values in that column\n\n\n\nSelect all of the cells containing your data\nFrom the main Home bar, select ‘Format as Table’ (near the middle of the toolbar) and choose a style. In the popup that appears, make sure ‘My table has headers’ is checked then click OK.\nYou can now use the drop down arrow in the header cell of each column to sort/filter the data by the values in that column\n\n\n\n\n\n\nSpreadsheet with checkm output formatted as a table.\n\n\nHere the bins are sorted by completeness. Completeness is evaluated by looking for the presence of a set of marker genes - 100% complete means all the genes were found. Contamination is determined by the fraction of marker genes that occur as duplicates, indicating that more than one genome is present.\nOther columns to consider:\n\nmarker lineage tells you what taxa your MAG might belong to (even if this is as broad as just “bacteria” or even “root”)\n# genomes tells you how many genomes were used to generate each marker set (which is based on the marker lineage, so if CheckM couldn’t work out what your MAG was beyond “bacteria” it uses marker genes from 5449 different species)\n# markers tells you how many markers were needed for the genome to be 100% complete\nnumbers 0 to 5+ tell you how many times marker genes were identified e.g.\n\n0 tells you how many markers were not found at all\n1 tells you how many marker were found once only\n2 tells you how many markers were found twice\nand so on.\n\nstrain heterogeneity tells you how much of the contamination is likely to come from another strain of the same species.\n\nFor example, in the CheckM output shown above, Bin 5 is 100% complete. However, it has 400% contamination, meaning the markers were present multiple times instead of just once. Indeed we can see that 52 markers were present 5 or more times, 17 present 4 times etc. The strain heterogeneity is quite low suggesting that this contamination is not due to having several strains of one species present. Likely as a result of this contamination, CheckM was only able to classify the MAG as “Bacteria” and could not be more specific.\nAlternatively, Bin 46 is 86.77% complete and only has 16% contamination. We can see that 263 marker genes were present once only - this indicates that this is mostly one genome, with a bit of contamination mixed in. CheckM classified this MAG as belonging to order Rhodospirallales and subsequently used 63 genomes from this lineage to generate the marker sets. It has 40% strain heterogeneity so we can assume that some of the contamination comes from very similar strains being mixed together. This is a much better bin than Bin 5, even though it is ostensibly less complete.\n\n\nIt is important to remember that the “completeness” metric relies on having well-characterised lineages present in CheckM’s database. If the MAG belongs to a poorly-characterised lineage, the results may not be accurate.\nYou will find that none of our bins satisfy the requirements for a “high quality” bin. That’s okay! Binning is still quite an inexact science and while various tools exist to do the job of binning, all of them need to be taken with a pinch of salt.\nSo how can you make your bin outputs more reliable? There are ways of combining outputs from different binning tools which can help crosscheck results and refine your bins. You can also try using different parameters, or go back and tweak parameters in the assembly and polishing steps.\n\n\n\n\n\n\nExercise 2: Explore the quality of the obtained MAGs\n\n\n\nOnce you have downloaded the MAGs_checkm.tsv file, you can open it in Excel or another spreadsheet program. If you didn’t manage to download the file, or do not have an appropriate program to view it in you can see or download our example file here.\nLooking at the results of our quality checks, which MAGs could be classified as medium quality? Use the table above to remind yourself of the quality requirements.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBins 22, 45 and 50 all have completeness more than or equal to 50%, and contamination below 10%, meaning they can be classed as medium quality.\nA much larger number of bins have completeness less than 50% and contamination below 10% (low quality). Notably, bin 56 is 48.9% complete with 7.7% contamination, meaning it comes very close to being classed as medium quality but technically should be considered low quality.\nYour bins may have different names/numbers to these but you should still see similar results.",
    "crumbs": [
      "Home",
      "Binning & Functional Annotation",
      "QC of metagenome bins"
    ]
  },
  {
    "objectID": "docs/lesson05-binning-functional-annotation/02-binning_quality.html#quality-check",
    "href": "docs/lesson05-binning-functional-annotation/02-binning_quality.html#quality-check",
    "title": "QC of metagenome bins",
    "section": "",
    "text": "The quality of a metagenome-assembled genome (MAG) or bin is highly dependent on several things:\n\nthe depth of sequencing\nthe abundance of the organism in the community\nhow successful the assembly was\nhow successful the polishing (if used) was\n\nIn order to determine the quality of a MAG we can look at two different metrics. These are:\n\ncompleteness (i.e. how much of the genome is captured in the MAG?) and\ncontamination (i.e. do all the sequences in the MAG belong to the same organism?).\n\nWe can use the program CheckM to determine the quality of MAGs. CheckM uses a collection of domain and lineage-specific markers to estimate completeness and contamination of a MAG. This short YouTube video by Dr Robert Edwards explains how CheckM uses a hidden Markov model to calculate the level of contamination and completeness of bins, based on marker gene sets.\nCheckM has multiple different workflows available which are appropriate for different datasets. See CheckM documentation on Workflows for more information.\nWe will be using the lineage-specific workflow here. lineage_wf places your bins in a reference tree to determine which lineage it corresponds to. This allows it to use the appropriate marker genes to estimate quality parameters.\nFirst let’s move into our cs_course folder and make a directory for our CheckM results.\n\n\nCode\n\ncd ~/cs_course\nmkdir results/checkm\n\nCheckM has been pre-installed on the instance so we can check the help documentation for the lineage-specific workflow using the -h flag/option.\n\n\nCode\n\ncheckm lineage_wf -h\n\n\n\n\n\n\n\nOutput — CheckM help documentation\n\n\n\n\n\nusage: checkm lineage_wf [-h] [-r] [--ali] [--nt] [-g] [-u UNIQUE] [-m MULTI]\n                         [--force_domain] [--no_refinement]\n                         [--individual_markers] [--skip_adj_correction]\n                         [--skip_pseudogene_correction]\n                         [--aai_strain AAI_STRAIN] [-a ALIGNMENT_FILE]\n                         [--ignore_thresholds] [-e E_VALUE] [-l LENGTH]\n                         [-f FILE] [--tab_table] [-x EXTENSION] [-t THREADS]\n                         [--pplacer_threads PPLACER_THREADS] [-q]\n                         [--tmpdir TMPDIR]\n                         bin_input output_dir\n\nRuns tree, lineage_set, analyze, qa\n\npositional arguments:\n  bin_input             directory containing bins (fasta format) or path to file describing genomes/genes - tab separated in 2 or 3 columns [genome ID, genome fna, genome translation file (pep)]\n  output_dir            directory to write output files\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -r, --reduced_tree    use reduced tree (requires &lt;16GB of memory) for determining lineage of each bin\n  --ali                 generate HMMER alignment file for each bin\n  --nt                  generate nucleotide gene sequences for each bin\n  -g, --genes           bins contain genes as amino acids instead of nucleotide contigs\n  -u, --unique UNIQUE   minimum number of unique phylogenetic markers required to use lineage-specific marker set (default: 10)\n  -m, --multi MULTI     maximum number of multi-copy phylogenetic markers before defaulting to domain-level marker set (default: 10)\n  --force_domain        use domain-level sets for all bins\n  --no_refinement       do not perform lineage-specific marker set refinement\n  --individual_markers  treat marker as independent (i.e., ignore co-located set structure)\n  --skip_adj_correction\n                        do not exclude adjacent marker genes when estimating contamination\n  --skip_pseudogene_correction\n                        skip identification and filtering of pseudogenes\n  --aai_strain AAI_STRAIN\n                        AAI threshold used to identify strain heterogeneity (default: 0.9)\n  -a, --alignment_file ALIGNMENT_FILE\n                        produce file showing alignment of multi-copy genes and their AAI identity\n  --ignore_thresholds   ignore model-specific score thresholds\n  -e, --e_value E_VALUE\n                        e-value cut off (default: 1e-10)\n  -l, --length LENGTH   percent overlap between target and query (default: 0.7)\n  -f, --file FILE       print results to file (default: stdout)\n  --tab_table           print tab-separated values table\n  -x, --extension EXTENSION\n                        extension of bins (other files in directory are ignored) (default: fna)\n  -t, --threads THREADS\n                        number of threads (default: 1)\n  --pplacer_threads PPLACER_THREADS\n                        number of threads used by pplacer (memory usage increases linearly with additional threads) (default: 1)\n  -q, --quiet           suppress console output\n  --tmpdir TMPDIR       specify an alternative directory for temporary files\n\nExample: checkm lineage_wf ./bins ./output\n\n\n\nThis readout tells us what we need to include in the command:\n\nthe x flag telling CheckM the format of our bins (fa)\nthe directory that contains the bins (assembly_ERR5000342.fasta.metabat-bins1500-YYYYMMDD_HHMMSS/)\nthe directory that we want the output to be saved in (checkm/)\nthe --reduced_tree flag to limit the memory requirements\nthe -f flag to specify an output file name/format\nthe --tab_table flag so the output is in a tab-separated format\nthe -t flag to set the number of threads used to eight, which is the number we have on our instance\n\nAs a result our command looks like this:\n\n\nCode\n\ncheckm lineage_wf -x fa results/binning/assembly_ERR5000342.fasta.metabat-bins1500-YYYYMMDD_HHMMSS/ results/checkm/ --reduced_tree -t 8 --tab_table -f results/checkm/MAGs_checkm.tsv &&gt; results/checkm/checkm.out &\n\n(Don’t forget to change YYYYMMDD-HHMMSS to match your directory’s name.)\nAs always you can check the command’s progress by looking inside the checkm.out file or using jobs (as long as you haven’t logged out of your instance since starting the command running).\nWhen the run ends (it should take around 20 minutes) we can open our results file.\n\n\nCode\n\ncd results/checkm\nless MAGs_checkm.tsv\n\n\n\nOutput\n\n| Bin Id | Marker lineage       | # genomes | # markers | # marker sets | 0   | 1  | 2 | 3 | 4 | 5+ | Completeness | Contamination | Strain heterogeneity |\n|--------|----------------------|-----------|-----------|---------------|-----|----|---|---|---|----|--------------|---------------|---------|\n| bin.1  | k__Bacteria (UID203) | 5449      | 104       | 58            | 95  | 9  | 0 | 0 | 0 | 0  | 1.79         | 0.00          | 0.00    |\n| bin.10 | k__Bacteria (UID203) | 5449      | 104       | 58            | 100 | 4  | 0 | 0 | 0 | 0  | 3.45         | 0.00          | 0.00    |\n| bin.11 | root (UID1)          | 5656      | 56        | 24            | 56  | 0  | 0 | 0 | 0 | 0  | 0.00         | 0.00          | 0.00    |\n| bin.12 | k__Bacteria (UID203) | 5449      | 102       | 57            | 93  | 9  | 0 | 0 | 0 | 0  | 12.28        | 0.00          | 0.00    |\n| bin.13 | root (UID1)          | 5656      | 56        | 24            | 56  | 0  | 0 | 0 | 0 | 0  | 0.00         | 0.00          | 0.00    |\n| bin.14 | k__Bacteria (UID203) | 5449      | 104       | 58            | 92  | 12 | 0 | 0 | 0 | 0  | 10.11        | 0.00          | 0.00    |\n| bin.15 | root (UID1)          | 5656      | 56        | 24            | 55  | 1  | 0 | 0 | 0 | 0  | 4.17         | 0.00          | 0.00    |\n\nRunning this workflow is equivalent to running six separate CheckM commands. The CheckM documentation explains this is more detail.\n\n\n\n\n\n\nExercise 1: Downloading the tsv file\n\n\n\nFill in the blanks to complete the code you need to download the MAGs_checkm.tsv to your local computer using SCP:\n\n\nCode\n\nscp -i ___ csuser@instanceNNN.cloud-span.aws.york.ac.uk.:___/cs_course/results/checkm/MAGs_checkm.tsv ____\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn a terminal logged into your local machine type:\n\n\nCode\n\nscp -i login-key-instanceNNN.pem csuser@instanceNNN.cloud-span.aws.york.ac.uk:~/cs_course/results/checkm/MAGs_checkm.tsv &lt;the destination directory of your choice&gt;\n\n\n\n\n\n\nHow much contamination we can tolerate and how much completeness we need depends on the scientific question being tackled.\nTo help us, we can use a standard called Minimum Information about a Metagenome-Assembled Genome (MIMAG), developed by the Genomics Standard Consortium. You can read more about MIMAG in this 2017 paper.\nAs part of the standard, a framework to determine MAG quality from statistics is outlined. A MAG can be assigned one of three different metrics: High, Medium or Low quality draft metagenome assembled genomes.\nSee the table below for an overview of each category.\n\n\n\n\n\n\n\n\n\nQuality Category\nCompleteness\nContamination\nrRNA/tRNA encoded\n\n\n\n\nHigh\n&gt; 90%\n≤ 5%\nYes (≥ 18 tRNA and all rRNA)\n\n\nMedium\n≥ 50%\n≤ 10%\nNo\n\n\nLow\n&lt; 50%\n≤ 10%\nNo\n\n\n\nWe have already determined the completeness and contamination of each of our MAGs using CheckM. Next we will use a program to determine which rRNA and tRNAs are present in each MAG.\nNote that due to the difficulty in the assembly of short-read metagenomes, often just a completeness of &gt;90% and a contamination of ≤ 5% is treated as a good quality MAG.\nTo best examine your bins, you might want to import it into a spreadsheet software program (you should be able to directly copy and paste the contents of your MAGS_checkm.tsv text file into a spreadsheet without needing to do any further formatting or splitting into columns). Then, you can use “filter” (Google Sheets) or “format as table” (Excel) to sort your bins by completeness and/or contamination.\n\n\n\nSelect the entire top row (containing your headers) of the sheet\nClick the symbol which looks like a funnel in the toolbar at the top (second from the right) or go to Data &gt; Create a filter\nYou can now click the inverted pyramid in the header cell of each column to sort/filter the data by the values in that column\n\n\n\nSelect all of the cells containing your data\nFrom the main Home bar, select ‘Format as Table’ (near the middle of the toolbar) and choose a style. In the popup that appears, make sure ‘My table has headers’ is checked then click OK.\nYou can now use the drop down arrow in the header cell of each column to sort/filter the data by the values in that column\n\n\n\n\n\n\nSpreadsheet with checkm output formatted as a table.\n\n\nHere the bins are sorted by completeness. Completeness is evaluated by looking for the presence of a set of marker genes - 100% complete means all the genes were found. Contamination is determined by the fraction of marker genes that occur as duplicates, indicating that more than one genome is present.\nOther columns to consider:\n\nmarker lineage tells you what taxa your MAG might belong to (even if this is as broad as just “bacteria” or even “root”)\n# genomes tells you how many genomes were used to generate each marker set (which is based on the marker lineage, so if CheckM couldn’t work out what your MAG was beyond “bacteria” it uses marker genes from 5449 different species)\n# markers tells you how many markers were needed for the genome to be 100% complete\nnumbers 0 to 5+ tell you how many times marker genes were identified e.g.\n\n0 tells you how many markers were not found at all\n1 tells you how many marker were found once only\n2 tells you how many markers were found twice\nand so on.\n\nstrain heterogeneity tells you how much of the contamination is likely to come from another strain of the same species.\n\nFor example, in the CheckM output shown above, Bin 5 is 100% complete. However, it has 400% contamination, meaning the markers were present multiple times instead of just once. Indeed we can see that 52 markers were present 5 or more times, 17 present 4 times etc. The strain heterogeneity is quite low suggesting that this contamination is not due to having several strains of one species present. Likely as a result of this contamination, CheckM was only able to classify the MAG as “Bacteria” and could not be more specific.\nAlternatively, Bin 46 is 86.77% complete and only has 16% contamination. We can see that 263 marker genes were present once only - this indicates that this is mostly one genome, with a bit of contamination mixed in. CheckM classified this MAG as belonging to order Rhodospirallales and subsequently used 63 genomes from this lineage to generate the marker sets. It has 40% strain heterogeneity so we can assume that some of the contamination comes from very similar strains being mixed together. This is a much better bin than Bin 5, even though it is ostensibly less complete.\n\n\nIt is important to remember that the “completeness” metric relies on having well-characterised lineages present in CheckM’s database. If the MAG belongs to a poorly-characterised lineage, the results may not be accurate.\nYou will find that none of our bins satisfy the requirements for a “high quality” bin. That’s okay! Binning is still quite an inexact science and while various tools exist to do the job of binning, all of them need to be taken with a pinch of salt.\nSo how can you make your bin outputs more reliable? There are ways of combining outputs from different binning tools which can help crosscheck results and refine your bins. You can also try using different parameters, or go back and tweak parameters in the assembly and polishing steps.\n\n\n\n\n\n\nExercise 2: Explore the quality of the obtained MAGs\n\n\n\nOnce you have downloaded the MAGs_checkm.tsv file, you can open it in Excel or another spreadsheet program. If you didn’t manage to download the file, or do not have an appropriate program to view it in you can see or download our example file here.\nLooking at the results of our quality checks, which MAGs could be classified as medium quality? Use the table above to remind yourself of the quality requirements.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBins 22, 45 and 50 all have completeness more than or equal to 50%, and contamination below 10%, meaning they can be classed as medium quality.\nA much larger number of bins have completeness less than 50% and contamination below 10% (low quality). Notably, bin 56 is 48.9% complete with 7.7% contamination, meaning it comes very close to being classed as medium quality but technically should be considered low quality.\nYour bins may have different names/numbers to these but you should still see similar results.",
    "crumbs": [
      "Home",
      "Binning & Functional Annotation",
      "QC of metagenome bins"
    ]
  },
  {
    "objectID": "docs/lesson05-binning-functional-annotation/01-binning.html",
    "href": "docs/lesson05-binning-functional-annotation/01-binning.html",
    "title": "Metagenome Binning",
    "section": "",
    "text": "Now we are ready to start doing analysis of our metagenomic assembly!",
    "crumbs": [
      "Home",
      "Binning & Functional Annotation",
      "Metagenome Binning"
    ]
  },
  {
    "objectID": "docs/lesson05-binning-functional-annotation/01-binning.html#a-new-assembly",
    "href": "docs/lesson05-binning-functional-annotation/01-binning.html#a-new-assembly",
    "title": "Metagenome Binning",
    "section": "A new assembly",
    "text": "A new assembly\nIn the last two lessons we constructed and polished an assembly based on a subset of a bigger dataset. That’s why our nano_fastq file was called ERR5000342_sub12.fastq - it contained a randomly chosen 12% subset of the original dataset. We did this because the full nano_fastq dataset contains over 2 million reads, and the instance doesn’t have enough computing power to cope with that much data.\nSubsetting your data is a great way to practice and troubleshoot workflows like the one we’re following in this course. However, only assembling 12% of the reads means the assembly is far less likely to be complete. We saw the effects of this when we used seqkit and MetaQUAST to quality check our assemblies in the last lesson. Only about 0.2-0.3% of the genomes were complete.\nFortunately, we have access to an assembly that was generated from the full ERR5000342.fastq long read dataset. The process to generate and polish it was exactly the same; we just started with a larger dataset (2,000,000 reads compared to 300,000 reads). Now that we are onto analysis, which relies on having a strong assembly, we will switch to using this bigger assembly.\n\nWhere to find the new assembly\nThe new assembly is stored in a hidden file in our data directory. Let’s take a look using the -a flag for ls.\n\n\nCode\n\ncd ~/cs_course/data\nls -a\n\n\n\nOutput\n\nillumina_fastq/   nano_fastq/  .assembly_ERR5000342.fasta.gz\n\nThe .gz extension means the file is zipped using the command gzip. Gzip stores files in a compressed format so they take up less space. It can be reversed with the command gunzip.\n\n\nCode\n\ngunzip .assembly_ERR5000342.fasta.gz\n\nNow the assembly is unzipped, the .gz extension is gone.\n\n\nCode\n\nls -a\n\n\n\nOutput\n\nillumina_fastq/   nano_fastq/  .assembly_ERR5000342.fasta\n\nLet’s rename the file to get rid of the . at the start and put it into its own directory called full_assembly.\n\n\nCode\n\nmv .assembly_ERR5000342.fasta assembly_ERR5000342.fasta\nmkdir full_assembly\nmv assembly_ERR5000342.fasta full_assembly/\n\nNow we’re ready to start binning!",
    "crumbs": [
      "Home",
      "Binning & Functional Annotation",
      "Metagenome Binning"
    ]
  },
  {
    "objectID": "docs/lesson05-binning-functional-annotation/01-binning.html#metagenomic-binning",
    "href": "docs/lesson05-binning-functional-annotation/01-binning.html#metagenomic-binning",
    "title": "Metagenome Binning",
    "section": "Metagenomic binning",
    "text": "Metagenomic binning\nNow we can start to separate out the individual genomes using a process called binning. This will allow us to analyse each of the species inside our sample individually. We call these genomes metagenome-assembled genomes (MAGs).\nThe assembled contigs that make up the metagenome will be assigned to different “bins” (FASTA files that contain certain contigs). Ideally, each bin will correspond to one original genome only (a MAG).\nAs we covered in the assembly section, assembling the pieces of a metagenome is more difficult compared to single genome assembly. Most assemblers are not able to reconstruct complete genomes for the organisms that are represented in the metagenome - even using our new bigger dataset. As a result each organism will be represented by multiple contigs following assembly and polishing. This means that we need to be able to separate these contigs so we can identify which belong to each organism in our metagenome. This is where binning comes in.\n\n\n\n\nDiagram depicting the DNA sequences in the original sample as circular chromosomes, then the DNA fragmented into reads, then assembled into contigs, and then binned.\n\n\nOne way to separate contigs that belong to different species is by their taxonomic assignation. However, this can be time consuming and require a lot of computational power. There are easier methods that perform binning to a high quality using characteristics of the contigs, such as their GC content, their tetranucleotide frequencies (TNF), their coverage (abundance), sets of marker genes, taxonomic aligments and their preferred codons.\nMost binning tools use short reads for the binning; only a few use Hi-C sequencing. Hi-C is a method of sequencing that gives spatial proximity information, as described here. Different tools use different algorithms for performing the binning. A few popular tools are summarised below. For more information see Section 2.4 (Tools for metagenome binning) of this review.\n\n\n\nTool\nCore algorithm\nWebsite\nPublication\n\n\n\n\nMaxBin2\nExpectation-maximization\nhttp://sourceforge.net/projects/maxbin/\nWu et al, 2016\n\n\nCONCOCT\nGaussiAN Mixture Models\nhttps://github.com/BinPro/CONCOCT\nAlneberg et al, 2014\n\n\nMetaBAT2\nLabel propagation\nhttps://bitbucket.org/berkeleylab/metabat\nKang et al, 2019\n\n\n\nThere are other tools that bin MAGs using several different methods and then further refine these bins. DAStool, MetaWRAP and Metagenome Assembled Genomes Orchestra MAGO are capable of doing this.\nMetaBAT2 is a binning algorithm that distinguishes between contigs that belong to different bins according to their coverage levels and the tetranucleotide frequencies they have. We will be using this algorithm for our binning today, but first we need to prepare our assembly.",
    "crumbs": [
      "Home",
      "Binning & Functional Annotation",
      "Metagenome Binning"
    ]
  },
  {
    "objectID": "docs/lesson05-binning-functional-annotation/01-binning.html#preparation-for-binning",
    "href": "docs/lesson05-binning-functional-annotation/01-binning.html#preparation-for-binning",
    "title": "Metagenome Binning",
    "section": "Preparation for binning",
    "text": "Preparation for binning\nThis preparation process follows exactly the same steps we used to prepare our long-read polished assembly for short-read polishing. We need to align our short reads to our assembly to determine depth-of-coverage — how many reads align to each portion of the assembly.\nFirst we index the polished reference using bwa index. Remember, we’re using our new assembly which is in our data/full_assembly directory. The outputs will automatically be put in the same folder as the assembly, as they should always stay together.\n\n\nCode\n\ncd ~/cs_course\nbwa index data/full_assembly/assembly_ERR5000342.fasta\n\nThis is a big file so indexing will take about 4-5 minutes to complete. We ran this command in the foreground so you won’t be able to use your prompt until it’s finished.\nWe then make a directory for the output of our binning.\n\n\nCode\n\nmkdir results/binning\n\nWe can then use an adapted form of the bwa mem command we used earlier to align our short reads to the polished assembly and determine the abundance of each contig.\n\n\nCode\n\n( bwa mem -t 8 data/full_assembly/assembly_ERR5000342.fasta data/illumina_fastq/ERR4998593_1.fastq data/illumina_fastq/ERR4998593_2.fastq | samtools view - -Sb | samtools sort - -@8 -o results/binning/assembly_short_read_alignment.bam ) &&gt; results/binning/alignment.out &\n\nThis should take around 80 minutes to complete.\nOnce the file is created, we can check the alignment.out log file.\n\n\nCode\n\ncd results/binning\nless alignment.out\n\nThe start of the file should look like this:\n\n\nOutput\n\n[M::bwa_idx_load_from_disk] read 0 ALT contigs\n[M::process] read 529802 sequences (80000102 bp)...\n[M::process] read 529802 sequences (80000102 bp)...\n[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (23, 49949, 37, 28)\n[M::mem_pestat] analyzing insert size distribution for orientation FF...\n[M::mem_pestat] (25, 50, 75) percentile: (7, 872, 2681)\n[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 8029)\n[M::mem_pestat] mean and std.dev: (1744.43, 2239.00)\n[M::mem_pestat] low and high boundaries for proper pairs: (1, 10703)\n[M::mem_pestat] analyzing insert size distribution for orientation FR...\n[M::mem_pestat] (25, 50, 75) percentile: (347, 430, 555)\n[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 971)\n[M::mem_pestat] mean and std.dev: (453.47, 163.23)\n\nIf we scroll down, the end should look like this:\n\n\nOutput\n\n[M::mem_process_seqs] Processed 182762 reads in 84.398 CPU sec, 11.894 real sec\n[main] Version: 0.7.17-r1188\n[main] CMD: bwa mem -t 8 ../../data/full_assembly/assembly_ERR5000342.fasta ../../data/illumina_fastq/ERR4998593_1.fastq ../../data/illumina_fastq/ERR4998593_2.fastq\n[main] Real time: 4154.680 sec; CPU: 30896.696 sec\n[bam_sort_core] merging from 7 files and 4 in-memory blocks...\n\nWe can also check that the new BAM file exists using ls.\n\n\nCode\n\nls\n\n\n\nOutput\n\nalignment.out   assembly_short_read_alignment.bam\n\nIn order to use this new BAM with MetaBAT2 we also need to index the alignment using the command samtools index. This only takes 2-3 minutes.\n\n\nCode\n\ncd ~/cs_course\nsamtools index results/binning/assembly_short_read_alignment.bam\n\nWhen we have the sorted and indexed BAM file we are then ready to use MetaBAT2.",
    "crumbs": [
      "Home",
      "Binning & Functional Annotation",
      "Metagenome Binning"
    ]
  },
  {
    "objectID": "docs/lesson05-binning-functional-annotation/01-binning.html#binning-using-metabat2",
    "href": "docs/lesson05-binning-functional-annotation/01-binning.html#binning-using-metabat2",
    "title": "Metagenome Binning",
    "section": "Binning using MetaBAT2",
    "text": "Binning using MetaBAT2\nMetaBAT2 has been pre-installed on your instance. The documentation tells us how to run the program via the command line.\nThe easiest way to run MetaBAT2 is using the command runMetaBat.sh &lt;options&gt; assembly.fasta sample1.bam [sample2.bam ...]. This will generate a depth file and then do the binning for us. In this example, we’re also going to add the flag -m 1500, which sets the minimum contig length to 1500bp - any contigs shorter than this will not be binned.\n\n\nCode\n\ncd ~/cs_course\nrunMetaBat.sh -m 1500 data/full_assembly/assembly_ERR5000342.fasta results/binning/assembly_short_read_alignment.bam\n\nMetaBAT2 first reads in the .bam file, then generates bins. This should take around 2 or 3 minutes.\nWhile MetaBAT2 is processing the .bam file you will see the following output:\n\n\nOutput\n\nExecuting: 'jgi_summarize_bam_contig_depths --outputDepth assembly_ERR5000342.fasta.depth.txt --percentIdentity 97 --minContigLength 1000 --minContigDepth 1.0  --referenceFasta data/full_assembly/assembly_ERR5000342.fasta results/binning/assembly_short_read_alignment.bam' at Tue 14 Mar 2023 05:39:47 PM UTC\nOutput depth matrix to assembly_ERR5000342.fasta.depth.txt\nMinimum percent identity for a mapped read: 0.97\nminContigLength: 1000\nminContigDepth: 1\nReference fasta file /home/csuser/cs_course/data/full_assembly/assembly_ERR5000342.fasta\njgi_summarize_bam_contig_depths 2.15 (Bioconda) 2020-01-04T21:10:40\nOutput matrix to assembly_ERR5000342.fasta.depth.txt\nReading reference fasta file: /home/csuser/cs_course/data/full_assembly/assembly_ERR5000342.fasta\n... 7250 sequences\n0: Opening bam: assembly_short_read_alignment.bam\nProcessing bam files\n\nOnce the .bam file has processed and binning has completed, the output will look like this:\n\n\nOutput\n\nThread 0 finished: assembly_short_read_alignment.bam with 68793457 reads and 11049740 readsWellMapped\nCreating depth matrix file: assembly_ERR5000342.fasta.depth.txt\nClosing most bam files\nClosing last bam file\nFinished\nFinished jgi_summarize_bam_contig_depths at Fri 17 Mar 2023 12:24:35 PM UTC\nCreating depth file for metabat at Fri 17 Mar 2023 12:24:35 PM UTC\nExecuting: 'metabat2  -m 1500 --inFile /home/csuser/cs_course/data/full_assembly/assembly_ERR5000342.fasta --outFile assembly_ERR5000342.fasta.metabat-bins1500-20230317_122435/bin --abdFile assembly_ERR5000342.fasta.depth.txt' at Fri 17 Mar 2023 12:24:35 PM UTC\nMetaBAT 2 (2.15 (Bioconda)) using minContig 1500, minCV 1.0, minCVSum 1.0, maxP 95%, minS 60, maxEdges 200 and minClsSize 200000. with random seed=1679055875\n90 bins (212166000 bases in total) formed.\nFinished metabat2 at Tue 14 Mar 2023 05:40:46 PM UTC\n\nThe penultimate line tells us that MetaBAT has produced 90 bins containing 212166000 bases (your number might vary slightly depending on how the algorithm has analysed your assembly).\nUsing ls will show that MetaBAT2 has generated a depth file (assembly_ERR5000342.fasta.depth.txt) and a directory (assembly_ERR5000342.fasta.metabat-bins1500-YYYYMMDD_HHMMSS/). Annoyingly, the “easy” way of running MetaBat2 (which we just used) doesn’t allow us to specify an output directory, so we’ll need to move our outputs into results/binning manually using mv.\nmv assembly* results/binning\nNow let’s move into our new directory and take a look!\n\n\n\n\n\n\nNote\n\n\n\nThe name of the directory containing the bins will depend on exactly when the binning completes. Don’t forget to replace YYYMMDD_HHMMSS in the command below with the actual name of your directory.\n\n\n\n\nCode\n\ncd results/binning/assembly_ERR5000342.fasta.metabat-bins1500-YYYYMMDD_HHMMSS/\nls\n\n\n\nOutput\n\nbin.10.fa  bin.22.fa  bin.34.fa  bin.46.fa  bin.58.fa  bin.6.fa   bin.81.fa\nbin.11.fa  bin.23.fa  bin.35.fa  bin.47.fa  bin.59.fa  bin.70.fa  bin.82.fa\nbin.12.fa  bin.24.fa  bin.36.fa  bin.48.fa  bin.5.fa   bin.71.fa  bin.83.fa\nbin.13.fa  bin.25.fa  bin.37.fa  bin.49.fa  bin.60.fa  bin.72.fa  bin.84.fa\nbin.14.fa  bin.26.fa  bin.38.fa  bin.4.fa   bin.61.fa  bin.73.fa  bin.85.fa\nbin.15.fa  bin.27.fa  bin.39.fa  bin.50.fa  bin.62.fa  bin.74.fa  bin.86.fa\nbin.16.fa  bin.28.fa  bin.3.fa   bin.51.fa  bin.63.fa  bin.75.fa  bin.87.fa\nbin.17.fa  bin.29.fa  bin.40.fa  bin.52.fa  bin.64.fa  bin.76.fa  bin.88.fa\nbin.18.fa  bin.2.fa   bin.41.fa  bin.53.fa  bin.65.fa  bin.77.fa  bin.89.fa\nbin.19.fa  bin.30.fa  bin.42.fa  bin.54.fa  bin.66.fa  bin.78.fa  bin.8.fa\nbin.1.fa   bin.31.fa  bin.43.fa  bin.55.fa  bin.67.fa  bin.79.fa  bin.90.fa\nbin.20.fa  bin.32.fa  bin.44.fa  bin.56.fa  bin.68.fa  bin.7.fa   bin.9.fa\nbin.21.fa  bin.33.fa  bin.45.fa  bin.57.fa  bin.69.fa  bin.80.fa\n\nNote these output files have the file extensions of .fa. This is exactly the same format as a .fasta file but with a shortened version of the extension. See the wikipedia page on FASTA format - file for some other examples of file extensions.\nIdeally we would like only one contig per bin, with a length similar to the genome size of the corresponding taxa. This is challenging as this would require knowing what species are present in the mixed community, but all we have is “microbial dark matter”. Instead, other statistics can demonstrate how effective our assembly and binning were.\nOne useful statistic is the N50 which will give an indication of the size of the contigs (fragments) each bin is made up. We looked at this statistic previously when we were quality checking our assemblies, using seqkit stats. We can do the same again for each of the bins.\n\n\nCode\n\nseqkit stats -a *.fa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfile\nformat\ntype\nnum_seqs\nsum_len\nmin_len\navg_len\nmax_len\nQ1\nQ2\nQ3\nsum_gap\nN50\nQ20(%)\nQ30(%)\nGC(%)\n\n\n\n\nbin.10.fa\nFASTA\nDNA\n5\n221,552\n22,334\n44,310.4\n65,450\n30,590\n38,514\n64,664\n0\n64,664\n0\n0\n57.31\n\n\nbin.11.fa\nFASTA\nDNA\n11\n319,827\n8,885\n29,075.2\n48,350\n25,427.5\n30,240\n34,944.5\n0\n30,309\n0\n0\n54.83\n\n\nbin.12.fa\nFASTA\nDNA\n13\n415,561\n13,844\n31,966.2\n72,402\n17,948\n29,809\n35,084\n0\n34,115\n0\n0\n69.98\n\n\nbin.13.fa\nFASTA\nDNA\n1\n206,331\n206,331\n206,331\n206,331\n103,165.5\n206,331\n103,165.5\n0\n206,331\n0\n0\n56.92\n\n\nbin.14.fa\nFASTA\nDNA\n8\n392,143\n22,611\n49,017.9\n85,177\n32,278.5\n49,048\n60,851\n0\n53,439\n0\n0\n59.51\n\n\nbin.15.fa\nFASTA\nDNA\n5\n317,530\n12,569\n63,506\n138,947\n51,122\n53,427\n61,465\n0\n61,465\n0\n0\n64.14\n\n\n\nThe table is quite long as we have 90 separate bins! It may also be hard to read depending on how your shell program formats and wraps the table columns. You might find it useful to copy and paste the table into a Google Sheets or Excel spreadsheet and use the ‘split text to columns’ feature to have a better look at your bins.\n\nGoogle Sheets\n\nCopy the data from the shell. Open a new Google Sheet and use Ctrl-v (or right click on the top left cell and select Paste) to paste the data into the sheet\nWith all of the leftmost column selected, go to Data &gt; Split text to columns.\nYour data should magically split so that each value has its own cell. If you are prompted to choose the type of separator, select ‘Space’ from the drop-down list.\n\nExcel\n\nCopy the data from the shell. Open a new spreadsheet and use Ctrl-v (or right click on the top left cell and select Paste) to paste the data into the sheet\nWith all of the leftmost column selected, go to Data &gt; Text to columns.\nA ‘Wizard’ popup will ask you to select either ‘Delimited’ or ‘Fixed width’ - choose ‘Fixed width’ and click Next.\nCheck that the column breaks are correct (they should be fine but worth checking anyway).\nClick Finish and your data should be split so each value has its own cell.\n\n\nYou can now peruse your bins at your leisure. In the next section we will be doing more analysis of our bins and deciding which are highest quality.\n\n\n\n\n\n\nRecommended reading:\n\n\n\nGenerating metagenome bins can be challenging, especially in complex community samples or where degradation of the DNA has resulted in a very incomplete assembly and short contig lengths. This workflow for binning might not work for you, and you might find that a different binning method might result in better refined MAGs for your dataset. There are lots of other binning software methods inlcuding:\n\nCONCOCT Clustering cOntigs with COverage and ComposiTion, the manual for running this is here\nMaxbin2 uses an expectation-maximization algorithm to form bins. The link to installing maxbin2 as a conda package is here\nThere are also tools that can combine different binning methods and use them to refine, DAS tool being one of them. DAS tool can also give completeness information, similarly to checkM.\nThis review gives a thorough look at the pros and cons of different tools used for generating MAGs and including binning .",
    "crumbs": [
      "Home",
      "Binning & Functional Annotation",
      "Metagenome Binning"
    ]
  },
  {
    "objectID": "docs/lesson04-polishing/index.html",
    "href": "docs/lesson04-polishing/index.html",
    "title": "Polishing",
    "section": "",
    "text": "Welcome back to this Metagenomics with High Performance Computing course!\nIn previous lessons we had an introduction to the command line and completed the first part of our worflow: we assessed the quality of long and short reads and used the long reads to generate a draft assembly.\nLong read data are great for tasks like this because they produce a less fragmented assembly and are more likely to span areas with repeats. However, they are also more likely to contain sequencing errors than short read data.\nWe must therefore use further tools to improve the quality of our draft assembly. We can “polish” our assembly using both long and short read data. After that, we can perform quality control (QC) checks to see what impact the polishing has had.\nBy the end of this lesson you will be able to:\n\nexplain what polishing is and why it is important\npolish a draft metagenome assembly with long reads using Medaka\npolish a draft metagenome assembly with short reads using Pilon\ncheck the quality of your draft assembly using Seqkit and metaQUAST\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Polishing"
    ]
  },
  {
    "objectID": "docs/lesson04-polishing/01-polishing-assembly.html",
    "href": "docs/lesson04-polishing/01-polishing-assembly.html",
    "title": "Polishing an assembly",
    "section": "",
    "text": "In the previous episode we generated a draft assembly using Flye from our long read Nanopore data.\nLong reads can span regions which make them difficult to assemble with short reads such as regions with large repeats. Despite this, some long reads will be misassembled. In addition, the base accuracy of long reads is lower than that of short reads and some bases will be incorrectly assigned. Consequently it is common to correct these errors by “polishing” an assembly. We will use two polishing strategies:",
    "crumbs": [
      "Home",
      "Polishing",
      "Polishing an assembly"
    ]
  },
  {
    "objectID": "docs/lesson04-polishing/01-polishing-assembly.html#why-bother-polishing",
    "href": "docs/lesson04-polishing/01-polishing-assembly.html#why-bother-polishing",
    "title": "Polishing an assembly",
    "section": "Why bother polishing?",
    "text": "Why bother polishing?\nHow important polishing is to your analysis will depend on what you need it for. Usually we generate metagenome assemblies so we can compare the sequences to a database and find out what taxa they belong to.\nYou might NOT need to polish your assembly if:\n\nyou only need to taxa to the genus level (meaning single incorrect bases are not important)\n\nYou DO need to polish your assembly if:\n\nyou want to identify taxa to the species level (if possible). This is a common requirement since one of the main advantages of whole genome sequencing over amplicon sequencing is that you can assign annotations to the species level. We will cover Taxonomic annotations later in the course.\nyou want to generate protein predictions or identify protein structure domains to determine the functionality of metagenomes. This is discussed in more detail in Watson and Warr (2019): Errors in long-read assemblies can critically affect protein prediction.",
    "crumbs": [
      "Home",
      "Polishing",
      "Polishing an assembly"
    ]
  },
  {
    "objectID": "docs/lesson04-polishing/01-polishing-assembly.html#polishing-an-assembly-with-long-reads",
    "href": "docs/lesson04-polishing/01-polishing-assembly.html#polishing-an-assembly-with-long-reads",
    "title": "Polishing an assembly",
    "section": "Polishing an assembly with long reads",
    "text": "Polishing an assembly with long reads\nFirst we will polish the draft Flye assembly using the filtered raw long reads. As with the assembly, we need to use polishing software that is especially written for long read raw reads.\nMedaka is a command line tool built by Oxford Nanopore Technologies which will polish an assembly by generating a consensus from raw Nanopore sequences using a recurrent neural network.\nWe will be using one Medaka command, medaka_consensus. This pipeline will first align the raw reads to the draft assembly, then process this alignment to generate a pileup. The pileup is presented to a recurrent neural network in order to produce a consensus sequence.\nMedaka is installed on the AWS instance. Let’s take a look at the help page for medaka_consensus:\n\n\nCode\n\nmedaka_consensus -h\n\n\n\n\n\n\n\nOutput --- medaka_consensus Help\n\n\n\n\n\nmedaka 1.7.2\n\nAssembly polishing via neural networks. Medaka is optimized\nto work with the Flye assembler.\n\nmedaka_consensus [-h] -i &lt;fastx&gt; -d &lt;fasta&gt;\n\n     -h  show this help text.\n     -i  fastx input basecalls (required).\n     -d  fasta input assembly (required).\n     -o  output folder (default: medaka).\n     -g  don't fill gaps in consensus with draft sequence.\n     -r  use gap-filling character instead of draft sequence (default: None)\n     -m  medaka model, (default: r941_min_hac_g507).\n         Choices: r103_fast_g507 r103_hac_g507 r103_min_high_g345 r103_min_high_g360 r103_prom_high_g360 r103_sup_g507 r1041_e82_400bps_fast_g615 r1041_e82_400bps_hac_g615 r1041_e82_400bps_sup_g615 r104_e81_fast_g5015 r104_e81_hac_g5015 r104_e81_sup_g5015 r104_e81_sup_g610 r10_min_high_g303 r10_min_high_g340 r941_e81_fast_g514 r941_e81_hac_g514 r941_e81_sup_g514 r941_min_fast_g303 r941_min_fast_g507 r941_min_hac_g507 r941_min_high_g303 r941_min_high_g330 r941_min_high_g340_rle r941_min_high_g344 r941_min_high_g351 r941_min_high_g360 r941_min_sup_g507 r941_prom_fast_g303 r941_prom_fast_g507 r941_prom_hac_g507 r941_prom_high_g303 r941_prom_high_g330 r941_prom_high_g344 r941_prom_high_g360 r941_prom_high_g4011 r941_prom_sup_g507 r941_sup_plant_g610\n         Alternatively a .tar.gz/.hdf file from 'medaka train'.\n     -f  Force overwrite of outputs (default will reuse existing outputs).\n     -x  Force recreation of alignment index.\n     -t  number of threads with which to create features (default: 1).\n     -b  batchsize, controls memory use (default: 100).\n\n\n\nTo use Medaka we need to specify certain parameters in the command, like we did when we ran Flye last session. The help page tells us that the basic format for medaka is medaka_consensus [-h] -i &lt;fastx&gt; -d &lt;fasta&gt;1, indicating that the -i and -d flags are mandatory.\nLet’s have a look at the flags and options we’re going to use:\n\n\n\nFlag/option\nMeaning\nOur input\n\n\n\n\n-i\nInput basecalls (i.e. what we are polishing with)\n-i data/nano_fastq/ERR5000342_sub12_filtered\n\n\n-d\nInput assembly (i.e. what is being polished)\n-d results/assembly/assembly.fasta\n\n\n-m\nNeural network model to use (described in the documentation)\n-m r941_min_hac_g507\n\n\n-o\nOutput directory\n-o results/medaka\n\n\n-t\nNumber of threads\n-t 8\n\n\n\nOnce again we will run this command in the background and redirect the output to a file. This means we add &&gt; to redirect the output and & to the very end to make it run in the background.\nBefore we start let’s make sure we’re in the cs_course folder and create a directory called medaka for our output in the results folder.\n\n\nCode\n\ncd ~/cs_course\nmkdir results/medaka\n\nNow it’s time to run Medaka!\n\n\nCode\n\nmedaka_consensus \\\n-i data/nano_fastq/ERR5000342_sub12_filtered.fastq \\\n-d results/assembly/assembly.fasta \\\n-m r941_min_hac_g507 \\\n-o results/medaka \\\n-t 8 \\\n&&gt; results/medaka/medaka.out &\n\nWe can check the command is running using jobs:\n\n\nCode\n\njobs\n\nIf it is successfully running you should see an output like:\n\n\nOutput\n\n[1]+  Running                 medaka_consensus -i data/nano_fastq/ERR3152367_sub5_filtered.fastq -d results/assembly/assembly.fasta -m r941_min_hac_g507 -o results/medaka -t 8 &&gt; results/medaka/medaka.out &\n\nWe can also look in the output file (medaka.out) to check the progress of the command. In this case Medaka will take about three hours to run.\n\n\nCode\n\nless results/medaka/medaka.out\n\nIf the Medaka command has been run correctly you will see something like this near the start of the output:\n\n\nOutput\n\nChecking program versions\nThis is medaka 1.7.2\nThis is medaka 1.7.2\nProgram    Version            Required   Pass\nbcftools   1.16-20-g6f4732b   1.11       True\nbgzip      1.16-16-g3c6f83f   1.11       True\nminimap2   2.24               2.11       True\nsamtools   1.16.1-33-gbd942a0 1.11       True\ntabix      1.16-16-g3c6f83f   1.11       True\nAligning basecalls to draft\nConstructing minimap index.\n[M::mm_idx_gen::0.515*0.99] collected minimizers\n[M::mm_idx_gen::0.648*1.40] sorted minimizers\n[M::main::0.877*1.29] loaded/built the index for 146 target sequence(s)\n[M::mm_idx_stat] kmer size: 15; skip: 10; is_hpc: 0; #seq: 146\n[M::mm_idx_stat::0.910*1.28] distinct minimizers: 2598367 (94.68% are singletons); average occurrences: 1.076; average spacing: 5.350; total length: 14953273\n\n\n\n\n\n\n\nHelp!\n\n\n\nMedaka may give you a warning along the lines of:\n\n\nOutput\n\n2023-03-31 12:22:21.572954: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-03-31 12:22:21.573012: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-03-31 12:22:24.690162: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Couldnot load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-03-31 12:22:24.690198: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n\nDon’t worry, you can safely ignore this warning wherever it pops up. It is telling us that it couldn’t load a library required for parallel computing using GPUs. We are not using a GPU setup and so this warning is irrelevant.\n\n\n\n\n\n\n\n\nHelp!\n\n\n\nIf you have entered the command incorrectly you will usually find out quite quickly! An easy way to tell is if you get an output in your terminal that starts with ‘Exit’ rather than ‘Done’:\n\n\nOutput\n\n[1]+  Exit 1                  medaka_consensus -i data/nano_fastq/ERR5000342_sub12_filtered.fastq -d results/assembly/assembly.fasta -m r941_min_hac_g507 -o results/medaka -t 8 & results/medaka/medaka.out\n\nAnother way to tell is by looking inside the medaka.out log file. You will get some of the usual text output as Medaka loads its dependencies but once it starts processing the command it will tell you that something is wrong, e.g.\n\n\nOutput\n\nCreating fai index file /home/csuser/cs_course/results/assembly/assembly.fasta.fai\n[E::fai_build3_core] Failed to open the file /home/csuser/cs_course/results/assembly/assembly.fasta\n[faidx] Could not build fai index /home/csuser/cs_course/results/assembly/assembly.fasta.fai\nFailed to run alignment of reads to draft.\n\nThings to check if your command fails:\n\nspelling/typos — have you spelled the command correctly and typed the directory names without error?\npaths — are all of your absolute/relative paths complete and accurate? (tip: using tab completion can help with peace of mind here, as you can check that your path definitely follows the right trail)\nflags — have you included all the mandatory flags, including the one (-) or two (--) dashes that usually accompany them?\noutput — does the output in your log/.out file give you any clues as to where your error lies? e.g. in the example above the line “Failed to open the file /home/csuser/cs_course/results/assembly/assembly.fasta” suggests that this path might be wrong somehow.\n\nIf you really can’t find your mistake then the your instructors and/or other participants will be able to help you sort it out, so there is no need to panic!\n\n\nMedaka first looks for the other programs that it needs (known as dependencies) and their versions. These dependencies are installed on the AWS instance. Once it confirms they are present, it begins by aligning the raw reads (basecalls) to the assembly using minimap.\nOnce Medaka has completed the end of the file (which you can skip to by typing G) will contain something like:\n\n\nCode\n\nless results/medaka/medaka.out\n\n\n\nOutput\n\n[20:51:16 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[20:51:16 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[20:51:16 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[20:51:16 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[20:51:16 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[20:51:16 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[20:51:16 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[20:51:17 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[20:51:17 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[20:51:17 - DataIndx] Loaded 1/1 (100.00%) sample files.\nPolished assembly written to medaka/consensus.fasta, have a nice day.\n\nOnce Medaka has completed (~ 3 hours on average) we can take a look at the files it has generated.\n\n\nCode\n\ncd results/medaka\nls\n\n\n\nOutput\n\ncalls_to_draft.bam  calls_to_draft.bam.bai  consensus.fasta  consensus.fasta.gaps_in_draft_coords.bed  consensus_probs.hdf\n\nMedaka has created multiple files:\n\ncalls_to_draft.bam - a BAM file containing the alignment of the raw reads (basecalls) to the draft assembly\ncalls_to_draft.bam.bai — an index file of the above BAM file\nconsensus.fasta — the consensus sequence, or polished assembly in our case in FASTA format\nconsensus.fasta.gaps_in_draft_coords.bed — a BED file containing information about the location of any gaps in the consensus sequence which can be used when visualising the assembly\nconsensus_probs.hdf — a file that contains the output of the neural network calculations and is not an output for end-users, so we don’t need to worry about this file\n\nIn our case we’re interested in the polished assembly, so we want the consensus.fasta file.\n\n\n\n\n\n\nBAM and SAM Files\n\n\n\nA SAM file, is a tab-delimited text file that contains information for each individual read and its alignment to the genome. The paper by Heng Li et al. provides the full specification.\nThe compressed binary version of SAM is called a BAM file. We use this version to reduce size and to allow for indexing, which enables efficient random access of the data contained within the file.\nThe file begins with a header, which is optional. The header describes the source of data, reference sequence, method of alignment etc. - these will change depending on the aligner being used. Following the header is the alignment section. Each line that follows corresponds to alignment information for a single read. There are 11 mandatory fields for essential mapping information and a variable number of other fields for aligner specific information.\nSee Genomics - Variant Calling for a deeper dive.",
    "crumbs": [
      "Home",
      "Polishing",
      "Polishing an assembly"
    ]
  },
  {
    "objectID": "docs/lesson04-polishing/01-polishing-assembly.html#polishing-with-short-reads-1",
    "href": "docs/lesson04-polishing/01-polishing-assembly.html#polishing-with-short-reads-1",
    "title": "Polishing an assembly",
    "section": "Polishing with short reads",
    "text": "Polishing with short reads\nWe will be using the program Pilon to further polish the draft assembly using the raw short reads. Pilon will improve a draft assembly by filling gaps, fixing misassemblies and correcting bases. You can read more about how it works in the paper Pilon: An Integrated Tool for Comprehensive Microbial Variant Detection and Genome Assembly Improvement.\nBioinformatics programs are not built equally. Some programs, like Flye or Medaka, require very few input files as they will generate any that they need within the pipeline. Some programs however, require a lot of user input to generate the input files that are needed.\nPilon is in the latter group of bioinformatics software, so we will need to do some pre-processing using other programs to create some of the inputs needed.\n\nGenerating the Pilon input files\nWe will first use the program BWA to generate an alignment of the raw short reads against the draft genome in consensus.fasta. The steps will be:\n\nindexing the polished assembly, consensus.fasta with bwa index. Indexing allows the aligner to quickly find potential alignment sites for query sequences in a genome, which saves time during alignment.\ncreating a directory for the outputs of Pilon\naligning the short reads (the illumina data) to the assembly, consensus.fasta with bwa mem\nconverting the short read alignment to a BAM file samtools view\nsorting the short read alignment with samtools sort\nindexing the short read alignment with samtools index\n\nMake sure you are in the cs_course folder and use bwa index to index the consensus assembly:\n\n\nCode\n\ncd ~/cs_course\nbwa index results/medaka/consensus.fasta\n\nThis should only take a few seconds to complete so we don’t need to run the job in the background. Once the indexing is complete you should see an output like:\n\n\nOutput\n\n[bwa_index] Pack FASTA... 0.51 sec\n[bwa_index] Construct BWT for the packed sequence...\n[bwa_index] 5.86 seconds elapse.\n[bwa_index] Update BWT... 0.10 sec\n[bwa_index] Pack forward-only FASTA... 0.10 sec\n[bwa_index] Construct SA from BWT and Occ... 1.81 sec\n[main] Version: 0.7.17-r1188\n[main] CMD: bwa index consensus.fasta\n[main] Real time: 8.704 sec; CPU: 8.395 sec\n\nThis will generate five additional files in the medaka directory with the file extensions .pac, .bwt, .ann, .amb and .sa. These files are used by BWA in step 3.\nNext we will make a directory in the results directory for our pilon polishing outputs:\n\n\nCode\n\nmkdir results/pilon\n\nWe will now do steps 3, 4 and 5 in one go by chaining them together with pipes.\n\n\n\n\n\n\nChaining together commands with a pipe\n\n\n\nIt is possible to chain together commands in Linux (and Unix) using a command known as “pipe”. This allows the output from one command to be directly passed as input to other command without the need for using intermediate files. This is useful when the intermediate file is not needed and keeps your workspace tidy. The pipe command is the character | which is obtained with ⇧ Shift + \\ on most keyboards.\nYou can use multiple pipes in a single line of commands but data will only go from the left to the right:\ncommand1 | command2 | command3 | .... |\n\n\nWe will be using two pipes to join three separate steps. First we will align the raw reads to the draft assembly, then convert the output to BAM format, before finally sorting this alignment to generate a sorted BAM file. Chaining the steps together together will only generate one final output file, avoiding the need to generate large intermediary files we don’t need again between the other two steps.\n\nwe will align the short reads (the illumina data) to the assembly, consensus.fasta with bwa mem: bwa mem -t 8 results/medaka/consensus.fasta data/illumina_fastq/ERR4998593_1.fastq data/illumina_fastq/ERR4998593_2.fastq\nconvert the short read alignment alignment to a BAM file with samtools view: samtools view - -Sb\nsort the short read alignment with samtools sort: samtools sort - -@4 -o pilon/short_read_alignment.bam\n\nHere are the various flags/options used in these commands and what they mean:\n\n\n\n\n\n\n\n\n\nCommand\nFlag/option\nMeaning\n\n\n\n\nbwa mem -t 8 [input assembly] [input short read file(s)]\n-t 8\nNumber of threads (8)\n\n\nsamtools view - -Sb\n-\nTake piped output from bwa mem as input\n\n\n\n-Sb\nConvert from SAM to BAM format\n\n\nsamtools sort - -@8 -o [filename]\n-\nTake piped output from samtools view as input\n\n\n\n-@8\nNumber of threads (8)\n\n\n\n-o [filename]\nOutput a file with name [filename]\n\n\n\nThis will take around 60 minutes so we will use redirection and & to print the output to a file and run the command in the background (just like in the previous lesson). We will also wrap our whole three-fold command in brackets so we run all three steps in the background.\nAdd the pipes between these commands and run:\n\n\nCode\n\n(bwa mem -t 8 results/medaka/consensus.fasta data/illumina_fastq/ERR4998593_1.fastq data/illumina_fastq/ERR4998593_2.fastq | samtools view - -Sb | samtools sort - -@8 -o results/pilon/short_read_alignment.bam) &&gt; results/pilon/alignment.out &\n\nOnce the command is running, you can check the process of this job by looking at the alignment.out file.\n\n\nCode\n\nless results/pilon/alignment.out\n\n\n\nOutput\n\n[M::bwa_idx_load_from_disk] read 0 ALT contigs\n[M::process] read 529802 sequences (80000102 bp)...\n[M::process] read 529802 sequences (80000102 bp)...\n[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (2, 14035, 29, 8)\n[M::mem_pestat] skip orientation FF as there are not enough pairs\n[M::mem_pestat] analyzing insert size distribution for orientation FR...\n[M::mem_pestat] (25, 50, 75) percentile: (333, 416, 538)\n[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 948)\n[M::mem_pestat] mean and std.dev: (435.53, 162.74)\n[M::mem_pestat] low and high boundaries for proper pairs: (1, 1153)\n[M::mem_pestat] analyzing insert size distribution for orientation RF...\n[M::mem_pestat] (25, 50, 75) percentile: (891, 2304, 6802)\n[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 18624)\n[M::mem_pestat] mean and std.dev: (3479.45, 2918.28)\n\nThe command should take around 45 minutes to run. Once completed, the end of the alignment.out file should contain something like:\n\n\nOutput\n\nlow and high boundaries for proper pairs: (1, 1222)\n[M::mem_pestat] skip orientation RF as there are not enough pairs\n[M::mem_pestat] skip orientation RR as there are not enough pairs\n[M::mem_process_seqs] Processed 182762 reads in 37.961 CPU sec, 5.397 real sec\n[main] Version: 0.7.17-r1188\n[main] CMD: bwa mem -t 8 ~/cs_course/results/medaka/consensus.fasta ~/cs_course/data/illumina_fastq/ERR4998593_1.fastq ~/cs_course/data/illumina_fastq/ERR4998593_2.fastq\n[main] Real time: 2207.540 sec; CPU: 14851.383 sec\n[bam_sort_core] merging from 7 files and 4 in-memory blocks...\n\nWe have now generated the short_read_alignment.bam file - this is a binary file (meaning it’s not human readable) so we won’t be checking its contents.\nNow carry out step 6, index the alignment:\n\n\nCode\n\nsamtools index results/pilon/short_read_alignment.bam\n\n\n\n\n\n\n\nSomething to think about\n\n\n\nWhy didn’t we include this command in the sequence of pipes in the previous step?\nThe answer is that we will need access to the BAM file produced for further analysis. If we included this step as part of a pipe the intermediate BAM file would not be saved.\n\n\nThis command will take around one or two minutes so we don’t need to run it in the background.\nOnce your prompt has returned you should also have a file named short_read_alignment.bam.bai in your pilon directory, which is the index.\n\n\nRunning Pilon\nNow we have generated the necessary input files we can finally run Pilon.\nPilon is installed on the AWS instance and we can view the help documentation using:\n\n\nCode\n\npilon --help\n\n\n\n\n\n\n\nOutput — Pilon help Documentation\n\n\n\n\n\nPilon version 1.24 Thu Jan 28 13:00:45 2021 -0500\n\n   Usage: pilon --genome genome.fasta [--frags frags.bam] [--jumps jumps.bam] [--unpaired \n                unpaired.bam]  [...other options...]\n          pilon --help for option details\n\n\n         INPUTS:\n           --genome genome.fasta\n              The input genome we are trying to improve, which must be the reference used\n              for the bam alignments.  At least one of --frags or --jumps must also be given.\n           --frags frags.bam\n              A bam file consisting of fragment paired-end alignments, aligned to the --genome\n              argument using bwa or bowtie2.  This argument may be specified more than once.\n           --jumps jumps.bam\n              A bam file consisting of jump (mate pair) paired-end alignments, aligned to the\n              --genome argument using bwa or bowtie2.  This argument may be specified more than once.\n           --unpaired unpaired.bam\n              A bam file consisting of unpaired alignments, aligned to the --genome argument\n              using bwa or bowtie2.  This argument may be specified more than once.\n           --bam any.bam\n              A bam file of unknown type; Pilon will scan it and attempt to classify it as one\n              of the above bam types.\n           --nanopore ont.bam\n              A bam file containing Oxford Nanopore read alignments. Experimental.\n           --pacbio pb.bam\n              A bam file containing Pacific Biosciences read alignments. Experimental.\n         OUTPUTS:\n           --output prefix\n              Prefix for output files\n           --outdir directory\n              Use this directory for all output files.\n           --changes\n              If specified, a file listing changes in the &lt;output&gt;.fasta will be generated.\n           --vcf\n              If specified, a vcf file will be generated\n           --vcfqe\n               If specified, the VCF will contain a QE (quality-weighted evidence) field rather\n               than the default QP (quality-weighted percentage of evidence) field.\n           --tracks\n               This options will cause many track files (*.bed, *.wig) suitable for viewing in\n               a genome browser to be written.\n         CONTROL:\n           --variant\n              Sets up heuristics for variant calling, as opposed to assembly improvement;\n              equivalent to \"--vcf --fix all,breaks\".\n           --chunksize\n              Input FASTA elements larger than this will be processed in smaller pieces not to\n              exceed this size (default 10000000).\n           --diploid\n              Sample is from diploid organism; will eventually affect calling of heterozygous SNPs\n           --fix fixlist\n              A comma-separated list of categories of issues to try to fix:\n                \"snps\": try to fix individual base errors;\n                \"indels\": try to fix small indels;\n                \"gaps\": try to fill gaps;\n                \"local\": try to detect and fix local misassemblies;\n                \"all\": all of the above (default);\n                \"bases\": shorthand for \"snps\" and \"indels\" (for back compatibility);\n                \"none\": none of the above; new fasta file will not be written.\n              The following are experimental fix types:\n                \"amb\": fix ambiguous bases in fasta output (to most likely alternative);\n                \"breaks\": allow local reassembly to open new gaps (with \"local\");\n                \"circles\": try to close circular elements when used with long corrected reads;\n                \"novel\": assemble novel sequence from unaligned non-jump reads.\n           --dumpreads\n              Dump reads for local re-assemblies.\n           --duplicates\n              Use reads marked as duplicates in the input BAMs (ignored by default).\n           --iupac\n              Output IUPAC ambiguous base codes in the output FASTA file when appropriate.\n           --nonpf\n              Use reads which failed sequencer quality filtering (ignored by default).\n           --targets targetlist\n              Only process the specified target(s).  Targets are comma-separated, and each target\n              is a fasta element name optionally followed by a base range.\n              Example: \"scaffold00001,scaffold00002:10000-20000\" would result in processing all of \n              scaffold00001 and coordinates 10000-20000 of scaffold00002.\n              If \"targetlist\" is the name of a file, each line will be treated as a target\n              specification.\n           --verbose\n              More verbose output.\n           --debug\n              Debugging output (implies verbose).\n           --version\n              Print version string and exit.\n         HEURISTICS:\n           --defaultqual qual\n              Assumes bases are of this quality if quals are no present in input BAMs (default 10).\n           --flank nbases\n              Controls how much of the well-aligned reads will be used; this many bases at each\n              end of the good reads will be ignored (default 10).\n           --gapmargin\n              Closed gaps must be within this number of bases of true size to be closed (100000)\n           --K\n              Kmer size used by internal assembler (default 47).\n           --mindepth depth\n              Variants (snps and indels) will only be called if there is coverage of good pairs\n              at this depth or more; if this value is &gt;= 1, it is an absolute depth, if it is a\n              fraction &lt; 1, then minimum depth is computed by multiplying this value by the mean\n              coverage for the region, with a minumum value of 5 (default 0.1: min depth to call\n              is 10% of mean coverage or 5, whichever is greater).\n           --mingap\n              Minimum size for unclosed gaps (default 10)\n           --minmq\n              Minimum alignment mapping quality for a read to count in pileups (default 0)\n           --minqual\n              Minimum base quality to consider for pileups (default 0)\n           --nostrays\n              Skip making a pass through the input BAM files to identify stray pairs, that is,\n              those pairs in which both reads are aligned but not marked valid because they have\n              inconsistent orientation or separation. Identifying stray pairs can help fill gaps\n              and assemble larger insertions, especially of repeat content.  However, doing so\n              sometimes consumes considerable memory.\n\n\n\nYou can read more about the possible outputs Pilon can produce in the Wiki.\nWe can see there are many different options for pilon. We will be using the defaults for our assembly.\n\n--genome — this will be the output assembly from Medaka\n--frags — the short reads we used to create the BAM alignment were paired-end fragments, so we need to specify this using this flag\n--outdir — this specifies a directory for all the output\n\nCheck you are in the cs_course folder and run Pilon:\n\n\nCode\n\ncd ~/cs_course\npilon --genome results/medaka/consensus.fasta --frags results/pilon/short_read_alignment.bam --outdir results/pilon &&gt; results/pilon/pilon.out &\n\nWe can again keep track of the analysis by looking at the pilon.out file with less.\n\n\nCode\n\nless results/pilon/pilon.out\n\nThe top of the file:\n\n\nOutput\n\nPilon version 1.24 Thu Jan 28 13:00:45 2021 -0500\nGenome: medaka/consensus.fasta\nFixing snps, indels, gaps, local\nInput genome size: 18930486\nScanning BAMs\nshort_read_alignment.bam: 68579742 reads, 0 filtered, 7344428 mapped, 6036898 proper, 167134 stray, FR 100% 401+/-206, max 1018\nProcessing contig_1057:1-17080\nfrags short_read_alignment.bam: coverage 36\nTotal Reads: 9147, Coverage: 36, minDepth: 5\nConfirmed 10586 of 17080 bases (61.98%)\nCorrected 234 snps; 40 ambiguous bases; corrected 22 small insertions totaling 38 bases, 101 small deletions totaling 141 bases\n# Attempting to fix local continuity breaks\n# fix break: contig_1057:780-2830 0 -0 +0 NoSolution\n# fix break: contig_1057:3081-4075 0 -0 +0 NoSolution\n# fix break: contig_1057:4312-5132 0 -0 +0 NoSolution\n# fix break: contig_1057:5367-5452 0 -0 +0 NoSolution\n# fix break: contig_1057:5798-6336 0 -0 +0 NoSolution\n# fix break: contig_1057:6798-8683 0 -0 +0 NoSolution\n# fix break: contig_1057:8886-9132 0 -0 +0 NoSolution\n# fix break: contig_1057:9394-12244 0 -0 +0 NoSolution\n\nWhen Pilon finishes (around 1.5 hours) the end of the file will contain something like:\n\n\nOutput\n\nWriting updated contig_1125_pilon to ./pilon.fasta\nWriting updated contig_885_pilon to ./pilon.fasta\nWriting updated contig_1130_pilon to ./pilon.fasta\nWriting updated contig_1121_pilon to ./pilon.fasta\nWriting updated contig_697_pilon to ./pilon.fasta\nMean frags coverage: 30\nMean total coverage: 30\n\nNavigate into the pilon directory and have a look at the output files Pilon has produced.\n\n\nCode\n\ncd results/pilon\nls\n\n\n\nOutput\n\nalignment.out  pilon.fasta short_read_alignment.bam  short_read_alignment.bam.bai\n\nWe can see pilon has produced a fasta file pilon.fasta, which is the newly polished assembly. This file is now our assembly.\nIn the next episode we will assess the quality of this assembly and compare its quality to that of the unpolished assembly.\n\n\n\n\n\n\nRecommended reading:\n\n\n\nWhile you’re waiting for the polishing to finish, here’s some things you might want to read about:\n\nComparison of combined assembly and polishing methods Trycycler: consensus long-read assemblies for bacterial genomes\nPolishing strategy for ONT and Pacbio Hifi reads Polishing high-quality genome assemblies\nComparison of polishing of ONT data with alignment free tool Jasper compared to POLCA, NextPolish and ntEdit JASPER: a fast genome polishing tool that improves accuracy and creates population-specific reference genomes\nComparison of short read polishers including pilon to the polisher Polypolish Polypolish: Short-read polishing of long-read bacterial genome assemblies\nPilon short read polisher paper Pilon: An Integrated Tool for Comprehensive Microbial Variant Detection and Genome Assembly Improvement\nAccuracy of polishers including medaka for nanopore data Nanpore consensus quality\nComparison of nanopore polishing tools Comparative evaluation of Nanopore polishing tools for microbial genome assembly and polishing strategies for downstream analysis",
    "crumbs": [
      "Home",
      "Polishing",
      "Polishing an assembly"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/02-logging-onto-cloud.html",
    "href": "docs/lesson01-files-and-directories/02-logging-onto-cloud.html",
    "title": "Logging onto the Cloud",
    "section": "",
    "text": "This lesson covers how to log into, and out of, an already running AWS instance.",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Logging onto the Cloud"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/02-logging-onto-cloud.html#background-to-aws",
    "href": "docs/lesson01-files-and-directories/02-logging-onto-cloud.html#background-to-aws",
    "title": "Logging onto the Cloud",
    "section": "Background to AWS",
    "text": "Background to AWS\nAn Amazon Web Services (AWS) instance is a remote computer that runs on AWS infrastructure and that is accessible from any laptop or desktop as described below. For this course, an AWS instance has been created for you by the Cloud-SPAN team.\nRemote computers are also called cloud computers. They allow you to access computers with many more resources (memory, processors, disk space etc) than the average laptop or PC. This is useful when you want to run analyses that use lots of data and/or take a lot of computational power to run.\nTo login into your AWS instance for this course, you’ll need:\n\nthe name of your instance and a login key file, both of which you received via email\nthe shell/terminal application — Windows users should have already installed the Git Bash shell; otherwise follow the Precourse Instructions.\nthe secure shell (ssh) application, which is readily available in MacOS and Linux. Windows users will use ssh through Git Bash.",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Logging onto the Cloud"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/02-logging-onto-cloud.html#open-a-terminal-and-change-the-access-permissions-of-your-login-key-file",
    "href": "docs/lesson01-files-and-directories/02-logging-onto-cloud.html#open-a-terminal-and-change-the-access-permissions-of-your-login-key-file",
    "title": "Logging onto the Cloud",
    "section": "Open a Terminal and change the access permissions of your login key file",
    "text": "Open a Terminal and change the access permissions of your login key file\nThe first thing we need to do is change the access permissions of the login key file so it can be used safely.\n\n1. Open the cloudspan folder you created for the course\nOpen your file manager and navigate to the cloudspan folder (hint: we recommended you make the folder in your Desktop directory - but you might have made it somewhere else). If you cannot find the folder, you can remind yourself where it is stored by looking at the absolute path you wrote down in the previous episode.\nThe folder should contain the login key file we downloaded in the previous episode and nothing else.\n\n\n2. Right-click and open your machine’s command line interface\nNow we can open the command line.\n\nWindows users:\n\nRight click anywhere inside the blank space of the file manager, then select Git Bash Here. A new window will open - this is your command line interface, also known as the shell or the terminal. It will automatically open with your cloudspan directory as the working directory.\n\nMac users, you have two options:\n\nEITHER: Open Terminal in one window and type cd followed by a space. Do not press enter! Now open Finder in another window. Drag and drop the cloudspan folder from the Finder to the Terminal. You should see the file path leading to your cloudspan folder appear. Now press enter to navigate to the folder.\nOR: Open Terminal and type cd followed by the absolute path that leads to your cloudspan folder. Press enter.\n\n\nThe terminal displays/outputs the command prompt to signal that it is ready to accept commands (instructions). The command prompt is 1 or 2 lines depending on your operating system (Windows, Linux, MacOS) and will be similar to the following.\nTypical command prompt for Windows Git Bash users:\nusername@machineid MINGW64 ~\n$\nTypical command prompt for Linux users:\nusername@machineid:~ $\nTypical command prompt for MacOS users:\nmachineid:~ username $\nObviously “username” and “machineid” in the prompt shown above will be different and will correspond to the actual username and the name of the machine you are using.\nThe character $ is the typical ending of user prompts (the ending of admin users prompts is typically #). Commands you type will follow the $.\n\n\n\n\n\n\nIMPORTANT\n\n\n\nIn this course we will NOT show the prompt in the Code boxes so that it is easier for you to copy long commands. You can copy the commands in a Code box by clicking on the green icon on the right of the Code box.\n\n\n\n\n3. Check that you are in the right folder\nThe terminal should have automatically set our cloudspan folder as the current working directory. This is because we asked the terminal to open from a specific location.\nYou can check if the working directory is set correctly by looking at the file path which is defined to the left of your command prompt ($). It should display the second half of the absolute path we wrote down previously, usually starting after your computer’s username, and always ending in /cloudspan.\nYou can also check by typing the letters ls after the command prompt and pressing enter. This will list all the files in the working directory AKA all files in the cloudspan folder. In our case, this should be just one file, the login key ending in .pem.\n\n\n4. Change the access permissions of your login key file\nEnter the following command to change the access permissions of your file but replace NNN with the actual number in your file name:\n\n\nCode\n\nchmod 400 login-key-instanceNNN.pem \n\nThe command chmod (change access mode) makes your login key file accessible to you only (and non-accessible to any other potential users of your computer), a condition that is required and checked by the program ssh that you will use next to log in to your AWS instance. You will learn about file access permissions later in the course.",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Logging onto the Cloud"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/02-logging-onto-cloud.html#login-into-your-instance-with-ssh",
    "href": "docs/lesson01-files-and-directories/02-logging-onto-cloud.html#login-into-your-instance-with-ssh",
    "title": "Logging onto the Cloud",
    "section": "Login into your instance with ssh",
    "text": "Login into your instance with ssh\nNow it’s time to log in to the Cloud! You do this with a secure shell protocol or ssh. As the name implies, ssh provides you with a secure (encrypted) way to use a remote shell.\nA few seconds after you enter that command to the shell in your computer, you will be logged into your AWS instance and start using a (Linux) shell running in your instance.\n\n1. Copy and paste the command in the Code box below to your terminal, but replace NNN with the number in your login key file name.\n\n\n\n\n\n\nPasting in the shell\n\n\n\nYou can copy the command from the course materials by clicking the clipboard at the right of the code box.\nHowever, you cannot paste in the shell using Ctrl + V like you can usually. Instead you have two options:\n\nright click and select paste\nhover the mouse pointer over the terminal window and press the mouse middle button\n\n\n\n\n\nCode\n\nssh -i login-key-instanceNNN.pem csuser@instanceNNN.cloud-span.aws.york.ac.uk\n\nBe sure to replace NNN twice with your unique instance number (the one in your key). You can use the left and right arrow keys to move to where NNN is.\nThe -i option tells ssh where to find the key which proves you have permission to connect to the instance.\n\n\n2. The terminal will display a security message, after you enter the ssh command, similar to the message below:\n\n\nOutput\n\nThe authenticity of host instanceNNN-cloud-span.aws.york.ac.uk (52.211.132.120) can't be\nestablished. ECDSA key fingerprint is SHA256:8N054prkkCeM4GCDSsa0AUnSQw5ngBQHbOR40FqfqLg.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\n\nType yes to continue and get connected to your AWS instance.\nAfter logging in, you will see a screen showing something like this:\n\n\nOutput\n\nWelcome to Ubuntu 20.04.3 LTS (GNU/Linux 5.4.0-84-generic x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\n\n  System information as of Thu 21 Oct 2021 10:47:55 AM UTC\n\n  System load:  1.68               Processes:             189\n  Usage of /:   24.0% of 98.30GB   Users logged in:       0\n  Memory usage: 25%                IPv4 address for eth0: 10.0.32.254\n  Swap usage:   0%\n\n  Get cloud support with Ubuntu Advantage Cloud Guest:\n    http://www.ubuntu.com/business/services/cloud\n\n73 updates can be applied immediately.\n32 of these updates are standard security updates.\nTo see these additional updates run: apt list --upgradable\n\nUbuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\napplicable law.\n    _____________________________\n\n    W E L C O M E    T O    T H E\n\n\n     ____ _                 _         ______ _____   _    __   _\n    / ___| | ___  _   _  __| |       / ____ |  _  \\ / \\  |  \\ | |\n   | |   | |/ _ \\| | | |/ _` |  ___  \\___  \\| |_) '/ _ \\ | \\ \\| |\n   | |___| | (_) | |_| | (_| | |___| ____)  |  __ / ___ \\| |\\ | |\n    \\____|_|\\___/ \\___/ \\__,_|       \\_____/|_|  /_/   \\_|_| \\__|\n\n\n\n    F O U N D A T I O N     C O U R S E     E N V I R O N M E N T\n\n    _____________________________________________________________\n\n    Scroll up with the mouse for information before this welcome\n\n    Type \"csguide\" (and the Enter (↵) key) for some guidance\n    _____________________________________________________________\n\nLast login: Thu Oct 14 11:13:28 2021 from xxxxxxx\n\nYour prompt will now look like this:\ncsuser@instanceNNN:~ $\nNote that you did not need to give a password to log in to your instance – you are using your login key file for authentication.",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Logging onto the Cloud"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/02-logging-onto-cloud.html#logging-off-your-cloud-instance",
    "href": "docs/lesson01-files-and-directories/02-logging-onto-cloud.html#logging-off-your-cloud-instance",
    "title": "Logging onto the Cloud",
    "section": "Logging off your cloud instance",
    "text": "Logging off your cloud instance\nWhen you are finished using the cloud instance and want to return to your local terminal, you can log off. Be aware that AWS instances accrue charges whenever they are running, even if you are logged off. Today, however, you do not need to worry about this!\nTo log off, use the exit command in the same terminal you connected with. This will close the connection, and your terminal will go back to showing your local computer prompt, for example:\n\n\nCode\n\nexit\n\n\n\nOutput\n\nlogout\nConnection to instance05-gc.cloud-span.aws.york.ac.uk closed.\nusername@machineid $",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Logging onto the Cloud"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/02-logging-onto-cloud.html#subsequent-logins-to-your-aws-instance",
    "href": "docs/lesson01-files-and-directories/02-logging-onto-cloud.html#subsequent-logins-to-your-aws-instance",
    "title": "Logging onto the Cloud",
    "section": "Subsequent logins to your AWS instance",
    "text": "Subsequent logins to your AWS instance\nTo login back to your instance, open a terminal, make sure you are in your cloudspan folder and ssh as before:\n\n\nCode\n\nssh -i login-key-instanceNNN.pem csuser@instanceNNN.cloud-span.aws.york.ac.uk",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Logging onto the Cloud"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/03-shell-introduction.html",
    "href": "docs/lesson01-files-and-directories/03-shell-introduction.html",
    "title": "Introducing the Shell",
    "section": "",
    "text": "A shell is a computer program that has a command line where you type commands to do things on your computer rather than using menus and buttons on a Graphical User Interface (GUI).\nThere are many reasons to learn how to use the shell/command line:\n\nSoftware access - many bioinformatics tools can only be used through a command line interface, or have extra capabilities in the command line version that are not available in the GUI (this is true of most of the software used in this course).\nCloud access - bioinformatics tasks which require large amounts of computing power (like the ones we’ll do later in this course!) are best performed on remote computers or cloud computing platforms, which are accessed via a shell.\nAutomation - repetitive tasks (e.g. doing the same set of tasks on a large number of files) can be easily automated in the shell, saving you time and preventing human error.\nReproducibility - when using the shell your computer keeps a record of every step that you’ve carried out, which you can use to re-do your work when you need to.\n\nIn this lesson you will learn how to use the command line interface to move around in your file system.",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Introducing the Shell"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/03-shell-introduction.html#what-is-a-shell-and-why-should-i-care",
    "href": "docs/lesson01-files-and-directories/03-shell-introduction.html#what-is-a-shell-and-why-should-i-care",
    "title": "Introducing the Shell",
    "section": "",
    "text": "A shell is a computer program that has a command line where you type commands to do things on your computer rather than using menus and buttons on a Graphical User Interface (GUI).\nThere are many reasons to learn how to use the shell/command line:\n\nSoftware access - many bioinformatics tools can only be used through a command line interface, or have extra capabilities in the command line version that are not available in the GUI (this is true of most of the software used in this course).\nCloud access - bioinformatics tasks which require large amounts of computing power (like the ones we’ll do later in this course!) are best performed on remote computers or cloud computing platforms, which are accessed via a shell.\nAutomation - repetitive tasks (e.g. doing the same set of tasks on a large number of files) can be easily automated in the shell, saving you time and preventing human error.\nReproducibility - when using the shell your computer keeps a record of every step that you’ve carried out, which you can use to re-do your work when you need to.\n\nIn this lesson you will learn how to use the command line interface to move around in your file system.",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Introducing the Shell"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/03-shell-introduction.html#how-to-access-the-shell",
    "href": "docs/lesson01-files-and-directories/03-shell-introduction.html#how-to-access-the-shell",
    "title": "Introducing the Shell",
    "section": "How to access the shell",
    "text": "How to access the shell\nWe have already accessed the shell in the previous episode, when we logged onto the Cloud instance.\nWe will spend most of our time learning about the basics of the shell by manipulating some experimental data. Some of the data we’re going to be working with is quite large, and we’re also going to be using several bioinformatic packages in later lessons to work with this data. To avoid having to spend time downloading the data and downloading and installing all of the software, we’re going to continue using our Cloud instance.\nAs a reminder, we log in by launching Git Bash or Terminal from the cloudspan folder we made in today’s first episode, and then using the ssh command.\n\n\nCode\n\nssh -i login-key-instanceNNN.pem csuser@instanceNNN.cloud-span.aws.york.ac.uk",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Introducing the Shell"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/03-shell-introduction.html#navigating-your-file-system",
    "href": "docs/lesson01-files-and-directories/03-shell-introduction.html#navigating-your-file-system",
    "title": "Introducing the Shell",
    "section": "Navigating your file system",
    "text": "Navigating your file system\nNow we have logged into our Cloud instance, we have access to a new file directory. This instance has been set up with some existing files and directories for you, and we will also be adding some new ones later in the course.\nSeveral commands are frequently used to create, inspect, rename, and delete files and directories.\nLet’s find out where we are by running a command called pwd(which stands for “print working directory”). At any moment, our current working directory is our current default directory, i.e., the directory that the computer assumes we want to run commands in,unless we explicitly specify something else. Here, the computer’s response is /home/csuser:\n\n\nCode\n\npwd\n\n\n\nOutput\n\n/home/csuser\n\nThis csuser directory is our home directory within our cloud system.\nLet’s look at how our file system is organised. We can see what files and subdirectories are in this directory by running ls, which stands for “listing”:\n\n\nCode\n\nls\n\n\n\nOutput\n\nbin  cs_course  software\n\nls prints the names of the files and directories in the current directory in alphabetical order, arranged neatly into columns. We’ll be working within the cs_course subdirectory, and creating new subdirectories, throughout this workshop.\nThe command to change locations in our file system is cd, followed by a directory name to change our working directory. cd stands for “change directory”.\nLet’s say we want to navigate to the cs_course directory we saw above. We can use the following command to get there:\n\n\nCode\n\ncd cs_course\n\nLet’s look at what is in this directory:\n\n\nCode\n\nls\n\n\n\nOutput\n\ndata   databases\n\nWe can make the ls output easier to understand by using the flag -F, which tells ls to add a trailing / to the names of directories:\n\n\nCode\n\nls -F\n\n\n\nOutput\n\ndata/   databases/\n\nAnything with a / after it is a directory. Things with a * after them are programs. If there are no decorations, it’s a file.\nls has lots of other options. To find out what they are, we can type:\n\n\nCode\n\nman ls\n\nman (short for manual) displays detailed documentation (also referred as man page or man file) for bash commands. It is a powerful resource to explore bash commands, understand their usage and flags. Some manual files are very long. You can scroll through the file using your keyboard’s down arrow or use the Space key to go forward one page and the b key to go backwards one page. When you are done reading, hit q to quit.\n\n\n\n\n\n\nChallenge\n\n\n\nUse the -l option for the ls command to display more information for each item in the directory. What is one piece of additional information this long format gives you that you don’t see with the bare ls command?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nCode\n\nls -l\n\n\n\nOutput\n\ntotal 12\ndrwxrwxr-x 4 csuser csuser 4096 Oct 22 09:04 data\ndrwxrwxr-x 3 csuser csuser 4096 Oct  6 13:08 databases\n\nThe additional information given includes the name of the owner of the file, when the file was last modified, and whether the current user has permission to read and write to the file.\n\n\n\n\n\nNo one can possibly learn all of these arguments - that’s what the manual page is for! It does take practice to get used to using and understanding the information in the manual. Often, you don’t need to understand completely what it is saying to be able to guess what to try.\nLet’s go into the data directory and see what is in there using the cd and ls commands.\n\n\nCode\n\ncd data\nls -F\n\n\n\nOutput\n\nillumina_fastq/   nano_fastq/\n\nThis directory contains two subdirectories. We can tell they are directories and not files because of the trailing ‘/’. They contain all of the raw data we will need for the rest of the course.\nFor now, let’s have a look in illumina_fastq. We can do this without changing directories using the ls command followed by the name of the directory.\n\n\nCode\n\nls illumina_fastq\n\n\n\nOutput\n\nERR4998593_1.fastq  ERR4998593_2.fastq\n\nThis directory contains two files with .fastq extensions. FASTQ is a format for storing information about sequencing reads and their quality. We will be learning more about FASTQ files in a later lesson.\nLet’s also have a look in the nano_fastq directory.\n\n\nCode\n\nls nano_fastq\n\n\n\nOutput\n\nERR5000342_sub12.fastq\n\nLearning to navigate a new file directory can be confusing at first. To help, here is a tree diagram showing what we have explored so far.\n\n\n\n\nA file hierarchy tree.\n\n\nFirst we moved from our home directory at csuser into the cs_course directory, which is one level down. From there we opened up the data directory, which contains subdirectories - illumina_fastq and nano_fastq. We had a peek inside both of these directories and found that illumina_fastq contained two files, while nano_fastq contained one.\n\nShortcut: Tab Completion\nIt is very easy to make mistakes typing our filenames and commands. Thankfully, “tab completion” can help us! When you start typing out the name of a directory or file, then hit the tab key, the shell will try to fill in the rest of the directory or file name.\nFirst of all, typing cd after the prompt and pressing enter will always take you back to your home directory. Let’s do this:\n\n\nCode\n\ncd\n\nthen type:\n\n\nCode\n\ncd cs\n\nand press tab.\nThe shell will fill in the rest of the directory name for cs_course. Press enter to execute the command and move directories.\nNow change directories again to data.\n\n\nCode\n\ncd data\n\nAnd again into illumina_fastq.\n\n\nCode\n\ncd illumina_fastq\n\nUsing tab complete can be very helpful. However, it will only autocomplete a file or directory name if you’ve typed enough characters to provide a unique identifier for the file or directory you are trying to access.\nFor example, if we now try to list the files in illumina_fastq with names starting with ERR by using tab complete:\n\n\nCode\n\nls ERR&lt;tab&gt;\n\nThe shell auto-completes your command to ERR4998593_, because all file names in the directory begin with this prefix. When you hit tabagain, the shell will list the possible choices.\n\n\nOutput\n\nERR4998593_1.fastq  ERR4998593_2.fastq\n\nTab completion can also fill in the names of programs, which can be useful if you remember the beginning of a program name.\n\n\nCode\n\npw&lt;tab&gt;&lt;tab&gt;\n\n\n\nOutput\n\npwck      pwconv    pwd       pwdx      pwunconv\n\nDisplays the name of every program that starts with pw.\n\n\n\n\n\n\nTip\n\n\n\nYou might find it useful to keep a note of the commands you learn in this course, so you can easily remember them in future. This will be faster than scrolling through the course each time you forget a command. While using your Cloud-SPAN AWS instance you can also type csguide into the command prompt and hit enter for a text-based guide to the command line, including frequently used commands.",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Introducing the Shell"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/03-shell-introduction.html#moving-around-the-file-system",
    "href": "docs/lesson01-files-and-directories/03-shell-introduction.html#moving-around-the-file-system",
    "title": "Introducing the Shell",
    "section": "Moving around the file system",
    "text": "Moving around the file system\nNow we’re going to learn some additional commands for moving around within our file system.\nUse the commands we’ve learned so far to navigate to the illumina_fastq directory from our home:\n\n\nCode\n\ncd\ncd cs_course\ncd data\ncd illumina_fastq\n\nWhat if we want to move back up and out of this directory and to our top level directory? Can we type cd data? Try it and see what happens.\n\n\nCode\n\ncd data\n\n\n\nOutput\n\n-bash: cd: shell_data: No such file or directory\n\nYour computer looked for a directory or file called data within the directory you were already in. It didn’t know you wanted to look at a directory level above the one you were located in.\nWe have a special command to tell the computer to move us back or up one directory level.\n\n\nCode\n\ncd ..\n\nNow we can use pwd to make sure that we are in the directory we intended to navigate to, and ls to check that the contents of the directory are correct.\n\n\nCode\n\npwd\n\n\n\nOutput\n\n/home/csuser/cs_course/data\n\n\n\nCode\n\nls\n\n\n\nOutput\n\nillumina_fastq  nano_fastq\n\nFrom this output, we can see that .. did indeed take us back one level in our file system, to data.\nYou can chain these together like so:\n\n\nCode\n\nls ../../\n\n\n\nOutput\n\nbin  cs_course  software \n\nThis prints the contents of the folder called csuser (our home folder).\n\n\n\n\n\n\nFinding hidden directories\n\n\n\nThere is a hidden directory inside the cs_course directory. Explore the options for ls in the man page to find out how to see hidden directories. List the contents of the directory and identify the name of the text file in the hidden directory.\nHint 1: hidden files and folders in Unix start with ., for example .my_hidden_directory\nHint 2: cs_course is one level above data, our current working directory.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst use the man command to look at the options for ls.\n\n\nCode\n\nman ls\n\nThe -a option is short for all and says that it causes ls to “not ignore entries starting with .” This is the option we want.\nWe can use .. to view the contents of the directory above our current working directory.\n\n\nCode\n\nls -a ..\n\n\n\nOutput\n\n.  ..   data   databases   .hidden\n\nThe name of the hidden directory is .hidden. We can navigate to that directory using cd.\n\n\nCode\n\ncd ..\ncd .hidden\n\nAnd then list the contents of the directory using ls.\n\n\nCode\n\nls\n\n\n\nOutput\n\nyoufoundit.txt\n\nThe name of the text file is youfoundit.txt.\n\n\n\n\n\nIn most commands the flags can be combined together in no particular order to obtain the desired results/output.\n\n\nCode\n\nls -Fa\nls -laF",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Introducing the Shell"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/03-shell-introduction.html#summary",
    "href": "docs/lesson01-files-and-directories/03-shell-introduction.html#summary",
    "title": "Introducing the Shell",
    "section": "Summary",
    "text": "Summary\nWe now know how to move around our file system using the command line. This gives us an advantage over interacting with the file system through a GUI as it allows us to work on a remote server, carry out the same set of operations on a large number of files quickly, and opens up many opportunities for using bioinformatic software that is only available in command line versions.\nIn the next few episodes, we’ll be expanding on these skills and seeing how using the command line shell enables us to make our workflow more efficient and reproducible.\nFor now, log off using the exit command: This will close the connection, and your terminal will go back to showing your local computer prompt, for example:\n\n\nCode\n\nexit\n\nThis will close the connection, and your terminal will go back to showing your local computer prompt, for example:\n\n\nOutput\n\nlogout\nConnection to instanceNNN.cloud-span.aws.york.ac.uk closed.\nusername@machineid $",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Introducing the Shell"
    ]
  },
  {
    "objectID": "docs/miscellanea/extras/data.html",
    "href": "docs/miscellanea/extras/data.html",
    "title": "Data",
    "section": "",
    "text": "This course uses data from a 2022 paper published in BMC Environmental Microbiome titled In-depth characterization of denitrifier communities across different soil ecosystems in the tundra.\nThe paper characterises microbial communities across 43 mountain tundra sites in northern Finland with focus on their role in nitrous oxide cycling. Each site was classified as being barren, heathland, meadow or fen.\nEnvironmental information such as elevation, soil moisture and N2O fluxes was gathered for each site. DNA from each site was extracted and sequenced using Illumina technology, giving “short reads”. Two sites were additionally sequenced using a Nanopore MinION, giving “long reads”. In this course we will focus on these two sites as they have both long and short read data available.",
    "crumbs": [
      "Home",
      "Extras",
      "Data"
    ]
  },
  {
    "objectID": "docs/miscellanea/extras/data.html#the-paper",
    "href": "docs/miscellanea/extras/data.html#the-paper",
    "title": "Data",
    "section": "",
    "text": "This course uses data from a 2022 paper published in BMC Environmental Microbiome titled In-depth characterization of denitrifier communities across different soil ecosystems in the tundra.\nThe paper characterises microbial communities across 43 mountain tundra sites in northern Finland with focus on their role in nitrous oxide cycling. Each site was classified as being barren, heathland, meadow or fen.\nEnvironmental information such as elevation, soil moisture and N2O fluxes was gathered for each site. DNA from each site was extracted and sequenced using Illumina technology, giving “short reads”. Two sites were additionally sequenced using a Nanopore MinION, giving “long reads”. In this course we will focus on these two sites as they have both long and short read data available.",
    "crumbs": [
      "Home",
      "Extras",
      "Data"
    ]
  },
  {
    "objectID": "docs/miscellanea/extras/data.html#the-sites",
    "href": "docs/miscellanea/extras/data.html#the-sites",
    "title": "Data",
    "section": "The sites",
    "text": "The sites\n\n\n\n.\n\n\n\nThe main site we will be looking at is site m11216, a heathland site with elevation 812.7 metres above sea level. We have two samples for this site: a short-read sample called ERR4998593 and a long-read sample called ERR5000342. These are the samples we will be working with most and using in our metagenomics analysis workflow.\nTowards the end of the course we will compare this site with site m12208, a fenland site with elevation 706.3m above sea level. We will not be using the samples associated with this site during the course but we will use the outputs of their analysis for comparison with our own.",
    "crumbs": [
      "Home",
      "Extras",
      "Data"
    ]
  },
  {
    "objectID": "docs/miscellanea/extras/workflow.html",
    "href": "docs/miscellanea/extras/workflow.html",
    "title": "Workflow Reference",
    "section": "",
    "text": "Here is a flowchart showing the workflow we will be following during this course. You may find it helpful to refer back to this diagram as you work through the course to remind yourself what you’re doing and why.\nThe pale blue blocks indicate which lesson each step falls under.",
    "crumbs": [
      "Home",
      "Extras",
      "Workflow Reference"
    ]
  },
  {
    "objectID": "docs/miscellanea/extras/workflow.html#workflow",
    "href": "docs/miscellanea/extras/workflow.html#workflow",
    "title": "Workflow Reference",
    "section": "Workflow",
    "text": "Workflow\n\n\n\n.",
    "crumbs": [
      "Home",
      "Extras",
      "Workflow Reference"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/03-hands_on-diversity.html",
    "href": "docs/lesson06-taxonomic-annotations/03-hands_on-diversity.html",
    "title": "Taxonomic Analysis with R",
    "section": "",
    "text": "In the last lesson, we created our phyloseq object, which contains the information of our samples: ERR4998593 and ERR4998600. Let´s take a look again at the number of reads in our data.\nFor the whole metagenome:\n\n\nCode (R)\n\nbiom_metagenome\nsample_sums(biom_metagenome)\n\n\n\nOutput\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 7637 taxa and 2 samples ]\ntax_table()   Taxonomy Table:    [ 7637 taxa by 7 taxonomic ranks ] \n\nERR4998593 ERR4998600 \n    444454     311439  \n\n\n\n\n\n\n\nExercise 1\n\n\n\nRepeat this for the bacterial metagenome.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse bac_biom_metagenome instead of biom_metagenome\n\n\nCode (R)\n\nbac_biom_metagenome\nsample_sums(bac_biom_metagenome)\n\n\n\nOutput\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 7231 taxa and 2 samples ]\ntax_table()   Taxonomy Table:    [ 7231 taxa by 7 taxonomic ranks ]\n \n   ERR4998593 ERR4998600 \n     442490     305135 \n\n\n\n\n\n\nWe saw how to find out how many phyla we have and how many OTU there are in each phyla, by combining commands we:\n\nturned the tax_table into a data frame (a useful data structure in R)\ngrouped by the Phylum column\nsummarised by counting the number of rows for each phylum\nviewed the result This can be achieved with the following command:\n\n\n\nCode (R)\n\nbac_biom_metagenome@tax_table %&gt;% \n  data.frame() %&gt;% \n  group_by(Phylum) %&gt;% \n  summarise(n = length(Phylum)) %&gt;% \n  View()",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Taxonomic Analysis with R"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/03-hands_on-diversity.html#reminder",
    "href": "docs/lesson06-taxonomic-annotations/03-hands_on-diversity.html#reminder",
    "title": "Taxonomic Analysis with R",
    "section": "",
    "text": "In the last lesson, we created our phyloseq object, which contains the information of our samples: ERR4998593 and ERR4998600. Let´s take a look again at the number of reads in our data.\nFor the whole metagenome:\n\n\nCode (R)\n\nbiom_metagenome\nsample_sums(biom_metagenome)\n\n\n\nOutput\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 7637 taxa and 2 samples ]\ntax_table()   Taxonomy Table:    [ 7637 taxa by 7 taxonomic ranks ] \n\nERR4998593 ERR4998600 \n    444454     311439  \n\n\n\n\n\n\n\nExercise 1\n\n\n\nRepeat this for the bacterial metagenome.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse bac_biom_metagenome instead of biom_metagenome\n\n\nCode (R)\n\nbac_biom_metagenome\nsample_sums(bac_biom_metagenome)\n\n\n\nOutput\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 7231 taxa and 2 samples ]\ntax_table()   Taxonomy Table:    [ 7231 taxa by 7 taxonomic ranks ]\n \n   ERR4998593 ERR4998600 \n     442490     305135 \n\n\n\n\n\n\nWe saw how to find out how many phyla we have and how many OTU there are in each phyla, by combining commands we:\n\nturned the tax_table into a data frame (a useful data structure in R)\ngrouped by the Phylum column\nsummarised by counting the number of rows for each phylum\nviewed the result This can be achieved with the following command:\n\n\n\nCode (R)\n\nbac_biom_metagenome@tax_table %&gt;% \n  data.frame() %&gt;% \n  group_by(Phylum) %&gt;% \n  summarise(n = length(Phylum)) %&gt;% \n  View()",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Taxonomic Analysis with R"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/03-hands_on-diversity.html#summarise-metagenomes",
    "href": "docs/lesson06-taxonomic-annotations/03-hands_on-diversity.html#summarise-metagenomes",
    "title": "Taxonomic Analysis with R",
    "section": "Summarise metagenomes",
    "text": "Summarise metagenomes\nphyloseq has a useful function that turns a phyloseq object into a dataframe. Since the dataframe is a standard data format in R, this makes it easier for R users to apply methods they are familiar with.\nUse psmelt() to make a dataframe for the bacterial metagenomes:\nbac_meta_df &lt;- psmelt(bac_biom_metagenome)\nClicking on bac_meta_df on the Environment window will open a spreadsheet-like view of it.\nNow we can more easily summarise our metagenomes by sample using standard syntax. The following filters out all the rows with zero abundance then counts the number of taxa in each phylum for each sample:\n\n\nCode (R)\n\nnumber_of_taxa &lt;- bac_meta_df %&gt;% \n  filter(Abundance &gt; 0) %&gt;% \n  group_by(Sample, Phylum) %&gt;% \n  summarise(n = length(Abundance))\n\nClicking on number_of_taxa on the Environment window will open a spreadsheet-like view of it\nOne way to visualise the phyla is with a Venn diagram. The package ggvenn will draw one for us. It needs a data structure called a list which will contain an item for each sample of the phyla in that sample. We can see the phyla in the ERR4998593 sample with:\n\n\nCode (R)\n\nunique(number_of_taxa$Phylum[number_of_taxa$Sample == \"ERR4998593\"])\n\n\n\nOutput\n\n[1] \"Acidobacteria\"                 \"Actinobacteria\"                \"Aquificae\"                    \n[4] \"Armatimonadetes\"               \"Atribacterota\"                 \"Bacteroidetes\"                \n[7] \"Balneolaeota\"                  \"Caldiserica\"                   \"Calditrichaeota\"              \n[10] \"Candidatus Absconditabacteria\" \"Candidatus Bipolaricaulota\"    \"Candidatus Cloacimonetes\"     \n[13] \"Candidatus Omnitrophica\"       \"Candidatus Saccharibacteria\"   \"Chlamydiae\"                   \n[16] \"Chlorobi\"                      \"Chloroflexi\"                   \"Chrysiogenetes\"               \n[19] \"Coprothermobacterota\"          \"Cyanobacteria\"                 \"Deferribacteres\"              \n[22] \"Deinococcus-Thermus\"           \"Dictyoglomi\"                   \"Elusimicrobia\"                \n[25] \"Fibrobacteres\"                 \"Firmicutes\"                    \"Fusobacteria\"                 \n[28] \"Gemmatimonadetes\"              \"Ignavibacteriae\"               \"Kiritimatiellaeota\"           \n[31] \"Nitrospirae\"                   \"Planctomycetes\"                \"Proteobacteria\"               \n[34] \"Spirochaetes\"                  \"Synergistetes\"                 \"Tenericutes\"                  \n[37] \"Thermodesulfobacteria\"         \"Thermotogae\"                   \"Verrucomicrobia\"\n\n\n\n\n\n\n\nExercise 2\n\n\n\nRepeat this for the ERR4998600 sample\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse ERR4998600 instead of ERR4998593\n\n\nCode (R)\n\nunique(number_of_taxa$Phylum[number_of_taxa$Sample == \"ERR4998600\"])\n\n\n\n\n\n\"[1] \"Acidobacteria\"                 \"Actinobacteria\"                \"Aquificae\"                    \n [4] \"Armatimonadetes\"               \"Atribacterota\"                 \"Bacteroidetes\"                \n [7] \"Balneolaeota\"                  \"Caldiserica\"                   \"Calditrichaeota\"              \n [10] \"Candidatus Absconditabacteria\" \"Candidatus Bipolaricaulota\"    \"Candidatus Cloacimonetes\"     \n [13] \"Candidatus Omnitrophica\"       \"Candidatus Saccharibacteria\"   \"Chlamydiae\"                   \n [16] \"Chlorobi\"                      \"Chloroflexi\"                   \"Chrysiogenetes\"               \n [19] \"Coprothermobacterota\"          \"Cyanobacteria\"                 \"Deferribacteres\"              \n [22] \"Deinococcus-Thermus\"           \"Dictyoglomi\"                   \"Elusimicrobia\"                \n [25] \"Fibrobacteres\"                 \"Firmicutes\"                    \"Fusobacteria\"                 \n [28] \"Gemmatimonadetes\"              \"Ignavibacteriae\"               \"Kiritimatiellaeota\"           \n [31] \"Nitrospirae\"                   \"Planctomycetes\"                \"Proteobacteria\"               \n [34] \"Spirochaetes\"                  \"Synergistetes\"                 \"Tenericutes\"                  \n [37] \"Thermodesulfobacteria\"         \"Thermotogae\"                   \"Verrucomicrobia\"              \n\n\n\n\n\n\nTo place the two sets of phlya in a list, we use\n\n\nCode (R)\n\nvenn_data &lt;- list(ERR4998593 = unique(number_of_taxa$Phylum[number_of_taxa$Sample == \"ERR4998593\"]),\n                  ERR4998600 = unique(number_of_taxa$Phylum[number_of_taxa$Sample == \"ERR4998600\"]))\n\nAnd to draw the venn diagram\n\n\nCode (R)\n\nggvenn(venn_data)\n\nThe Venn diagram shows that all of the phyla are found in both samples. There are no phyla exclusive to either ERR4998593 or ERR4998600.\n\n\n\n\n\n\nVenn diagram for the phyla in the two sample.\n\n\n\nImagine that there were some phylas different between the two. Perhaps you would like to know which phyla are in ERR4998593 only? The following command would print that for us:\n\n\nCode (R)\n\nvenn_data$ERR4998593[!venn_data$ERR4998593 %in% venn_data$ERR4998600]\n\n\n\nOutput\n\nNULL\n\nOf course, this time we get a NULL reponse since the statement doesn’t apply to any phyla.",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Taxonomic Analysis with R"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/03-hands_on-diversity.html#visualizing-our-data",
    "href": "docs/lesson06-taxonomic-annotations/03-hands_on-diversity.html#visualizing-our-data",
    "title": "Taxonomic Analysis with R",
    "section": "Visualizing our data",
    "text": "Visualizing our data\nWe summarised our metagenomes by the number of phyla in each sample in number_of_taxa. We can use a similar approach to examine the abundance of each of these taxa:\n\n\nCode (R)\n\nabundance_of_taxa &lt;- bac_meta_df %&gt;% \n  filter(Abundance &gt; 0) %&gt;% \n  group_by(Sample, Phylum) %&gt;% \n  summarise(Abundance = sum(Abundance))\n\nIt would be nice to see the abundance by phyla as a figure. We can use the ggplot() function to visualise the breakdown by Phylum in each of our two bacterial metagenomes:\n\n\nCode (R)\n\nabundance_of_taxa %&gt;% \n  ggplot(aes(x = Sample, y = Abundance, fill = Phylum)) +\n  geom_col(position = \"stack\")\n\n \n\n\n\nPlot of the absolute abundance of each Phylum in the two samples.\n\n\n\nWe can see that the most abundant phyla in both samples are Proteobacteria and Actinobacteria.",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Taxonomic Analysis with R"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/03-hands_on-diversity.html#transformation-of-data",
    "href": "docs/lesson06-taxonomic-annotations/03-hands_on-diversity.html#transformation-of-data",
    "title": "Taxonomic Analysis with R",
    "section": "Transformation of data",
    "text": "Transformation of data\nSince our metagenomes have different sizes, it is imperative to convert the number of assigned reads into percentages (i.e. relative abundances) to compare them.\nWe can achieve this with:\n\n\nCode (R)\n\nabundance_of_taxa &lt;- abundance_of_taxa %&gt;% \n  group_by(Sample) %&gt;% \n  mutate(relative = Abundance/sum(Abundance) * 100)\n\nThen plot the result:\n\n\nCode (R)\n\nabundance_of_taxa %&gt;% \n  ggplot(aes(x = Sample, y = relative, fill = Phylum)) +\n  geom_col(position = \"stack\")\n\n\n\n\n\n\nPlot of the relative abundance of each Phylum in the two samples.\n\n\n\nThis bar chart still isn’t ideal. It has too many colours to accurately identify each phyla.\nLet’s try plotting the phyla as points on a scatter graph, with the two samples as axes.\nFirst we select all the columns in abundance_of_taxa except Abundance, leaving Sample, Phylum and relative. Then we pivot the data frame so that each Sample has its own column, containing the values that used to be in the relative column. One row represents one phylum.\n\n\nCode (R)\n\nabundance_of_taxa_wide &lt;- abundance_of_taxa %&gt;%\n  select(-Abundance) %&gt;%\n  pivot_wider(names_from = Sample, values_from = relative)\n\nThen plot a scatter graph with:\n\n\nCode (R)\n\nabundance_of_taxa_wide %&gt;% \n  ggplot(aes(x = ERR4998593, y = ERR4998600)) +\n  geom_point() +\n  ggtitle(\"Relative abundances of phyla\")\n\n \n\n\n\nScatter diagram for the phyla in the two samples.\n\n\n\nIt looks like our data is all clustered at the low end of the scale, with only a couple of points further along. Let’s try using a log scale to see the spread a bit better. We’d better add labels to our axes too, to remind us that a log scale was used.\n\n\nCode (R)\n\nabundance_of_taxa_wide %&gt;% \n  ggplot(aes(x = ERR4998593, y = ERR4998600)) +\n  geom_point() +\n  scale_x_log10() +\n  xlab(\"Log10(ERR4998593)\") +\n  scale_y_log10() +\n  ylab(\"Log10(ERR4998600)\") +\n  ggtitle(\"Relative abundances of phyla\")\n\n \n\n\n\nScatter diagram for the phyla in the two samples with log scale.\n\n\n\nThat’s better! We should also add a reference line too so we can see where the points would be if they were found in equal proportions in both samples. Then we can see which phyla have a higher relative abundance in each sample. We can do this with geom_abline() which draws a line with formula y = x.\n\n\nCode (R)\n\nabundance_of_taxa_wide %&gt;% \n  ggplot(aes(x = ERR4998593, y = ERR4998600)) +\n  geom_point() +\n  scale_x_log10() +\n  xlab(\"Log10(ERR4998593)\") +\n  scale_y_log10() +\n  ylab(\"Log10(ERR4998600)\") +\n  geom_abline() +\n  ggtitle(\"Relative abundances of phyla\")\n\n \n\n\n\nScatter diagram for the phyla in the two samples with log scale and y=x reference line.\n\n\n\nMost of the points fall closer to the ERR4998600 axis, telling us most of the phyla recorded had higher relative abundance in this sample than in ERR4998593. ERR4998593 is probably dominated by one or two extremely abundant phyla - we actually already know this from looking at the relative abundances as a bar chart.\nLet’s finish by marking the phyla that are more abundant in ERR4998593 in a different colour and labelling them. We start by calculating the difference between the relative abundances for each sample - this will tell us if each phylum is more abundant in ERR4998600 (a positive number) or in ERR4998593 (a negative number). Then we make a list of all the phyla which have a negative difference.\n\n\nCode (R)\n\nabundance_of_taxa_wide &lt;- abundance_of_taxa_wide %&gt;%\n  mutate(diff = ERR4998600 - ERR4998593) %&gt;%\n  arrange(diff)\nERR4998593_phyla &lt;- filter(abundance_of_taxa_wide, diff &lt; 0)$Phylum\n\nNow we have a list of phyla we can plot new points on the graph that are only present in the list we just made, this time in a different colour. We can also add a label with geom_text() to tell us what phyla these points represent.\n\n\nCode (R)\n\nabundance_of_taxa_wide %&gt;% \n  ggplot(aes(x = ERR4998593, y = ERR4998600)) +\n  geom_point() +\n  geom_point(\n    data = filter(abundance_of_taxa_wide, Phylum %in% ERR4998593_phyla),\n    aes(), \n    col = \"red\") +\n  geom_text(\n    data = filter(abundance_of_taxa_wide, Phylum %in% ERR4998593_phyla),\n    aes(label = Phylum), \n    nudge_y = -0.125,\n    col = \"red\") +\n  scale_x_log10() +\n  xlab(\"Log10(ERR4998593)\") +\n  scale_y_log10() +\n  ylab(\"Log10(ERR4998600)\") +\n  geom_abline() +\n  ggtitle(\"Relative abundances of phyla\")\n\n \n\n\n\nScatter diagram for the phyla in the two samples with points closer to ERR4998593 axis highlighted.\n\n\n\nThe two phyla which are more abundant in ERR4998593 than in ERR4998600 are Proteobacteria and Chlamydiae. We’ve seen Proteobacteria before, in our bar chart, where it was the most abundant phylum in both samples. We could probably have guessed from that graph that Proteobacteria had a higher relative abundance in one sample than the other, because the bars were very large. But we probably wouldn’t have been able to tell which sample Chlamydiae was more abundant in - its relative proportion in both is too small to see in the bar chart.",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Taxonomic Analysis with R"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/index.html",
    "href": "docs/lesson06-taxonomic-annotations/index.html",
    "title": "Taxonomic Annotations",
    "section": "",
    "text": "Now we’ve assembled and polished our metagenome, it’s time to start using it! In this lesson we will find out which species are present in our sample using taxonomic assignment. This is possible due to the vast amount of sequence data that already exists. We can compare our reads to a database of these sequences and see where the best matches are.\nThere are a few ways of doing this but we will be using a strategy called k-mers for high accuracy and rapid classification. We’ll then go on to visualise our classification results using an interactive browser application.\nAfter looking at our taxonomy we will use our results to explore and visualise the diversity of our sample.\nBy the end of this lesson you will be able to:\n\napply Kraken2 to perform taxonomic assignment using exact k-mer matches\nuse Pavian to visualise and compare samples dynamically\nunderstand α and β diversity\ncalculate a biological observation matrix (BIOM) from Kraken output using KrakenBiom\nplot metagenome diversity using the phyloseq library\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Taxonomic Annotations"
    ]
  },
  {
    "objectID": "docs/lesson07-automation-bash-scripts/03-adding-logging-and-control.html",
    "href": "docs/lesson07-automation-bash-scripts/03-adding-logging-and-control.html",
    "title": "Adding Logging and Control to the Base Script",
    "section": "",
    "text": "Overview\nin construction\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/lesson07-automation-bash-scripts/index.html",
    "href": "docs/lesson07-automation-bash-scripts/index.html",
    "title": "Automating Analyses with Bash Scripts",
    "section": "",
    "text": "This lesson is an introduction to computer task automation using Bash scripts. Using the data management and data analysis tasks that you manually ran in the previous lessons as a running example, this lesson shows how to create Bash scripts that will run those tasks automatically one after the other. Thus, you will only need to run a single script to get done the tasks your analysis require.\nA script is a file that contains all the tasks that you want to be performed, that is: all the shell commands (e.g.: ls, mkdir, rm, etc.) and programs (e.g., fastqc, flye, pilon, etc.) that need to be run to get your analysis done.\nWhen you run a script, the shell will open the script file and run each of the commands and programs in the script one after the other, either in the foreground or in the background if you specify the backgroud operator & after the script name.\nScripts consisting of various tasks are also referred to as workflows. Bash workflow scripting is widely used to automate software installs, software updates, data backups, among many others tasks."
  },
  {
    "objectID": "docs/lesson07-automation-bash-scripts/index.html#introduction",
    "href": "docs/lesson07-automation-bash-scripts/index.html#introduction",
    "title": "Automating Analyses with Bash Scripts",
    "section": "",
    "text": "This lesson is an introduction to computer task automation using Bash scripts. Using the data management and data analysis tasks that you manually ran in the previous lessons as a running example, this lesson shows how to create Bash scripts that will run those tasks automatically one after the other. Thus, you will only need to run a single script to get done the tasks your analysis require.\nA script is a file that contains all the tasks that you want to be performed, that is: all the shell commands (e.g.: ls, mkdir, rm, etc.) and programs (e.g., fastqc, flye, pilon, etc.) that need to be run to get your analysis done.\nWhen you run a script, the shell will open the script file and run each of the commands and programs in the script one after the other, either in the foreground or in the background if you specify the backgroud operator & after the script name.\nScripts consisting of various tasks are also referred to as workflows. Bash workflow scripting is widely used to automate software installs, software updates, data backups, among many others tasks."
  },
  {
    "objectID": "docs/lesson07-automation-bash-scripts/index.html#overview",
    "href": "docs/lesson07-automation-bash-scripts/index.html#overview",
    "title": "Automating Analyses with Bash Scripts",
    "section": "Overview",
    "text": "Overview\nThe lesson consists of four episodes:\n\nScripting Basics — covers how to create a script, a few simple script examples, how to run scripts, and how and when to make a script runnable from any location within the file system hierarchy.\nBase Environmental Metagenomics Script — shows a workflow script that runs all of the tasks that you ran in the previous lessons except the R tasks in the last lesson and the tasks (commands) that you ran in your local machine, for example the scp commands you ran in your local machine to copy files from your AWS instance to your local machine. This workflow script is called cswf01_baseEnvmntlMetaGenomics.sh.\nAdding Logging and Control to the Base Script — shows how to add both logging information on the progress of a workflow script and execution control on the outcome of the script tasks. Taking the base workflow script (cswf01_base..sh) as a starting point, this episode shows how to add shell commands to both write onto a log file the time at which each task starts and finishes and to check whether each task runs successfully, aborting the script if a task is unsuccessful. It is best practise to abort a workflow script as soon as a task is unsuccessful as subsequent tasks will have either no data or wrong data and there is no point in waiting for bad results. The resulting workflow script is called cswf2_logAndCtrlEnvmntlMetaGenomics.sh.\nCreating a Module-based Script — covers how to create a modular version of the previous script (cswf02_log..sh). A modular version is split into various scripts, each script being responsible for performing a specific task. The advantages of modular scripting (and software development at large) include: module reuse and hence faster development of new scripts, and cleaner and simpler coding. The episode shows how to create (1) a script out of each of the program-based tasks in the previous script, and (2) a script that runs the program-based task scripts to accomplish the full functionality of the previous script. Program-based tasks correspond to those that run analysis tools, fastqc, flye, pilon, etc. The (program-based) module scripts will enable you to create new workflow scripts by simply invoking the module scripts that your workflow needs."
  },
  {
    "objectID": "docs/lesson07-automation-bash-scripts/04-creating-module-based-version.html",
    "href": "docs/lesson07-automation-bash-scripts/04-creating-module-based-version.html",
    "title": "Creating a Module-based Script",
    "section": "",
    "text": "Overview\nin construction\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/lesson03-qc-assembly/02-QC-quality-raw-reads.html",
    "href": "docs/lesson03-qc-assembly/02-QC-quality-raw-reads.html",
    "title": "Assessing Read Quality, Trimming and Filtering",
    "section": "",
    "text": "The first thing we need to do is log in to our cloud instance.\n\n\nOpen your file manager and navigate to the cloudspan folder (hint: we recommended you make the folder in your Desktop directory - but you might have made it somewhere else). If you cannot find the folder, you can remind yourself where it is stored by looking at the absolute path you wrote down in the previous episode.\nThe folder should contain the login key file we downloaded in the previous episode and nothing else.\n\n\n\nNow we can open the command line.\n\nWindows users:\n\nRight click anywhere inside the blank space of the file manager, then select Git Bash Here. A new window will open - this is your command line interface, also known as the shell or the terminal. It will automatically open with your cloudspan directory as the working directory.\n\nMac users, you have two options:\n\nEITHER: Open Terminal in one window and type cd followed by a space. Do not press enter! Now open Finder in another window. Drag and drop the cloudspan folder from the Finder to the Terminal. You should see the file path leading to your cloudspan folder appear. Now press enter to navigate to the folder.\nOR: Open Terminal and type cd followed by the absolute path that leads to your cloudspan folder. Press enter.\n\n\n\n\n\n\n\nCode\n\nssh -i login-key-instanceNNN.pem csuser@instanceNNN.cloud-span.aws.york.ac.uk\n\nBe sure to replace NNN with your own number, twice.",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Assessing Read Quality, Trimming and Filtering"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/02-QC-quality-raw-reads.html#getting-started",
    "href": "docs/lesson03-qc-assembly/02-QC-quality-raw-reads.html#getting-started",
    "title": "Assessing Read Quality, Trimming and Filtering",
    "section": "",
    "text": "The first thing we need to do is log in to our cloud instance.\n\n\nOpen your file manager and navigate to the cloudspan folder (hint: we recommended you make the folder in your Desktop directory - but you might have made it somewhere else). If you cannot find the folder, you can remind yourself where it is stored by looking at the absolute path you wrote down in the previous episode.\nThe folder should contain the login key file we downloaded in the previous episode and nothing else.\n\n\n\nNow we can open the command line.\n\nWindows users:\n\nRight click anywhere inside the blank space of the file manager, then select Git Bash Here. A new window will open - this is your command line interface, also known as the shell or the terminal. It will automatically open with your cloudspan directory as the working directory.\n\nMac users, you have two options:\n\nEITHER: Open Terminal in one window and type cd followed by a space. Do not press enter! Now open Finder in another window. Drag and drop the cloudspan folder from the Finder to the Terminal. You should see the file path leading to your cloudspan folder appear. Now press enter to navigate to the folder.\nOR: Open Terminal and type cd followed by the absolute path that leads to your cloudspan folder. Press enter.\n\n\n\n\n\n\n\nCode\n\nssh -i login-key-instanceNNN.pem csuser@instanceNNN.cloud-span.aws.york.ac.uk\n\nBe sure to replace NNN with your own number, twice.",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Assessing Read Quality, Trimming and Filtering"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/02-QC-quality-raw-reads.html#reminder-our-file-structure",
    "href": "docs/lesson03-qc-assembly/02-QC-quality-raw-reads.html#reminder-our-file-structure",
    "title": "Assessing Read Quality, Trimming and Filtering",
    "section": "Reminder: our file structure",
    "text": "Reminder: our file structure\nBefore we start, here’s a reminder of what our file structure looks like as a hierarchy tree:\n\n\n\n\nA file hierarchy tree.\n\n\nKeep this in mind as we continue to navigate the file system, and don’t hesitate to refer back to it if needed.",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Assessing Read Quality, Trimming and Filtering"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/02-QC-quality-raw-reads.html#quality-control",
    "href": "docs/lesson03-qc-assembly/02-QC-quality-raw-reads.html#quality-control",
    "title": "Assessing Read Quality, Trimming and Filtering",
    "section": "Quality control",
    "text": "Quality control\n\n\n\n\n\n\n\n\n\n\nAnalysis flow diagram that shows the steps: Sequence reads and Quality control.\n\n\n\n\nBefore assembling our metagenome from the the short-read Illumina sequences and the long-read Nanopore sequences, we need to apply quality control to both. The two types of sequence data require different QC methods. We will use:\n\nFastQC to examine the quality of the short-read Illumina data\nNanoPlot to examine the quality of the long-read Nanopore data and Seqkit to trim and filter them.",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Assessing Read Quality, Trimming and Filtering"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/02-QC-quality-raw-reads.html#illumina-quality-control-using-fastqc",
    "href": "docs/lesson03-qc-assembly/02-QC-quality-raw-reads.html#illumina-quality-control-using-fastqc",
    "title": "Assessing Read Quality, Trimming and Filtering",
    "section": "Illumina Quality control using FastQC",
    "text": "Illumina Quality control using FastQC\nIn previous lessons we had a look at our data files and found they were in FASTQ format, a common format for sequencing data. We used grep to look for ‘bad reads’ containing more than three consecutive Ns and put these reads into their own text file.\nThis could be rather time consuming and is very nuanced. Luckily, there’s a better way! Rather than assessing every read in the raw data by hand we can use FastQC to visualise the quality of the whole sequencing file.\n\nAbout FastQC\nRather than looking at quality scores for each individual read, FastQC looks at quality collectively across all reads within a sample. The image below shows one FastQC-generated plot that indicates a very high quality sample:\n\n\n\n\nExample of high quality sample - all bars are in green section.\n\n\nThe x-axis displays the base position (bp) in the read, and the y-axis shows quality scores. In this example, the sample contains reads that are 40 bp long.\nEach position has a box-and-whisker plot showing the distribution of quality scores for all reads at that position.\n\nThe horizontal red line indicates the median quality score.\nThe yellow box shows the 1st to 3rd quartile range (this means that 50% of reads have a quality score that falls within the range of the yellow box at that position).\nThe whiskers show the absolute range, which covers the lowest (0th quartile) to highest (4th quartile) values.\nThe plot background is also color-coded to identify good (green), acceptable (yellow), and bad (red) quality scores.\n\nNow let’s take a look at a quality plot on the other end of the spectrum.\n\n\n\n\nExample of low quality sample - more bars are not in green section.\n\n\nHere, we see positions within the read in which the boxes span a much wider range. Also, quality scores drop quite low into the “bad” range, particularly on the tail end of the reads. The FastQC tool produces several other diagnostic plots to assess sample quality, in addition to the one plotted above.\n\n\nGetting started with FastQC\nFirst, we are going to organise our results by creating a directory to contain the output of all of the results we generate in this course.\nThe mkdir command can be used to make a new directory. Using the -p flag for mkdir allows it to create a new directory, even if one of the parent directories doesn’t already exist. It also suppresses errors if the directory already exists, without overwriting that directory.\nReturn to your home directory (/home/csuser):\n\n\nCode\n\ncd \n\nCreate the directories results inside cs_course:\n\n\nCode\n\nmkdir -p cs_course/results\n\nYou might want to use ls to check those nested directories have been made.\nWe are going to have lots of outputs so it makes sense to organise them with some more subdirectories. Let’s make one called qc (for quality control) and another one inside that called illumina_qc.\n\n\nCode\n\nmkdir -p cs_course/results/qc/illumina_qc\n\nNow we have created the directories we are ready to start the quality control of the Illumina data.\nFastQC has been installed on your instance and we can run it with the -h flag to display the help documentation and remind ourselves how to use it and all the parameters available:\n\n\nCode\n\nfastqc -h\n\n\n\n\n\n\n\nOutput — FastQC seq help documentation\n\n\n\n\n\n\n\nOutput\n\n\n             FastQC - A high throughput sequence QC analysis tool\n\nSYNOPSIS\n\n      fastqc seqfile1 seqfile2 .. seqfileN\n\n      fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam]\n             [-c contaminant file] seqfile1 .. seqfileN\n\nDESCRIPTION\n\n     FastQC reads a set of sequence files and produces from each one a quality\n     control report consisting of a number of different modules, each one of\n     which will help to identify a different potential type of problem in your\n     data.\n     \n     If no files to process are specified on the command line then the program\n     will start as an interactive graphical application.  If files are provided\n     on the command line then the program will run with no user interaction\n     required.  In this mode it is suitable for inclusion into a standardised\n     analysis pipeline.\n     \n     The options for the program as as follows:\n     \n     -h --help       Print this help file and exit\n     \n     -v --version    Print the version of the program and exit\n     \n     -o --outdir     Create all output files in the specified output directory.\n                     Please note that this directory must exist as the program\n                     will not create it.  If this option is not set then the\n                     output file for each sequence file is created in the same\n                     directory as the sequence file which was processed.\n                     \n     --casava        Files come from raw casava output. Files in the same sample\n                     group (differing only by the group number) will be analysed\n                     as a set rather than individually. Sequences with the filter\n                     flag set in the header will be excluded from the analysis.\n                     Files must have the same names given to them by casava\n                     (including being gzipped and ending with .gz) otherwise they\n                     won't be grouped together correctly.\n                     \n     --nano          Files come from nanopore sequences and are in fast5 format. In\n                     this mode you can pass in directories to process and the program\n                     will take in all fast5 files within those directories and produce\n                     a single output file from the sequences found in all files.                    \n                     \n     --nofilter      If running with --casava then don't remove read flagged by\n                     casava as poor quality when performing the QC analysis.\n                    \n     --extract       If set then the zipped output file will be uncompressed in\n                     the same directory after it has been created.  By default\n                     this option will be set if fastqc is run in non-interactive\n                     mode.\n                     \n     -j --java       Provides the full path to the java binary you want to use to\n                     launch fastqc. If not supplied then java is assumed to be in\n                     your path.\n                    \n     --noextract     Do not uncompress the output file after creating it.  You\n                     should set this option if you do not wish to uncompress\n                     the output when running in non-interactive mode.\n                     \n     --nogroup       Disable grouping of bases for reads &gt;50bp. All reports will\n                     show data for every base in the read.  WARNING: Using this\n                     option will cause fastqc to crash and burn if you use it on\n                     really long reads, and your plots may end up a ridiculous size.\n                     You have been warned!\n                     \n     --min_length    Sets an artificial lower limit on the length of the sequence\n                     to be shown in the report.  As long as you set this to a value\n                     greater or equal to your longest read length then this will be\n                     the sequence length used to create your read groups.  This can\n                     be useful for making directly comaparable statistics from\n                     datasets with somewhat variable read lengths.\n                     \n     -f --format     Bypasses the normal sequence file format detection and\n                     forces the program to use the specified format.  Valid\n                     formats are bam,sam,bam_mapped,sam_mapped and fastq\n                     \n     -t --threads    Specifies the number of files which can be processed\n                     simultaneously.  Each thread will be allocated 250MB of\n                     memory so you shouldn't run more threads than your\n                     available memory will cope with, and not more than\n                     6 threads on a 32 bit machine\n                   \n     -c              Specifies a non-default file which contains the list of\n     --contaminants  contaminants to screen overrepresented sequences against.\n                     The file must contain sets of named contaminants in the\n                     form name[tab]sequence.  Lines prefixed with a hash will\n                     be ignored.\n\n     -a              Specifies a non-default file which contains the list of\n     --adapters      adapter sequences which will be explicity searched against\n                     the library. The file must contain sets of named adapters\n                     in the form name[tab]sequence.  Lines prefixed with a hash\n                     will be ignored.\n                     \n     -l              Specifies a non-default file which contains a set of criteria\n     --limits        which will be used to determine the warn/error limits for the\n                     various modules.  This file can also be used to selectively\n                     remove some modules from the output all together.  The format\n                     needs to mirror the default limits.txt file found in the\n                     Configuration folder.\n                     \n    -k --kmers       Specifies the length of Kmer to look for in the Kmer content\n                     module. Specified Kmer length must be between 2 and 10. Default\n                     length is 7 if not specified.\n                     \n    -q --quiet       Supress all progress messages on stdout and only report errors.\n    \n    -d --dir         Selects a directory to be used for temporary files written when\n                     generating report images. Defaults to system temp directory if\n                     not specified.\n                     \n BUGS\n\n     Any bugs in fastqc should be reported either to simon.andrews@babraham.ac.uk\n     or in www.bioinformatics.babraham.ac.uk/bugzilla/\n\n\n\n\nThis documentation tells us that to run FastQC we use the format fastqc seqfile1 seqfile2 .. seqfileN. Running the command will produce some files. By default, these are placed in the working directory from where you ran the command. We will use the -o option to specify a different directory for the output instead.\nFastQC can accept multiple file names as input so we can use the *.fastqc wildcard to run FastQC on both of the FASTQ files at the same time.\nFirst, navigate to your cs_course/ directory.\n\n\nCode\n\ncd ~/cs_course\n\nNow you can enter the command, using -o to tell FastQC to put its output files into our newly-made illumina_qc/ directory.\n\n\n\n\n\n\nWhy here?\n\n\n\nYou might be wondering why we’re running our command from the cs_course directory and not the place where the data is stored (~/cs_course/data/illumina_fastq), or where we want our outputs to end up (~/cs_course/results/qc/illumina_qc). The reason is that it’s best practice not to run commands from the same folder as your data in case you accidentally do something which would overwrite your data files. From cs_course we can easily “see” both our data and results directories to refer to them with local paths.\nOver the rest of the course we will be moving all over the file tree to look at files but will always return to ~/cs_course to run commands. We encourage you to do the same in your own work!\n\n\n\n\nCode\n\nfastqc data/illumina_fastq/*.fastq -o results/qc/illumina_qc/\n\nPress enter and you will see an automatically updating output message telling you the progress of the analysis. It should start like this:\n\n\nOutput\n\nStarted analysis of ERR4998593_1.fastq\nApprox 5% complete for ERR4998593_1.fastq\nApprox 10% complete for ERR4998593_1.fastq\nApprox 15% complete for ERR4998593_1.fastq\nApprox 20% complete for ERR4998593_1.fastq\nApprox 25% complete for ERR4998593_1.fastq\nApprox 30% complete for ERR4998593_1.fastq\n\nIn total, it should take around ten minutes for FastQC to run on our fastq files (however, this will depend on the size and number of files you give it). When the analysis completes, your prompt will return. So your screen will look something like this:\n\n\nOutput\n\nApprox 75% complete for ERR4998593_2.fastq\nApprox 80% complete for ERR4998593_2.fastq\nApprox 85% complete for ERR4998593_2.fastq\nApprox 90% complete for ERR4998593_2.fastq\nApprox 95% complete for ERR4998593_2.fastq\nAnalysis complete for ERR4998593_2.fastq\n$\n\n\n\nLooking at FastQC outputs\nThe FastQC program has created four new files (two for each .fastq file) within our results/qc/illumina_qc/ directory. We can see them by listing the contents of the illumina_qc folder\n\n\nCode\n\nls results/qc/illumina_qc/\n\n\n\nOutput\n\nERR4998593_1_fastqc.html  ERR4998593_1_fastqc.zip  ERR4998593_2_fastqc.html  ERR4998593_2_fastqc.zip   \n\nFor each input FASTQ file, FastQC has created a .zip file and a .html file. The .zip file extension indicates that this is actually a compressed set of multiple output files. A summary report for our data is in the the .html file.\nIf we were working on our local computers, we’d be able to look at each of these HTML files by opening them in a web browser.\nHowever, these files are currently sitting on our remote AWS instance, where our local computer can’t see them. And, since we are only logging into the AWS instance via the command line, it doesn’t have any web browser setup to display these files either.\nSo the easiest way to look at these webpage summary reports will be to transfer them to our local computers (i.e. your laptop).\nTo do this we will use the scp command. scp stands for ‘secure copy protocol’, and is a widely used UNIX tool for moving files between computers. You must run scp in your local terminal in your laptop.\nThe scp command takes this form:\n\n\nCode\n\nscp &lt;file I want to copy&gt; &lt;where I want the copy to be placed&gt;\n\nYou need to start a second terminal window that is not logged into the cloud instance and ensure you are in your cloudspan directory. This is important because it contains your .pem file, which will allow the scp command access to your AWS instance to copy the file.\n\n\n\n\n\n\nStarting a new terminal\n\n\n\n\nOpen your file manager and navigate to the cloudspan folder (which should contain the login key file)\nOpen your machine’s command line interface:\n\nWindows users: Right click anywhere inside the blank space of the file manager, then select Git Bash Here.\nMac users: Open Terminal and type cd followed by the absolute path that leads to your cloudspan folder. Press enter.\n\n\nCheck that you are in the right folder using pwd\n\n\n\nNow use scp to download the file. We need to add in our .pem file, like when we log in, and we’ll also use the symbol . (this directory) to tell the scp command where to deposit the downloaded files.\nThe command will look something like:\n\n\nCode\n\nscp -i login-key-instanceNNN.pem csuser@instanceNNN.cloud-span.aws.york.ac.uk:~/cs_course/results/qc/illumina_qc/*_fastqc.html .\n\nRemember to replace NNN with your instance number.\nAs the file is downloading you will see an output like:\n\n\nOutput\n\nERR4998593_1_fastqc.html         100%  543KB   1.8MB/s   00:00  \nERR4998593_2_fastqc.html         47%  539KB   1.8MB/s   00:00 \n\nOnce the files have downloaded, use File Explorer (Windows) or Finder (Mac) to find the files and open them - they should open up in your browser.\n\n\n\n\n\n\nHelp!\n\n\n\nIf you had trouble downloading and viewing the files you can view them here: ERR4998593_1_fastqc.html and ERR4998593_2_fastqc.html\n\n\nFirst we will look at the “Per base sequence quality” graphs for ERR4998593_1.fastq and ERR4998593_2.fastq.\n\n\n\n\n\n\n\nERR4998593_1.fastq\nERR4998593_2.fastq\n\n\n\n\n\n\n\n\n\n\nThe x-axis displays the base position in the read, and the y-axis shows quality scores. The blue line represents the mean quality across samples.\nIn both samples, the mean quality values do not drop much lower than 34 at any position. This is a high quality score meaning the sequences are high quality. That means that we do not need to do any filtering. Lucky us!\nWe should also have a look at the “Adapter Content” graph which will show us where adapter sequences occur in the reads. Adapter sequences are short sequences that are added to the sample to aid during the preparation of the DNA library. They therefore don’t tell us anything biologically important and should be removed if they are present in high numbers. They might also be removed in the case of certain applications, such as when the base sequence needs to be particularly accurate.\n\n\n\n\n\n\n\nERR4998593_1.fastq\nERR4998593_2.fastq\n\n\n\n\n\n\n\n\n\n\nThis graph shows us that these sequencing file have a low percentage (~1-2%) of adapter sequences in the reads, which means we do not need to trim any adapter sequences either.\n\n\n\n\n\n\nWhen sequencing is poor(er) Quality\n\n\n\nWhile the sequencing in this example is high quality this will not always be the case.\nThe programe cutadapt can be used to filter poor quality reads and trim poor quality bases. See Genomics - Trimming and Filtering to learn more about trimming and filtering poor quality reads.",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Assessing Read Quality, Trimming and Filtering"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/02-QC-quality-raw-reads.html#nanopore-quality-control",
    "href": "docs/lesson03-qc-assembly/02-QC-quality-raw-reads.html#nanopore-quality-control",
    "title": "Assessing Read Quality, Trimming and Filtering",
    "section": "Nanopore quality control",
    "text": "Nanopore quality control\nNext we will assess the quality of the Nanopore raw reads. These are found in the file located at ~/cs_course/data/nano_fastq/ERR5000342_sub12.fastq.\nThis file contains a subset of 12% of the Nanopore reads from our site of interest (hence sub12). The reason we aren’t using all of the reads is because there are so many that our remote computer would be overwhelmed! We will be using the sub12 data for the time being, but later in the course you will be given an assembly for analysis which was assembled using the full set of reads.\nWe haven’t looked at our Nanopore data yet, so let’s view the first complete read by using head to look at the first four lines.\nMove to the folder containing the Nanopore data:\n\n\nCode\n\ncd ~/cs_course/data/nano_fastq/\n\nUse head to look at the first four lines of the fastq file:\n\n\nCode\n\nhead -n 4 ERR5000342_sub12.fastq\n\n\n\nOutput\n\n@ERR5000342.1 1 length=407\nGGTATGCTTCGTTCAGTTACGTATTGCTGTTTCGTGGCTGACCAGCAACCCGGCACCGGCGCCGAAACCGGCTCGGCGGGAATCGAGGCCCACAGCGGCACCTGCGGCGCCACCGGCAGGAACGCCACCGCGACAGCGGCCAGCGCAACGGCCACCAAAGTCGTGCCATGCGGCCCGCCGCGCACCGCTAGCAACACCGACACCGCAATCGCGAGCGCACCCCCGCCGCCACTGACGCGGCGACTGCATTCGTCAGCCGATCGCGACCGTCGCGCCTAGCCAGTTGGATAACAAACGCGAGGATCCACCAGAGTGGCGAGCACTCCGGCCAGCCCGCGCACGTTGGCCCGGTCGTGCGTGCGCAGCAAGACGATGTCGGCTGCAACGAGCAATACGTAACTTCACCA\n+\n*-/00*CD;&gt;AEJ4/?FF7:73../A@@?DFBA0((+&%'&'&-)-().:B:?&gt;=FD.3HJ@A50%$$(%'$$//24%(*06A59&lt;:BGHM:FD@@8&lt;G@HHG/73*#-%'%)/')'&%$$8&gt;;&gt;=:G;BDCD*'7B)-&lt;&lt;:CC6355=48BC76C=;.6B9751+,((('')89?;A@@B943BA540.+5&lt;&gt;&gt;DEA87AEEA4?CDIA792*.G&lt;B&lt;LGDD@ALL@8:;&gt;&lt;98:?=&*;77864C2@A&gt;*'&lt;;;GDCAMH;811@A@IF&gt;A/+,'&2285&gt;C9+(EBBC@LKDH9&lt;;;A75H?=44-/$)12145&&('&6&lt;=@7:@&lt;9B6A&lt;;A.*)213847;@B&,@C@GB?7D:B3,)18%(*-(::=?9=47A@EADHF77DHACKGA774%\"%)%##$$\n\nThis read is 408 bp, longer than the Illumina reads we looked at earlier. The length of a raw read from Nanopore sequencing varies depends on the length of the length of the DNA strand being sequenced.\nLine 4 shows us the quality score of this read.\n\n\nOutput\n\n*-/00*CD;&gt;AEJ4/?FF7:73../A@@?DFBA0((+&%'&'&-)-().:B:?&gt;=FD.3HJ@A50%$$(%'$$//24%(*06A59&lt;:BGHM:FD@@8&lt;G@HHG/73*#-%'%)/')'&%$$8&gt;;&gt;=:G;BDCD*'7B)-&lt;&lt;:CC6355=48BC76C=;.6B9751+,((('')89?;A@@B943BA540.+5&lt;&gt;&gt;DEA87AEEA4?CDIA792*.G&lt;B&lt;LGDD@ALL@8:;&gt;&lt;98:?=&*;77864C2@A&gt;*'&lt;;;GDCAMH;811@A@IF&gt;A/+,'&2285&gt;C9+(EBBC@LKDH9&lt;;;A75H?=44-/$)12145&&('&6&lt;=@7:@&lt;9B6A&lt;;A.*)213847;@B&,@C@GB?7D:B3,)18%(*-(::=?9=47A@EADHF77DHACKGA774%\"%)%##$$\n\nBased on the PHRED quality scores (see above for a reminder) we can see that the quality scores of the bases in this read range widely. Overall they are lower than the scores for Illumina reads we looked at previously.\nInstead of using FastQC we will use a program called NanoPlot, which is installed on the instance, to create some plots for the whole sequencing file. NanoPlot is specially built for Nanopore sequences.\n\n\n\n\n\n\nOther programs for Nanopore QC\n\n\n\nAnother popular program for QC of Nanopore reads is PycoQC.\nIt produces similar plots to NanoPlot but will also give you information about the overall Nanopore sequencing run. In order to generate these, PycoQC uses a sequencing summary file generated by the Nanopore sequencer (e.g. MiniION or PromethION).\nWe are using NanoPlot because the sequencing summary that PycoQC needs is not avaiable for this dataset. You can see example output files from PycoQC here: Guppy-2.1.3_basecall-1D_DNA_barcode.html.\n\n\nWe first need to navigate to the cs_course directory.\n\n\nCode\n\ncd ~/cs_course\n\nWe are now going to run NanoPlot with the raw Nanopore sequencing file. First we can look at the help documenation for NanoPlot to see what options are available.\n\n\nCode\n\nNanoPlot --help\n\n\n\n\n\n\n\nOutput — NanoPlot Help Documentation\n\n\n\n\n\n\n\nOutput\n\nusage: NanoPlot [-h] [-v] [-t THREADS] [--verbose] [--store] [--raw] [--huge]m --downsample 10000\n                 [-o OUTDIR] [-p PREFIX] [--tsv_stats] [--maxlength N]\n                 [--minlength N] [--drop_outliers] [--downsample N]\n                 [--loglength] [--percentqual] [--alength] [--minqual N]\n                 [--runtime_until N] [--readtype {1D,2D,1D2}] [--barcoded]\n                 [--no_supplementary] [-c COLOR] [-cm COLORMAP]\n                 [-f {eps,jpeg,jpg,pdf,pgf,png,ps,raw,rgba,svg,svgz,tif,tiff}]\n                 [--plots [{kde,hex,dot,pauvre} [{kde,hex,dot,pauvre} ...]]]\n                 [--listcolors] [--listcolormaps] [--no-N50] [--N50]\n                 [--title TITLE] [--font_scale FONT_SCALE] [--dpi DPI]\n                 [--hide_stats]\n                 (--fastq file [file ...] | --fasta file [file ...] | --fastq_rich file \n                 [file ...] | --fastq_minimal file [file ...] | --summary file [file ...] | \n                 --bam file [file ...] | --ubam file [file ...] | --cram file [file ...] | \n                 --pickle pickle | --feather file [file ...])\n\nCREATES VARIOUS PLOTS FOR LONG READ SEQUENCING DATA.\n\nGeneral options:\n   -h, --help            show the help and exit\n   -v, --version         Print version and exit.\n   -t, --threads THREADS\n                         Set the allowed number of threads to be used by the script\n   --verbose             Write log messages also to terminal.\n   --store               Store the extracted data in a pickle file for future plotting.\n   --raw                 Store the extracted data in tab separated file.\n   --huge                Input data is one very large file.\n   -o, --outdir OUTDIR   Specify directory in which output has to be created.\n   -p, --prefix PREFIX   Specify an optional prefix to be used for the output files.\n   --tsv_stats           Output the stats file as a properly formatted TSV.\n\nOptions for filtering or transforming input prior to plotting:\n   --maxlength N         Hide reads longer than length specified.\n   --minlength N         Hide reads shorter than length specified.\n   --drop_outliers       Drop outlier reads with extreme long length.\n   --downsample N        Reduce dataset to N reads by random sampling.\n   --loglength           Additionally show logarithmic scaling of lengths in plots.\n   --percentqual         Use qualities as theoretical percent identities.\n   --alength             Use aligned read lengths rather than sequenced length (bam mode)\n   --minqual N           Drop reads with an average quality lower than specified.\n   --runtime_until N     Only take the N first hours of a run\n   --readtype {1D,2D,1D2}\n                         Which read type to extract information about from summary. Options are 1D, \n                         2D, 1D2\n   --barcoded            Use if you want to split the summary file by barcode\n   --no_supplementary    Use if you want to remove supplementary alignments\n\nOptions for customizing the plots created:\n   -c, --color COLOR     Specify a valid matplotlib color for the plots\n   -cm, --colormap COLORMAP\n                         Specify a valid matplotlib colormap for the heatmap\n   -f, --format {eps,jpeg,jpg,pdf,pgf,png,ps,raw,rgba,svg,svgz,tif,tiff}\n                         Specify the output format of the plots.\n   --plots [{kde,hex,dot,pauvre} [{kde,hex,dot,pauvre} ...]]\n                         Specify which bivariate plots have to be made.\n   --listcolors          List the colors which are available for plotting and exit.\n   --listcolormaps       List the colors which are available for plotting and exit.\n   --no-N50              Hide the N50 mark in the read length histogram\n   --N50                 Show the N50 mark in the read length histogram\n   --title TITLE         Add a title to all plots, requires quoting if using spaces\n   --font_scale FONT_SCALE\n                         Scale the font of the plots by a factor\n   --dpi DPI             Set the dpi for saving images\n   --hide_stats          Not adding Pearson R stats in some bivariate plots\n\nInput data sources, one of these is required.:\n   --fastq file [file ...]\n                         Data is in one or more default fastq file(s).\n   --fasta file [file ...]\n                         Data is in one or more fasta file(s).\n   --fastq_rich file [file ...]\n                         Data is in one or more fastq file(s) generated by albacore, MinKNOW or guppy\n                         with additional information concerning channel and time.\n   --fastq_minimal file [file ...]\n                         Data is in one or more fastq file(s) generated by albacore, MinKNOW or guppy\n                         with additional information concerning channel and time. Is extracted swiftly\n                         without elaborate checks.\n   --summary file [file ...]\n                         Data is in one or more summary file(s) generated by albacore or guppy.\n   --bam file [file ...]\n                         Data is in one or more sorted bam file(s).\n   --ubam file [file ...]\n                         Data is in one or more unmapped bam file(s).\n   --cram file [file ...]\n                         Data is in one or more sorted cram file(s).\n   --pickle pickle       Data is a pickle file stored earlier.\n   --feather file [file ...]\n                         Data is in one or more feather file(s).\n\nEXAMPLES:\n     NanoPlot --summary sequencing_summary.txt --loglength -o summary-plots-log-transformed\n     NanoPlot -t 2 --fastq reads1.fastq.gz reads2.fastq.gz --maxlength 40000 --plots hex dot\n     NanoPlot --color yellow --bam alignment1.bam alignment2.bam alignment3.bam --downsample 10000\n\n\n\n\nWe will use four flags when we run the NanoPlot command:\n\n--fastq to specify the filetype and file to analyse. The raw Nanopore data is in the location /cs_course/p/data/nano_fastq/ERR5000342_sub12.fastq and we will use this full absolute path in the NanoPlot command.\n--outdir to specify the where the output files should be written. We are going to put our results in a directory called nano_qc, inside of results/qc. Note: with NanoPlot you don’t need to create this directory before running the command.\n--threads specifies how many threads to run the program on (more threads = more compute power = faster). We will specify 8 to indicate that eight threads should be used, since our instance has eight cores.\n--loglength specifies that we want plots with a log scale.\n\n\n\nCode\n\nNanoPlot --fastq data/nano_fastq/ERR5000342_sub12.fastq --outdir results/qc/nano_qc --threads 8 --loglength\n\nNow we have the command set up we can press enter and wait for NanoPlot to finish.\nThis will take a couple of minutes. You will know it is finished once your terminal prompt has returned (i.e. you can type in the terminal again).\nOnce NanoPlot has finished we can have a look at the output. First we need to navigate into the nano_qc directory NanoPlot created, then list the files.\n\n\nCode\n\ncd results/qc/nano_qc\nls\n\n\n\n\n\n\nOutput\n\nLengthvsQualityScatterPlot_dot.html               NanoStats.txt         \nLengthvsQualityScatterPlot_dot.png                Non_weightedHistogramReadlength.html                          \nLengthvsQualityScatterPlot_kde.html               Non_weightedHistogramReadlength.png                                 \nLengthvsQualityScatterPlot_kde.png                Non_weightedLogTransformed_HistogramReadlength.html                                        WeightedLogTransformed_HistogramReadlength.html   Non_weightedLogTransformed_HistogramReadlength.png\nLengthvsQualityScatterPlot_loglength_dot.html     WeightedLogTransformed_HistogramReadlength.png\nLengthvsQualityScatterPlot_loglength_dot.png      WeightedHistogramReadlength.html               \nLengthvsQualityScatterPlot_loglength_kde.html     WeightedHistogramReadlength.png\nLengthvsQualityScatterPlot_loglength_kde.png      Yield_By_Length.html\nNanoPlot_202303307_1630.log                       Yield_By_Length.png\nNanoPlot-report.html\n\nWe can see that NanoPlot has generated a lot of different files.\nLike before, we can’t view most of these files in our terminal as we can’t open images or HTML files. Instead we’ll download the core information to our own computer. Luckily, the NanoPlot-report.html file contains all of the plots and information held in the other files so we only need to download that one onto our local computer using scp.\nUse a terminal that is not logged into the cloud instance and ensure you are in your cloudspan directory. You may have one from earlier. If you do not, unveil the instructions below to start one.\n\n\n\n\n\n\nStarting a new terminal\n\n\n\n\n\n\nOpen your file manager and navigate to the cloudspan folder which should contain the login key file\nOpen your machine’s command line interface: Windows users: Right click anywhere inside the blank space of the file manager, then select Git Bash Here. Mac users: Open Terminal and type cd followed by the absolute path that leads to your cloudspan folder. Press enter.\n\nCheck that you are in the right folder using pwd.\n\n\n\n\nUse scp to copy the file - the command will look something like:\n\n\nCode\n\nscp -i login-key-instanceNNN.pem csuser@instanceNNN.cloud-span.aws.york.ac.uk:~/cs_course/results/qc/nano_qc/NanoPlot-report.html .\n\nRemember to replace NNN with the instance number specific to you. As the file is downloading you will see an output like:\n\n\nOutput\n\nscp -i login-key-instanceNNN.pem csuser@instanceNNN.cloud-span.aws.york.ac.uk:~/cs_course/results/qc/nano_qc/NanoPlot-report.html .\nNanoPlot-report.html                                  100% 3281KB   2.3MB/s   00:01\n\nOnce the file has downloaded, use the File Explorer (Windows) or Finder (Mac) to find the file and open it - it should open up in your browser.\n\n\n\n\n\n\nHelp!\n\n\n\n\nIf you had trouble downloading and viewing the file you can view it here: NanoPlot-report.html\n\n\nIn the report we can view summary statistics followed by plots showing the distribution of read lengths and the read length vs average read quality.\nLooking at the summary statistics table answer the following questions:\n\n\n\n\n\n\nExercise 1:\n\n\n\n\nHow many sequences are in this file?\nHow many bases are there in this entire file?\nWhat is the length of the longest read in the file and its associated mean quality score?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThere are 316,251 sequences (also known as reads) in this file\nThere are 1,324,863,094 bases (bp) in total in this FASTQ file\nThe longest read in this file is 49893 bp and it has a mean quality score of 9.3\n\n\n\n\n\n\n\n\n\n\n\n\nQuality Encodings Vary\n\n\n\nNote that not all sequencing machines use the same encoding for quality. So # might not always mean 3, a poor quality score.\nThis means it’s essential that you know which sequencing platform was used to generate your data, so that you can tell your quality control program which encoding to use. If you choose the wrong encoding, you run the risk of throwing away good reads or (even worse) not throwing away bad reads! Nanopore quality encodings are no exception. You can read more about the differences with Nanopore sequencing here: EPI2ME - Quality Scores.\n\n\n\n\n\n\n\n\nN50\n\n\n\nThe N50 length is a useful statistic when looking at sequences of varying length as it indicates that 50% of the total sequence is in reads (i.e. chunks) that are that size or larger.\nFor this FASTQ file 50% of the total bases are in reads that have a length of 5,974 bp or longer.\nSee the webpage What’s N50? for a good explanation. We will be coming back to this statistic in more detail when we get to the assembly step.\n\n\nWe can also look at some of the plots produced by NanoPlot.\nOne useful plot is the plot titled “Read lengths vs Average read quality plot using dots after log transformation of read lengths”.\n\n\n\n\nNanoPlot KDE plot with the title Read lengths vs Average read quality plot using dots after log transformation of read lengths.”\n\n\nThis plot shows the average quality of the sequence against the read lengths. We can see that the majority of the sequences have a quality score at least 7. We don’t necessarily need to trim these reads, as this is a good score, but we will do it anyway to show you how.",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Assessing Read Quality, Trimming and Filtering"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/02-QC-quality-raw-reads.html#filtering-nanopore-sequences-by-quality-score",
    "href": "docs/lesson03-qc-assembly/02-QC-quality-raw-reads.html#filtering-nanopore-sequences-by-quality-score",
    "title": "Assessing Read Quality, Trimming and Filtering",
    "section": "Filtering Nanopore sequences by quality score",
    "text": "Filtering Nanopore sequences by quality score\nWe can use the program Seqkit (which contains many tools for FASTQ/A file manipulation) to filter our reads. We will be using the command seqkit seq to create a new file containing only the sequences with an average quality above a certain value.\nAfter returning to our cs_course directory, we can view the seqkit seq help documentation with the following command:\n\n\nCode\n\ncd ~/cs_course\nseqkit seq -h\n\n\n\n\n\n\n\nOutput — Seqkit seq help documentation\n\n\n\n\n\ntransform sequences (extract ID, filter by length, remove gaps...)\n\nUsage:\n   seqkit seq [flags]\n\nFlags:\n   -k, --color                     colorize sequences - to be piped into \"less -R\"\n   -p, --complement                complement sequence, flag '-v' is recommended to switch on\n       --dna2rna                   DNA to RNA\n   -G, --gap-letters string        gap letters (default \"- \\t.\")\n   -h, --help                      help for seq\n   -l, --lower-case                print sequences in lower case\n   -M, --max-len int               only print sequences shorter than the maximum length (-1 for no limit) (default -1)\n   -R, --max-qual float            only print sequences with average quality less than this limit (-1 for no limit) (default -1)\n   -m, --min-len int               only print sequences longer than the minimum length (-1 for no limit) (default -1)\n   -Q, --min-qual float            only print sequences with average quality qreater or equal than this limit (-1 for no limit) (default -1)\n   -n, --name                      only print names\n   -i, --only-id                   print ID instead of full head\n   -q, --qual                      only print qualities\n   -b, --qual-ascii-base int       ASCII BASE, 33 for Phred+33 (default 33)\n   -g, --remove-gaps               remove gaps\n   -r, --reverse                   reverse sequence\n       --rna2dna                   RNA to DNA\n   -s, --seq                       only print sequences\n   -u, --upper-case                print sequences in upper case\n   -v, --validate-seq              validate bases according to the alphabet\n   -V, --validate-seq-length int   length of sequence to validate (0 for whole seq) (default 10000)\n\nGlobal Flags:\n       --alphabet-guess-seq-length int   length of sequence prefix of the first FASTA record based on which seqkit guesses the sequence type (0 for whole seq) (default 10000)\n       --id-ncbi                         FASTA head is NCBI-style, e.g. &gt;gi|110645304|ref|NC_002516.2| Pseud...\n       --id-regexp string                regular expression for parsing ID (default \"^(\\\\S+)\\\\s?\")\n       --infile-list string              file of input files list (one file per line), if given, they are appended to files from cli arguments\n   -w, --line-width int                  line width when outputting FASTA format (0 for no wrap) (default 60)\n   -o, --out-file string                 out file (\"-\" for stdout, suffix .gz for gzipped out) (default \"-\")\n       --quiet                           be quiet and do not show extra information\n   -t, --seq-type string                 sequence type (dna|rna|protein|unlimit|auto) (for auto, it automatically detect by the first sequence) (default \"auto\")\n   -j, --threads int                     number of CPUs. can also set with environment variable SEQKIT_THREADS) (default 4)\n\n\n\nFrom this we can see that the flag -Q will “only print sequences with average quality qreater or equal than this limit (-1 for no limit) (default -1)”.\nFrom the plot above we identified that most of the reads had a quality score of at least 7. To make sure some filtering happens, we’ll use a minimum limit of 8.\n\n\nCode\n\nseqkit seq -Q 8 data/nano_fastq/ERR5000342_sub12.fastq &gt; data/nano_fastq/ERR5000342_sub12_filtered.fastq\n\nIn the command above we use redirection (&gt;) to generate a new file ERR5000342_sub12_filtered.fastq, containing only the reads with an average quality of 8 or above.\nWe can now re-run NanoPlot on the filtered file to see how it has changed.\n\n\nCode\n\ncd ~/cs_course\n\nNanoPlot --fastq data/nano_fastq/ERR5000342_sub12_filtered.fastq --outdir results/qc/nano_qc_filt --threads 8 --loglength\n\nOnce again, wait for the command to finish and then use scpto copy the report to your local computer. First, though, make sure you rename the file so you know which is the filtered report and which isn’t.\n\n\nCode\n\ncd results/qc/nano_qc_filt\nmv NanoPlot-report.html NanoPlot-report-filtered.html\n\n\n\n\n\n\n\nHelp!\n\n\n\n\nIf you had trouble downloading the file you can view it here: NanoPlot-filtered-report.html\n\n\n\nCompare the NanoPlot statistics of the Nanopore raw reads before filtering and after filtering and answer the questions below.\n\n\n\n\n\n\nExercise 2:\n\n\n\n\nHow many reads have been removed by filtering?\nHow many bases have been removed by filtering?\nWhat is the length of the new longest read and its associated average quality score?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nInitially there were 316,251 reads; in the filtered file there are 311,172 reads; so 5079 reads have been removed by the quality filtering\nInitially there were 1,324,863,094 bases and after filtering there are 1,303,931,978 base which means filtering has removed 20,931,116 bases\nThe longest read in the filtered file is the same as before: 49893bp and it has a mean quality score of 9.3.",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Assessing Read Quality, Trimming and Filtering"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/03-assembly.html",
    "href": "docs/lesson03-qc-assembly/03-assembly.html",
    "title": "Metagenome Assembly",
    "section": "",
    "text": "IMPORTANT\n\n\n\nThe analyses in this lesson will take several hours to complete! You can find some recommended reading at the end of the page that you might want to read whilst you’re waiting.",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Metagenome Assembly"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/03-assembly.html#assembling-reads",
    "href": "docs/lesson03-qc-assembly/03-assembly.html#assembling-reads",
    "title": "Metagenome Assembly",
    "section": "Assembling reads",
    "text": "Assembling reads\nIn the last episode, we put both the long and short raw reads through quality control.\nThey are now ready to be assembled into a metagenome. Genomic assembly is the process of joining smaller fragments of DNA (i.e., reads) to make longer segments to try and reconstruct the original genomes.\n\nGenomic assembly\nYou can think of Genomic assembly as a jigsaw puzzle: each raw read corresponds to a piece of the puzzle and you are aiming to complete the puzzle by joining these pieces together in the right order.\nThere are two main strategies for genome assembly:\n\nMapping to a reference genome - requires that there is a complete genome of the organism you have sequenced, or a closely related organism. This is the approach you would take if you were trying to identify variants for well-characterised species, such as humans.\nDe novo assembly - does not use a reference but instead assembles reads together based on the content of the reads (the specific approach depends on which assembly software you are using). It is commonly used for environmental samples which usually contain many organisms that have not been cultured previously.\n\nContinuing the jigsaw analogy, mapping to a reference genome would be equivalent to having an image of the final puzzle to compare your assembly to. In contrast, in de novo assembly you would have to depend entirely on which pieces fit together.\n\n\nMetagenomic assembly\nMetagenomic sequencing adds another layer to the challenge of assembly! Instead of having one organism to assemble you now have many! Depending on the complexity of a metagenome you could have anywhere from a handful of organisms in a community to thousands.\nYou no longer have one jigsaw puzzle, but many with all the pieces mixed together.\n\n\n\n\n\nMany of the communities sequenced using metagenomics contain previously uncultured microbes (often known as microbial dark matter) so they are unlikely to have reference genomes. In addition, you don’t usually know what you are sequencing - the community of organisms is unknown.\nAssembling our metaphorical jigsaw will be a challenge. We have many, perhaps thousands, of jigsaws to assemble and no pictures\nLuckily there are programs, known as assemblers, that will do this for us!\nMetagenomic assembly faces additional problems, which means we need an assembler built to handle metagenomes. These additional problems include:\n\nDifferences in coverage between the genomes, due to differences in abundance across the sample.\nThe fact that different species often share conserved regions.\nThe presence of several strains of a single species in the community\n\nThe assembly strategy also differs based on the sequencing technology used to generate the raw reads. Here we’re using raw data from Nanopore sequencing as the basis for this metagenome assembly so we need to use a metagenome assembler appropriate for long-read sequencing.\nWe will be using Flye, which is a long-read de novo assembler for assembling large and complex data with a metagenomic mode. Like all our programs, Flye has been pre-installed onto your instance.",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Metagenome Assembly"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/03-assembly.html#flye-is-a-long-read-assembler",
    "href": "docs/lesson03-qc-assembly/03-assembly.html#flye-is-a-long-read-assembler",
    "title": "Metagenome Assembly",
    "section": "Flye is a long-read assembler",
    "text": "Flye is a long-read assembler\n\n\n\n\n\n\nImportant\n\n\n\nMake sure you’re still logged into your cloud instance. If you can’t remember how to log on, visit the instructions from earlier today.\n\n\nNavigate to the cs_course directory.\n\n\nCode\n\ncd ~/cs_course\n\nRun the flye command without any arguments to see a short description of its use:\n\n\nCode\n\nflye\n\n\n\nOutput\n\n$ flye\nusage: flye (--pacbio-raw | --pacbio-corr | --pacbio-hifi | --nano-raw |\n      --nano-corr | --nano-hq ) file1 [file_2 ...]\n      --out-dir PATH\n\n      [--genome-size SIZE] [--threads int] [--iterations int]\n      [--meta] [--polish-target] [--min-overlap SIZE]\n      [--keep-haplotypes] [--debug] [--version] [--help]\n      [--scaffold] [--resume] [--resume-from] [--stop-after]\n      [--read-error float] [--extra-params]\nflye: error: the following arguments are required: -o/--out-dir\n\nA full description can be displayed by using the --help flag:\n\n\nCode\n\nflye --help\n\n\n\n\n\n\n\nOutput — flye help documentation\n\n\n\n\n\nusage: flye (--pacbio-raw | --pacbio-corr | --pacbio-hifi | --nano-raw |\n         --nano-corr | --nano-hq ) file1 [file_2 ...]\n         --out-dir PATH\n\n         [--genome-size SIZE] [--threads int] [--iterations int]\n         [--meta] [--polish-target] [--min-overlap SIZE]\n         [--keep-haplotypes] [--debug] [--version] [--help]\n         [--scaffold] [--resume] [--resume-from] [--stop-after]\n         [--read-error float] [--extra-params]\n\nAssembly of long reads with repeat graphs\n\noptional arguments:\n   -h, --help            show this help message and exit\n   --pacbio-raw path [path ...]\n                         PacBio regular CLR reads (&lt;20% error)\n   --pacbio-corr path [path ...]\n                         PacBio reads that were corrected with other methods (&lt;3% error)\n   --pacbio-hifi path [path ...]\n                         PacBio HiFi reads (&lt;1% error)\n   --nano-raw path [path ...]\n                         ONT regular reads, pre-Guppy5 (&lt;20% error)\n   --nano-corr path [path ...]\n                         ONT reads that were corrected with other methods (&lt;3% error)\n   --nano-hq path [path ...]\n                         ONT high-quality reads: Guppy5+ or Q20 (&lt;5% error)\n   --subassemblies path [path ...]\n                         [deprecated] high-quality contigs input\n   -g size, --genome-size size\n                         estimated genome size (for example, 5m or 2.6g)\n   -o path, --out-dir path\n                         Output directory\n   -t int, --threads int\n                         number of parallel threads [1]\n   -i int, --iterations int\n                         number of polishing iterations [1]\n   -m int, --min-overlap int\n                         minimum overlap between reads [auto]\n   --asm-coverage int    reduced coverage for initial disjointig assembly [not set]\n   --hifi-error float    [deprecated] same as --read-error\n   --read-error float    adjust parameters for given read error rate (as fraction e.g. 0.03)\n   --extra-params extra_params\n                         extra configuration parameters list (comma-separated)\n   --plasmids            unused (retained for backward compatibility)\n   --meta                metagenome / uneven coverage mode\n   --keep-haplotypes     do not collapse alternative haplotypes\n   --scaffold            enable scaffolding using graph [disabled by default]\n   --trestle             [deprecated] enable Trestle [disabled by default]\n   --polish-target path  run polisher on the target sequence\n   --resume              resume from the last completed stage\n   --resume-from stage_name\n                         resume from a custom stage\n   --stop-after stage_name\n                         stop after the specified stage completed\n   --debug               enable debug output\n   -v, --version         show program's version number and exit\n\nInput reads can be in FASTA or FASTQ format, uncompressed\nor compressed with gz. Currently, PacBio (CLR, HiFi, corrected)\nand ONT reads (regular, HQ, corrected) are supported. Expected error rates are\n&lt;15% for PB CLR/regular ONT; &lt;5% for ONT HQ, &lt;3% for corrected, and &lt;1% for HiFi. Note that Flye\nwas primarily developed to run on uncorrected reads. You may specify multiple\nfiles with reads (separated by spaces). Mixing different read\ntypes is not yet supported. The --meta option enables the mode\nfor metagenome/uneven coverage assembly.\n\nTo reduce memory consumption for large genome assemblies,\nyou can use a subset of the longest reads for initial disjointig\nassembly by specifying --asm-coverage and --genome-size options. Typically,\n40x coverage is enough to produce good disjointigs.\n\nYou can run Flye polisher as a standalone tool using\n--polish-target option.\n\n\n\nFlye has multiple different options available and we need to work out which ones are appropriate for our dataset.\nThe most important thing to know is which program was used to basecall our reads, as this determines which of (--pacbio-raw | --pacbio-corr | --pacbio-hifi | --nano-raw | --nano-corr | --nano-hq ) we choose. In the Methods section of our source paper the authors state that:\n\n“Nanopore data were basecalled with GPU guppy v4.0.11 using the high-accuracy model and applying a minimum quality score of 7.”\n\nThis means we should use the --nano-raw option (ONT regular reads, pre-Guppy5), as the reads were called with a version of Guppy that precedes v5. Guppy is a program used to convert the signals that come out of a sequencer into an actual string of bases.\nThis option will be followed by the relative path to the long-read .fastq file.\nWe also need to choose how many times we want Flye to ‘polish’ the data after assembly. Polishing is a way to improve the accuracy of the assembly. The number of rounds of polishing is specified using -i or --iterations. We will do three rounds of polishing, which is a standard practice (though the default in Flye is one round only).\nThe other options are a bit easier:\n\nWe use -o or --outdir to specify (using a relative path) where the Flye output should be stored\nWe also use the -t or --threads flag in order to run the assembly on multiple threads (aka running several processes at once) in order to speed it up\nFinally we indicate that the dataset is a metagenome using the --meta option\n\n\n\n\n\n\n\nUnused parameters\n\n\n\nThere are many parameters that we don’t need. Some of these are deprecated and some are only appropriate for certain types of data. Others are useful to allow tweaking to try to further improve an assembly (e.g. --genome-size and --read-error).\nMost bioinformatics programs have an associated website (which is often a GitHub page) with a whole manual to use the program. The Flye Manual contains a lot of further information about the parameters available. If you’re going to try using Flye on your own long-read dataset this is a good place to start.\n\n\nNow we’ve worked out what parameters are appropriate for our data we can put them all together in one command. Since the command is quite long, we will use backward slashes to allow it to span several lines. \\ basically means “start a new line and carry on reading without submitting the command”.\nFirst let’s make sure we’re in the cs_course directory, and make a new directory called assembly for the Flye output inside results.\n\n\nCode\n\ncd ~/cs_course\nmkdir results/assembly\n\nNow we can start constructing our command! You can type/copy this code into your command line but don’t press enter just yet.\n\n\nCode\n\nflye --nano-raw data/nano_fastq/ERR5000342_sub12_filtered.fastq \\\n     --out-dir results/assembly \\\n     --threads 8 \\\n     --iterations 3 \\\n     --meta\n\n\n--nano-raw tells Flye that it is receiving pre-Guppy5 reads and that the input is found at the path data/nano_fastq/ERR5000342_sub12_filtered.fastq (note that we are using our ‘filtered reads’ - there’d be no point doing quality control and filtering otherwise!)\n--out-dir tells Flye that the output should be saved in the results/assembly/ directory\n--threads indicates that the number of parallel cores is 8\n--iterations indicates that the data will be polished 3 times\n--meta indicates that the dataset is a metagenome\n\nDon’t run this command yet! — If you have, you can press Ctrl+z to stop the command.\nNow we’ve built our command we could stop here but metagenomic assembly takes a long time. If we were to run this command as is we’d have to stay logged into the instance (aka leaving your computer running) for hours.\nLuckily we don’t have to do that as we’re using a remote computer.\n\nRunning a command in the background\nAll the commands we have run so far have been in the “foreground”, meaning they’ve been run directly in the terminal window, the prompt disappears and we can’t use the terminal again until the command is finished.\nCommands can also be run in the “background” so the prompt is returned before the command is finished and we can continue using our terminal. Commands run in the background are often called “jobs”. A major advantage of running a long job in the background is that you can log out of your instance without killing the process.\n\n\n\n\n\n\nWarning\n\n\n\nIf you run a job in the foreground it will stop as soon as you log out of the instance! This could cause a problem if you momentarily have unstable internet or your computer runs out of battery. Running long commands in the background means you are protected from these circumstances and means you can do other things in the terminal in the meantime.\n\n\nTo run a command in the background, you follow it with an ampersand (&) symbol.\nBut we’re still not quite done! Flye is going to print a bunch of updates to the terminal while it runs. We need to tell it to send those updates to a file, otherwise the terminal will still be unusable despite using a &. We can do this with redirection: &gt; results/assembly/flye_output.txt will send any output that would be sent to the terminal to a file called flye_output.txt (inside results/assembly) instead.\nThe complete command is:\n\n\nCode\n\nflye --nano-raw data/nano_fastq/ERR5000342_sub12_filtered.fastq \\\n     --out-dir results/assembly \\\n     --threads 8 \\\n     --iterations 3 \\\n     --meta &&gt; results/assembly/flye_output.txt &\n\nThe first & sends the main command to the background. It is immediately followed by a &gt; to redirect the logging and progress information to a file. Finally, the second & puts that command into the background too.\nWe can now press enter to run the command. Your prompt should immediately return. This doesn’t mean that the code has finished already: it is now running in the background.\n\n\n\n\n\n\nRunning commands on different servers\n\n\n\nThere are many different ways to run jobs in the background in a terminal.\nHow you run these commands will depend on the computing resources (and their fair use policies) you are using. The main options include:\n\n&, which we’ve covered here. Depending on the infrastructure you’re running the command on, you may also need to use nohup to prevent the background job from being killed when you close the terminal.\n\nThe command line program screen, which allows you to create a shell session that can be completely detached from a terminal and re-attached when needed.\nQueuing system - many shared computing resources, like the High Performance Computing (HPC) clusters owned by some Universities, operate a queuing system (e.g. SLURM or SGE) so each user gets their fair share of computing resources. With these you submit your command / job to the queueing system, which will then handle when to run the job on the resources available.\n\n\n\nAs we’re running the command in the background we no longer see the output on the terminal but we can still check on the progress of the assembly. There are two ways to do this.\n\nUsing the command jobs to view what is running\nExamining the log file created by flye using less\n\n\n\nChecking progress: jobs\nJobs command is used to list the jobs that you are running in the background and in the foreground. If the prompt is returned with no information no commands are being run.\n\n\nCode\n\njobs\n\n\n\nOutput\n\n[1]+  Running      flye --nano-raw data/nano_fastq/ERR5000342_sub12_filtered.fastq --out-dir results/assembly --threads 8 --iterations 3 --meta &&gt; results/assembly/flye_output.txt &\n\nThe [1] is the job number. If you need to stop the job running, you can use kill %1, where 1 is the job number.\n\n\nChecking progress: the log file\nFlye generates a log file when running, which is stored in the output folder it has generated. Using less we can navigate through this file.\n\n\nCode\n\ncd results/assembly\nless flye.log\n\nThe contents of the file will depend on how far through the assembly Flye is. At the start of an assembly you’ll probably see something like this:\n\n\nOutput\n\n[2022-10-05 17:22:03] INFO: Starting Flye 2.9.1-b1780\n[2022-10-05 17:22:03] INFO: &gt;&gt;&gt;STAGE: configure\n[2022-10-05 17:22:03] INFO: Configuring run\n[2022-10-05 17:22:17] INFO: Total read length: 3023658929\n[2022-10-05 17:22:17] INFO: Reads N50/N90: 5389 / 2607\n[2022-10-05 17:22:17] INFO: Minimum overlap set to 3000\n[2022-10-05 17:22:17] INFO: &gt;&gt;&gt;STAGE: assembly\n\nDifferent steps in the assembly process take different amounts of time so it might appear stuck. However, it is almost certainly still running if it was run in the background.\nNote: this log file will contain data similar to the data in the flye_output.txt file we’re generating when redirecting the terminal output. But it’s easier to look at the log file as flye will always generate that even if you’re running the command differently (e.g. in the foreground).\n\n\n\n\n\n\nNavigation commands in less:\n\n\n\n\n\n\nkey\naction\n\n\n\n\nSpace\nto go forward\n\n\nb\nto go backward\n\n\ng\nto go to the beginning\n\n\nG\nto go to the end\n\n\nq\nto quit\n\n\n\nSee Prenomics - Working with Files and Directories for a full overview on using less.\n\n\nFlye is likely to take up to 5 hours to finish assembling - so feel free to leave this running overnight and come back to it tomorrow. You don’t need to remain connected to the instance during this time (and you can turn your computer off!) but once you have disconnected from the instance it does mean you can no longer use jobs to track the job.\nIn the meantime, if you wanted to read more about assembly and metagenomics there’s a few papers and resources at the end with recommended reading.\n\n\nDetermining if the assembly has finished\nAfter leaving it several hours, Flye should have finished assembling.\nIf you remained connected to the instance during the process you will be able to tell it has finished because you get the following output in your terminal when the command has finished.\n\n\nOutput\n\n[2]+  Done      flye --nano-raw data/nano_fastq/ERR5000342_sub12_filtered.fastq --out-dir results/assembly --threads 8 --iterations 3 --meta &&gt; results/assembly/flye_output.txt &\n\nThis message won’t be displayed if you disconnected from the instance for whatever reason during the assembly process. However, you can still examine the flye.log file in the assembly directory. If the assembly has finished the log file will have summary statistics and information about the location of the assembly at the end.\nMove to the assembly directory and use less to examine the contents of the log file:\n\n\nCode\n\ncd ~/cs_course/results/assembly/\nless flye.log\n\nNavigate to the end of the file using G. You should see something like:\n\n\nOutput\n\n[2024-05-02 16:42:37] root: INFO: Assembly statistics:\n\n        Total length:   11947363\n        Fragments:      787\n        Fragments N50:  19835\n        Largest frg:    95862\n        Scaffolds:      0\n        Mean coverage:  7\n\n[2024-05-02 16:42:37] root: INFO: Final assembly: /home/csuser/cs_course/results/assembly/assembly.fasta\n\nThere are some basic statistics about the final assembly created.\n\n\nWhat is the Assembly output?\nIf we use ls in the assembly directory we can see the that Flye has created many different files.\n\n\nOutput\n\n00-assembly   20-repeat     40-polishing    assembly_graph.gfa  assembly_info.txt  params.json\n10-consensus  30-contigger  assembly.fasta  assembly_graph.gv   flye.log\n\nOne of these is flye.log which we have already looked at.\n\nFlye generates a directory to contain the output for each step of the assembly process. (These are the 00-assembly, 10-consensus, 20-repeat, 30-contigger and 40-polishing directories.)\n\nWe also have a file containing the parameters we ran the assembly under params.json which is useful to keep our results reproducible.\n\nThe assembled contigs are in FASTA format (assembly.fasta), a common standard file type for storing sequence data without its quality scores.\n\nThere’s a text file which contains more information about each contig created (assembly_info.txt).\nFinally we have two files for a repeat graph (assembly_graph.gfa or assembly_graph.gv) which is a visual way to view the assembly.\n\nYou can see more about the output for Flye in the documentation on GitHub.\n\n\n\n\n\n\nContigs vs. reads\n\n\n\nWe have seen reads in the raw sequencing data - these are our individual jigsaw pieces.\nContigs (from the word contiguous) are longer fragments of DNA produced after raw reads are joined together by the assembly process. These are like the chunks of the jigsaw puzzle the assembler has managed to complete.\nContigs are usually much longer than raw reads but vary in length and number depending on how successful the assembly has been.",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Metagenome Assembly"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/03-assembly.html#assembly-statistics",
    "href": "docs/lesson03-qc-assembly/03-assembly.html#assembly-statistics",
    "title": "Metagenome Assembly",
    "section": "Assembly Statistics",
    "text": "Assembly Statistics\nFlye gave us basic statistics about the size of the assembly but not all assemblers do. We can use Seqkit to calculate summary statistics from the assembly. We previously used another Seqkit command, seq to Filter our Nanopore sequences by quality. This time we will use the command stats.\nMake sure you are in the cs_course folder then run seqkit stats on assembly.fasta:\n\n\nCode\n\ncd ~/cs_course\nseqkit stats results/assembly/assembly.fasta\n\nSeqKit is fast so we have run the command in the terminal foreground. It should take just a couple of seconds to process this assembly. Assemblies with more sequencing data can take a bit longer.\nOnce it has finished you should see an output table like this:\n\n\nOutput\n\nfile                             format  type  num_seqs     sum_len  min_len   avg_len  max_len\nresults/assembly/assembly.fasta  FASTA   DNA        787  11,947,363      923  15,180.9   95,862\n\nThis table shows the input file, the format of the file, the type of sequence and other statistics. The assembly process introduces small random variations in the assemly so your table will likely differ slightly. However, you should expect the numbers to be very similar.\nUsing this table of statistics, answer the questions below.\n\n\n\n\n\n\nExercise 1: Looking at basic statistics\n\n\n\nUsing the output for seqkit stats above, answer the following questions.\n\nHow many contigs are in this assembly?\n\nHow many bases in total have been assembled?\n\nWhat is the shortest and longest contig produced by this assembly?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFrom our table:\na) From num_seqs we can see that this assembly is made up of 787 contigs\nb) Looking at sum_length we can see that the assembly is 11,947,363bp in total (nearly 12 million basepairs!)\nc) From min_length we can see the shortest contig is 923bp and from max_length the longest contig is 95,862bp\n\n\n\n\n\n\n\n\n\n\n\nRecommended reading:\n\n\n\nWhile you’re waiting for the assembly to finish here are some things you might want to read about:\n\nAn overall background to the history of DNA sequencing in DNA sequencing at 40: past, present and future\n\nAn overview of a metagenomics project Shotgun metagenomics, from sampling to analysis - though note this paper is from 2017 so some techniques and software will be different now.\n\nThe challenges of genomic and metagenomic assembly and the algorithms that have been built to overcome these in Assembly Algorithms for Next-Generation Sequencing Data\n\nThe approach Flye uses to assemble metagenomes is covered in metaFlye: scalable long-read metagenome assembly using repeat graphs\nComparison of genome assembly for bacteria Comparison of De Novo Assembly Strategies for Bacterial Genomes\nBenchmarking of assemblers including flye in prokaryotes Benchmarking of long-read assemblers for prokaryote whole genome sequencing\nComparison of combined assembly and polishing method Trycycler: consensus long-read assemblies for bacterial genomes\nUsing nanopore to produce ultra long reads and contiguous assemblies Nanopore sequencing and assembly of a human genome with ultra-long reads",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Metagenome Assembly"
    ]
  },
  {
    "objectID": "docs/lesson02-using-the-command-line/03-redirection.html",
    "href": "docs/lesson02-using-the-command-line/03-redirection.html",
    "title": "Redirection",
    "section": "",
    "text": "We discussed in a previous episode how to search within a file using less. We can also search within files without even opening them, using grep. grep is a command-line utility for searching plain-text files for lines matching a specific set of characters (sometimes called a string) or a particular pattern (which can be specified using something called regular expressions). We’re not going to work with regular expressions in this lesson, and are instead going to specify the strings we are searching for. Let’s give it a try!\n\n\n\n\n\n\nNucleotide abbreviations\n\n\n\nThe four nucleotides that appear in DNA are abbreviated A, C, T and G. Unknown nucleotides are represented with the letter N. An N appearing in a sequencing file represents a position where the sequencing machine was not able to confidently determine the nucleotide in that position. You can think of an N as being aNy nucleotide at that position in the DNA sequence.\n\n\nWe’ll search for strings inside of our fastq files. Let’s first make sure we are in the correct directory:\n\n\nCode\n\ncd ~/cs_course/data/illumina_fastq\n\nHowever, suppose we want to see how many reads in our file have bad segments containing three or more unclassified nucleotides (N) in a row.\n\n\n\n\n\n\nDetermining quality\n\n\n\nIn this lesson, we’re going to be manually searching for strings of bases within our sequence results to illustrate some principles of file searching. It can be really useful to do this type of searching to get a feel for the quality of your sequencing results, however, in your research you will most likely use a bioinformatics tool that has a built-in program for filtering out low-quality reads. You’ll learn how to use one such tool in a later lesson. \n\n\nTo search files we use a new command called grep. The name “grep” comes from an abbreviation of global regular expression print.\nLet’s search for the string NNN in the ERR4998593_1 file:\n\n\nCode\n\ngrep NNN ERR4998593_1.fastq\n\nThis command returns quite a lot of output to the terminal. Every single line in the ERR4998593_1 file that contains at least 3 consecutive Ns is printed to the terminal, regardless of how long or short the file is.\nWe may be interested not only in the actual sequence which contains this string, but in the name (or identifier) of that sequence. Think back to the FASTQ format we discussed previously - if you need a reminder, you can click to reveal one below.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\n.\n\n\n\n\n\nTo get all of the information about each read, we will return the line immediately before each match and the two lines immediately after each match.\nWe can use the -B argument for grep to return a specific number of lines before each match. The -A argument returns a specific number of lines after each matching line. Here we want the line before and the two lines after each matching line, so we add -B1 -A2 to our grep command:\n\n\nCode\n\ngrep -B1 -A2 NNN ERR4998593_1.fastq\n\nOne of the sets of lines returned by this command is:\n\n\nOutput\n\n@ERR4998593.50651366 50651366 length=151\nTCCTTGCGGAGCCGGGCATGCAGGNCCTGCAGGTAGNGGCGGCGGCAGGNGCCGCCGCCGTCTACCACGGTGTANCGNNCNANNNNNCANTCATCGACGGNNGGTGNGACNNCTGCGATCTTGCGNCGCCTGGTCTNGCGGCCATCCTCTN\n+\nFFF,FFFFFFFF:,,F:F,FFFFF#,:F,FFFF:F,#,FFFFF::FF:,#FFFFFFFFF:F,FF,FFFF,F,FF#,F##:#:#####:F#FFFFFFF,FF##:FFF#F:F##FFF,FFFFFF,,,#FF:F:,FFFF#F:::F:FFF:FF,#\n\n\n\n\n\n\n\nExercise\n\n\n\n\nSearch for the sequence AAAACCCCGGGGTTTT in the ERR4998593_1.fastq file. Have your search return all matching lines and the name (or identifier) for each sequence that contains a match. What is the output?\nSearch for the sequence AACCGGTTAACCGGTT in both FASTQ files. Have your search return all matching lines and the name (or identifier) for each sequence that contains a match. How many matching sequences do you get? (this may take up to 4 minutes to complete so be patient!)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngrep -B1 AAAACCCCGGGGTTTT ERR4998593_1.fastq\n\n\n\nOutput\n\n@ERR4998593.49988640 49988640 length=151\nGGTCCATGAAGCTTGTACTGCGCGCCGATATTCTCGAAGCCGAGGCCCTAGTCCGCAAAAAACAAAAACCCCGGGGTTTTGGCCCCGGGGTTTGTCGTTTGAGCGTTTGCCGCGGCGATCAGAACCGGTAGTTGACGCCGGCGCGAACGAT\n--\n@ERR4998593.16888007 16888007 length=151\nGCCGAGGCCCTAGTCCGCAAAAAACAAAAACCCCGGGGTTTTGGCCCCGGGGTTTGTCGTTTTAGCGTTTGCCGCGGCGATCAGAACCGGTAGTTGACGCCGGCGCGAACGATGTTGGTGGTGAACGACACGTTGTCGGTCACGAACGGTC\n--\n@ERR4998593.39668571 39668571 length=151\nCCCCGGGGCCAAAACCCCGGGGTTTTTGTTTTTTGCGGACTAGGCCTCGGCTTCGAGAATATCGGCGCGCAGTCCAAGCTTCATGGACCTGTCGTGACCCAAGATGGCGGATCAGGCGGGAGACTCAGGTTTTCCCGAAAGGTCTTTATGC\n\n\ngrep -B1 AACCGGTTAACCGGTT *.fastq\n\n\n\nOutput\n\nERR4998593_1.fastq-@ERR4998593.64616570 64616570 length=151\nERR4998593_1.fastq:GACAAGCTCATCTTCCAAAATCCGCAACGGTTTTTAAGCCAGTGCCCGAAATTTAGATTAACCGATTAACCGGTTAACCGGTTCGTAGGAGACGGGTAACGAGACTCTAACTCAAGTTTCGCATACTACCACCAAAACAGCCCGTCCGCGT\n--\nERR4998593_1.fastq-@ERR4998593.64617528 64617528 length=151\nERR4998593_1.fastq:GACAAGCTCATCTTCCAAAATCCGCAACGGTTTTTAAGCCAGTGCCCGAAATTTAGATTAACCGATTAACCGGTTAACCGGTTCGTAGGAGACGGGTAACGAGACTCTAACTCAAGTTTCGCATACTACCACCAAAACAGCCCGTCCGCGT\n--\nERR4998593_1.fastq-@ERR4998593.52741374 52741374 length=151\nERR4998593_1.fastq:GACAAGCTCATCTTCCAAAATCCGCAACGGTTTTTAAGCCAGTGCCCGAAATTTAGATTAACCGATTAACCGGTTAACCGGTTCGTAGGAGACGGGTAACGAGACTCTAACTCAAGTTTCGCATACTACCACCAAAACAGCCCGTCCGCGT\n--\nERR4998593_2.fastq-@ERR4998593.2096117 2096117 length=151\nERR4998593_2.fastq:ATTGGCTGGCGGCAGTCGCGTTGGCGGCTTGGCGGTTAACCGGTTAACCGGTTGACTAATGGGAGGATAACACTTCGCGACAGGAACGCAACACAATTCCGGATCAATAGGGCAACTGCCCTGGGATGGTTTTTGAGGTGGACACGGACCA\n--\nERR4998593_2.fastq-@ERR4998593.11478972 11478972 length=151\nERR4998593_2.fastq:GAACAGGGCTGTTAGTTAACCGGTCAAAGCCGCCTTAACCGGTTAACCGGTTGTAACGCCCCGCTATGCTCGTTGTGTCCGCTATCTCTGTGATTGTGTTTGTTTCGGTGTTTGCTTCGGTTTGCCGTTTGTCTATGCCGAGAAGTGGAGG",
    "crumbs": [
      "Home",
      "Using the Command Line",
      "Redirection"
    ]
  },
  {
    "objectID": "docs/lesson02-using-the-command-line/03-redirection.html#searching-files",
    "href": "docs/lesson02-using-the-command-line/03-redirection.html#searching-files",
    "title": "Redirection",
    "section": "",
    "text": "We discussed in a previous episode how to search within a file using less. We can also search within files without even opening them, using grep. grep is a command-line utility for searching plain-text files for lines matching a specific set of characters (sometimes called a string) or a particular pattern (which can be specified using something called regular expressions). We’re not going to work with regular expressions in this lesson, and are instead going to specify the strings we are searching for. Let’s give it a try!\n\n\n\n\n\n\nNucleotide abbreviations\n\n\n\nThe four nucleotides that appear in DNA are abbreviated A, C, T and G. Unknown nucleotides are represented with the letter N. An N appearing in a sequencing file represents a position where the sequencing machine was not able to confidently determine the nucleotide in that position. You can think of an N as being aNy nucleotide at that position in the DNA sequence.\n\n\nWe’ll search for strings inside of our fastq files. Let’s first make sure we are in the correct directory:\n\n\nCode\n\ncd ~/cs_course/data/illumina_fastq\n\nHowever, suppose we want to see how many reads in our file have bad segments containing three or more unclassified nucleotides (N) in a row.\n\n\n\n\n\n\nDetermining quality\n\n\n\nIn this lesson, we’re going to be manually searching for strings of bases within our sequence results to illustrate some principles of file searching. It can be really useful to do this type of searching to get a feel for the quality of your sequencing results, however, in your research you will most likely use a bioinformatics tool that has a built-in program for filtering out low-quality reads. You’ll learn how to use one such tool in a later lesson. \n\n\nTo search files we use a new command called grep. The name “grep” comes from an abbreviation of global regular expression print.\nLet’s search for the string NNN in the ERR4998593_1 file:\n\n\nCode\n\ngrep NNN ERR4998593_1.fastq\n\nThis command returns quite a lot of output to the terminal. Every single line in the ERR4998593_1 file that contains at least 3 consecutive Ns is printed to the terminal, regardless of how long or short the file is.\nWe may be interested not only in the actual sequence which contains this string, but in the name (or identifier) of that sequence. Think back to the FASTQ format we discussed previously - if you need a reminder, you can click to reveal one below.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\n.\n\n\n\n\n\nTo get all of the information about each read, we will return the line immediately before each match and the two lines immediately after each match.\nWe can use the -B argument for grep to return a specific number of lines before each match. The -A argument returns a specific number of lines after each matching line. Here we want the line before and the two lines after each matching line, so we add -B1 -A2 to our grep command:\n\n\nCode\n\ngrep -B1 -A2 NNN ERR4998593_1.fastq\n\nOne of the sets of lines returned by this command is:\n\n\nOutput\n\n@ERR4998593.50651366 50651366 length=151\nTCCTTGCGGAGCCGGGCATGCAGGNCCTGCAGGTAGNGGCGGCGGCAGGNGCCGCCGCCGTCTACCACGGTGTANCGNNCNANNNNNCANTCATCGACGGNNGGTGNGACNNCTGCGATCTTGCGNCGCCTGGTCTNGCGGCCATCCTCTN\n+\nFFF,FFFFFFFF:,,F:F,FFFFF#,:F,FFFF:F,#,FFFFF::FF:,#FFFFFFFFF:F,FF,FFFF,F,FF#,F##:#:#####:F#FFFFFFF,FF##:FFF#F:F##FFF,FFFFFF,,,#FF:F:,FFFF#F:::F:FFF:FF,#\n\n\n\n\n\n\n\nExercise\n\n\n\n\nSearch for the sequence AAAACCCCGGGGTTTT in the ERR4998593_1.fastq file. Have your search return all matching lines and the name (or identifier) for each sequence that contains a match. What is the output?\nSearch for the sequence AACCGGTTAACCGGTT in both FASTQ files. Have your search return all matching lines and the name (or identifier) for each sequence that contains a match. How many matching sequences do you get? (this may take up to 4 minutes to complete so be patient!)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngrep -B1 AAAACCCCGGGGTTTT ERR4998593_1.fastq\n\n\n\nOutput\n\n@ERR4998593.49988640 49988640 length=151\nGGTCCATGAAGCTTGTACTGCGCGCCGATATTCTCGAAGCCGAGGCCCTAGTCCGCAAAAAACAAAAACCCCGGGGTTTTGGCCCCGGGGTTTGTCGTTTGAGCGTTTGCCGCGGCGATCAGAACCGGTAGTTGACGCCGGCGCGAACGAT\n--\n@ERR4998593.16888007 16888007 length=151\nGCCGAGGCCCTAGTCCGCAAAAAACAAAAACCCCGGGGTTTTGGCCCCGGGGTTTGTCGTTTTAGCGTTTGCCGCGGCGATCAGAACCGGTAGTTGACGCCGGCGCGAACGATGTTGGTGGTGAACGACACGTTGTCGGTCACGAACGGTC\n--\n@ERR4998593.39668571 39668571 length=151\nCCCCGGGGCCAAAACCCCGGGGTTTTTGTTTTTTGCGGACTAGGCCTCGGCTTCGAGAATATCGGCGCGCAGTCCAAGCTTCATGGACCTGTCGTGACCCAAGATGGCGGATCAGGCGGGAGACTCAGGTTTTCCCGAAAGGTCTTTATGC\n\n\ngrep -B1 AACCGGTTAACCGGTT *.fastq\n\n\n\nOutput\n\nERR4998593_1.fastq-@ERR4998593.64616570 64616570 length=151\nERR4998593_1.fastq:GACAAGCTCATCTTCCAAAATCCGCAACGGTTTTTAAGCCAGTGCCCGAAATTTAGATTAACCGATTAACCGGTTAACCGGTTCGTAGGAGACGGGTAACGAGACTCTAACTCAAGTTTCGCATACTACCACCAAAACAGCCCGTCCGCGT\n--\nERR4998593_1.fastq-@ERR4998593.64617528 64617528 length=151\nERR4998593_1.fastq:GACAAGCTCATCTTCCAAAATCCGCAACGGTTTTTAAGCCAGTGCCCGAAATTTAGATTAACCGATTAACCGGTTAACCGGTTCGTAGGAGACGGGTAACGAGACTCTAACTCAAGTTTCGCATACTACCACCAAAACAGCCCGTCCGCGT\n--\nERR4998593_1.fastq-@ERR4998593.52741374 52741374 length=151\nERR4998593_1.fastq:GACAAGCTCATCTTCCAAAATCCGCAACGGTTTTTAAGCCAGTGCCCGAAATTTAGATTAACCGATTAACCGGTTAACCGGTTCGTAGGAGACGGGTAACGAGACTCTAACTCAAGTTTCGCATACTACCACCAAAACAGCCCGTCCGCGT\n--\nERR4998593_2.fastq-@ERR4998593.2096117 2096117 length=151\nERR4998593_2.fastq:ATTGGCTGGCGGCAGTCGCGTTGGCGGCTTGGCGGTTAACCGGTTAACCGGTTGACTAATGGGAGGATAACACTTCGCGACAGGAACGCAACACAATTCCGGATCAATAGGGCAACTGCCCTGGGATGGTTTTTGAGGTGGACACGGACCA\n--\nERR4998593_2.fastq-@ERR4998593.11478972 11478972 length=151\nERR4998593_2.fastq:GAACAGGGCTGTTAGTTAACCGGTCAAAGCCGCCTTAACCGGTTAACCGGTTGTAACGCCCCGCTATGCTCGTTGTGTCCGCTATCTCTGTGATTGTGTTTGTTTCGGTGTTTGCTTCGGTTTGCCGTTTGTCTATGCCGAGAAGTGGAGG",
    "crumbs": [
      "Home",
      "Using the Command Line",
      "Redirection"
    ]
  },
  {
    "objectID": "docs/lesson02-using-the-command-line/03-redirection.html#redirecting-output",
    "href": "docs/lesson02-using-the-command-line/03-redirection.html#redirecting-output",
    "title": "Redirection",
    "section": "Redirecting output",
    "text": "Redirecting output\ngrep allowed us to identify sequences in our FASTQ files that match a particular pattern. All of these sequences were printed to our terminal screen, but in order to work with these sequences and perform other operations on them, we will need to capture that output in some way.\nWe can do this with something called “redirection”. The idea is that we are taking what would ordinarily be printed to the terminal screen and redirecting it to another location. In our case, we want to print this information to a file so that we can look at it later and use other commands to analyse this data.\n\nRedirecting output to a file — the &gt; command\nThe command for redirecting output to a file is &gt;.\nLet’s try out this command and copy all the records (including all four lines of each record) in our FASTQ files that contain ‘NNN’ to another file called bad_reads.txt. The new flag --no-group-separator stops grep from putting a dashed line (–) between matches. The reason this is necessary will become apparent shortly.\n\n\nCode\n\ngrep -B1 -A2 NNN --no-group-separator ERR4998593_1.fastq &gt; bad_reads.txt\n\nThe prompt should sit there a little bit, and then it should look like nothing happened. But type ls. You should see a new file called bad_reads.txt.\n\n\n\n\n\n\nFile extensions\n\n\n\nYou might be confused about why we’re naming our output file with a .txt extension. After all, it will be holding FASTQ formatted data that we’re extracting from our FASTQ files.\nWon’t it also be a FASTQ file?\nThe answer is, yes - it will be a FASTQ file and it would make sense to name it with a .fastq extension.\nHowever, using an extension such as .txt makes it easy to distinguish the files you may generate through some exploratory processing, as the one we just made with the grep program, from the original sequencing files of your project. So you can easily select all the files with a specific extension for further processing using the wildcard * character, for example: grep .. *.fastq or mv *.txt newlocation.\n\n\n\n\nCounting number of lines in files — the wc program\nWe can check the number of lines in our new file using a command called wc. wc stands for word count. This command counts the number of words, lines, and characters in a file.\n\n\nCode\n\nwc bad_reads.txt\n\n\n\nOutput\n\n  52   78 4511 bad_reads.txt\n\nThis will tell us the number of lines, words and characters in the file. If we want only the number of lines, we can use the -l flag for lines.\n\n\nCode\n\nwc -l bad_reads.txt\n\n\n\nOutput\n\n52 bad_reads.txt\n\nThe --no-group-separator flag used above prevents grep from adding unnecessary extra lines to the file which would alter the number of lines present.\n\n\n\n\n\n\nExercise\n\n\n\n\nHow many sequences are there in ERR4998593_1.fastq? Remember that every sequence is formed by four lines.\nHow many sequences in ERR4998593_1.fastq contain at least 5 consecutive Ns?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\nCode\n\nwc -l ERR4998593_1.fastq\n\n\n\nOutput\n\n137054440 ERR4998593_1.fastq\n\nNow you can divide this number by four to get the number of sequences in your fastq file (34263610).\n\n\n\n\n\nCode\n\ngrep NNNNN ERR4998593_1.fastq &gt; bad_reads.txt\nwc -l bad_reads.txt\n\n\n\nOutput\n\n3 bad_reads.txt\n\n\n\n\n\n\n\n\nThe command &gt; redirects and overwrites\nWe might want to search multiple FASTQ files for sequences that match our search pattern. However, we need to be careful, because each time we use the &gt; command to redirect output to a file, the new output will replace the output that was already present in the file. This is called “overwriting” and, just like you don’t want to overwrite your video recording of your kid’s first birthday party, you also want to avoid overwriting your data files.\n\n\nCode\n\ngrep -B1 -A2 NNN --no-group-separator ERR4998593_1.fastq &gt; bad_reads.txt\nwc -l bad_reads.txt\n\n\n\nOutput\n\n52 bad_reads.txt\n\n\n\nCode\n\ngrep -B1 -A2 NNNNNNNNNN --no-group-separator ERR4998593_1.fastq &gt; bad_reads.txt\nwc -l bad_reads.txt\n\n\n\nOutput\n\n0 bad_reads.txt\n\nHere, the output of our second call to wc shows that we no longer have any lines in our bad_reads.txt file. This is because the second string we searched (NNNNNNNNNN) does not match any strings in the file. So our file was overwritten and is now empty.\n\n\nThe command &gt;&gt; redirects and appends\nWe can avoid overwriting our files by using the command &gt;&gt;. &gt;&gt; is known as the “append redirect” and will append new output to the end of a file, rather than overwriting it.\n\n\nCode\n\ngrep -B1 -A2 NNN --no-group-separator ERR4998593_1.fastq &gt; bad_reads.txt\nwc -l bad_reads.txt\n\n\n\nOutput\n\n52 bad_reads.txt\n\n\n\nCode\n\ngrep -B1 -A2 NNNNNNNNNN --no-group-separator ERR4998593_1.fastq &gt;&gt; bad_reads.txt\nwc -l bad_reads.txt\n\n\n\nOutput\n\n52 bad_reads.txt\n\nThe output of our second call to wc shows that we have not overwritten our original data.\n\n\nRedirecting output as input to other program — the pipe | command\nSince we might have multiple different criteria we want to search for, creating a new output file each time has the potential to clutter up our workspace. We also thus far haven’t been interested in the actual contents of those files, only in the number of reads that we’ve found. We created the files to store the reads and then counted the lines in the file to see how many reads matched our criteria. There’s a way to do this, however, that doesn’t require us to create these intermediate files - the pipe command (|).\nThis is probably not a key on your keyboard you use very much, so let’s all take a minute to find that key. For the standard QWERTY keyboard layout, the | character can be found using the key combination\n\nShift+\\\n\nWhat | does is take the output that is scrolling by on the terminal and uses that output as input to another command. When our output was scrolling by, we might have wished we could slow it down and look at it, like we can with less. Well it turns out that we can! We can redirect our output from our grep call through the less command.\n\n\nCode\n\ngrep -B1 -A2 NNN ERR4998593_1.fastq | less\n\nWe can now see the output from our grep call within the less interface. We can use the up and down arrows to scroll through the output and use q to exit less.\nIf we don’t want to create a file before counting lines of output from our grep search, we could directly pipe the output of the grep search to the command wc -l. This can be helpful for investigating your output if you are not sure you would like to save it to a file.\n\n\nCode\n\ngrep -B1 -A2 NNN --no-group-separator ERR4998593_1.fastq | wc -l\n\n\n\nOutput\n\n52\n\n\n\n\n\n\n\nCustom grep control\n\n\n\nUse man grep to read more about other options to customize the output of grep including extended options, anchoring characters, and much more.\n\n\nRedirecting output is often not intuitive, and can take some time to get used to. Once you’re comfortable with redirection, however, you’ll be able to combine any number of commands to do all sorts of exciting things with your data!\nWe’ll be using the redirect &gt; and pipe | later in the course as part of our analysis workflow, so you will get lots of practice using them.\nNone of the command line programs we’ve been learning do anything all that impressive on their own, but when you start chaining them together, you can do some really powerful things very efficiently.",
    "crumbs": [
      "Home",
      "Using the Command Line",
      "Redirection"
    ]
  },
  {
    "objectID": "docs/lesson02-using-the-command-line/01-navigating-file-directories.html",
    "href": "docs/lesson02-using-the-command-line/01-navigating-file-directories.html",
    "title": "Navigating Files and Directories",
    "section": "",
    "text": "The first thing we need to do is log in to our cloud instance.\n\n\nOpen your file manager and navigate to the cloudspan folder (hint: we recommended you make the folder in your Desktop directory - but you might have made it somewhere else). If you cannot find the folder, you can remind yourself where it is stored by looking at the absolute path you wrote down in the previous episode.\nThe folder should contain the login key file we downloaded in the previous episode and nothing else.\n\n\n\nNow we can open the command line.\n\nWindows users:\n\nRight click anywhere inside the blank space of the file manager, then select Git Bash Here. A new window will open - this is your command line interface, also known as the shell or the terminal. It will automatically open with your cloudspan directory as the working directory.\n\nMac users, you have two options:\n\nEITHER: Open Terminal in one window and type cd followed by a space. Do not press enter! Now open Finder in another window. Drag and drop the cloudspan folder from the Finder to the Terminal. You should see the file path leading to your cloudspan folder appear. Now press enter to navigate to the folder.\nOR: Open Terminal and type cd followed by the absolute path that leads to your cloudspan folder. Press enter.\n\n\n\n\n\n\n\nCode\n\nssh -i login-key-instanceNNN.pem csuser@instanceNNN.cloud-span.aws.york.ac.uk\n\nBe sure to replace NNN with your own number, twice.",
    "crumbs": [
      "Home",
      "Using the Command Line",
      "Navigating Files and Directories"
    ]
  },
  {
    "objectID": "docs/lesson02-using-the-command-line/01-navigating-file-directories.html#getting-started",
    "href": "docs/lesson02-using-the-command-line/01-navigating-file-directories.html#getting-started",
    "title": "Navigating Files and Directories",
    "section": "",
    "text": "The first thing we need to do is log in to our cloud instance.\n\n\nOpen your file manager and navigate to the cloudspan folder (hint: we recommended you make the folder in your Desktop directory - but you might have made it somewhere else). If you cannot find the folder, you can remind yourself where it is stored by looking at the absolute path you wrote down in the previous episode.\nThe folder should contain the login key file we downloaded in the previous episode and nothing else.\n\n\n\nNow we can open the command line.\n\nWindows users:\n\nRight click anywhere inside the blank space of the file manager, then select Git Bash Here. A new window will open - this is your command line interface, also known as the shell or the terminal. It will automatically open with your cloudspan directory as the working directory.\n\nMac users, you have two options:\n\nEITHER: Open Terminal in one window and type cd followed by a space. Do not press enter! Now open Finder in another window. Drag and drop the cloudspan folder from the Finder to the Terminal. You should see the file path leading to your cloudspan folder appear. Now press enter to navigate to the folder.\nOR: Open Terminal and type cd followed by the absolute path that leads to your cloudspan folder. Press enter.\n\n\n\n\n\n\n\nCode\n\nssh -i login-key-instanceNNN.pem csuser@instanceNNN.cloud-span.aws.york.ac.uk\n\nBe sure to replace NNN with your own number, twice.",
    "crumbs": [
      "Home",
      "Using the Command Line",
      "Navigating Files and Directories"
    ]
  },
  {
    "objectID": "docs/lesson02-using-the-command-line/01-navigating-file-directories.html#reminder-our-file-structure",
    "href": "docs/lesson02-using-the-command-line/01-navigating-file-directories.html#reminder-our-file-structure",
    "title": "Navigating Files and Directories",
    "section": "Reminder: our file structure",
    "text": "Reminder: our file structure\nBefore we start, here’s a reminder of what our file structure looks like as a hierarchy tree:\n\n\n\n\nA file hierarchy tree.\n\n\nKeep this in mind as we continue to navigate the file system, and don’t hesitate to refer back to it if needed.",
    "crumbs": [
      "Home",
      "Using the Command Line",
      "Navigating Files and Directories"
    ]
  },
  {
    "objectID": "docs/lesson02-using-the-command-line/01-navigating-file-directories.html#examining-the-contents-of-other-directories",
    "href": "docs/lesson02-using-the-command-line/01-navigating-file-directories.html#examining-the-contents-of-other-directories",
    "title": "Navigating Files and Directories",
    "section": "Examining the contents of other directories",
    "text": "Examining the contents of other directories\nIn the previous session we learned how to use pwd to find our current location within our file system. We also learned how to use cd to change locations and ls to list the contents of a directory.\nBy default, the ls commands lists the contents of the working directory (i.e. the directory you are in). You can always find the directory you are in using the pwd command. However, you can also give ls the names of other directories to view. Navigate to your home directory if you are not already there.\n\n\nCode\n\ncd\n\nThen enter the command:\n\n\nCode\n\nls cs_course\n\n\n\nOutput\n\ndata   databases\n\nThis will list the contents of the cs_course directory without you needing to navigate there.\nThe cd command works in a similar way.\nTry entering:\n\n\nCode\n\ncd\ncd cs_course/data/\n\nThis will take you to the data directory without having to go through the intermediate cs_course directory.\n\n\n\n\n\n\nNavigating practice\n\n\n\nNavigate to your home directory. From there, list the contents of the illumina_fastq directory.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nCode\n\ncd\nls cs_course/data/illumina_fastq\n\n\n\nOutput\n\nERR4998593_1.fastq  ERR4998593_2.fastq",
    "crumbs": [
      "Home",
      "Using the Command Line",
      "Navigating Files and Directories"
    ]
  },
  {
    "objectID": "docs/lesson02-using-the-command-line/01-navigating-file-directories.html#full-vs.-relative-paths",
    "href": "docs/lesson02-using-the-command-line/01-navigating-file-directories.html#full-vs.-relative-paths",
    "title": "Navigating Files and Directories",
    "section": "Full vs. Relative Paths",
    "text": "Full vs. Relative Paths\nThe cd command takes an argument which is a directory name. Directories can be specified using either a relative path or a full absolute path. The directories on the computer are arranged into a hierarchy. The full path tells you where a directory is in that hierarchy. Navigate to the home directory, then enter the pwd command.\n\n\nCode\n\ncd  \npwd  \n\nYou will see:\n\n\nOutput\n\n/home/csuser\n\nThis is the full name of your home directory. This tells you that you are in a directory called csuser, which sits inside a directory called home which sits inside the very top directory in the hierarchy. The very top of the hierarchy is a directory called / which is usually referred to as the root directory. So, to summarize: csuser is a directory in home which is a directory in /. More on root and home in the next section.\nNow enter the following command:\n\n\nCode\n\ncd /home/csuser/cs_course/.hidden\n\nThis jumps forward multiple levels to the .hidden directory. Now go back to the home directory.\n\n\nCode\n\ncd\n\nYou can also navigate to the .hidden directory using:\n\n\nCode\n\ncd cs_course/.hidden\n\nThese two commands have the same effect - they both take us to the .hidden directory. The first uses the absolute path, giving the full address from the root directory /. The second uses a relative path, giving only the address from the working directory. A absolute (full) path always starts with a /. A relative path does not.\nYou can usually use either a full path or a relative path depending on what is most convenient. If you want to reach a directory further down the same branch as your current working directory, it’s easiest to use the relative path since it involves less typing. If you’re trying to get to a directory in a different branch, it might be more convenient to use the full path instead of navigating “backwards” and then forwards.\nOver time, it will become easier for you to keep a mental note of the structure of the directories that you are using and how to quickly navigate amongst them. We will be using the same directory structure for this whole course so navigating it should get easier as you progress.\n\n\n\n\n\n\nRelative path resolution\n\n\n\nUsing the file system diagram below, if pwd displays /home/csuser/cs_course/data/illumina_fastq, what will ls ../nano_fastq display?\nCan you explain why?\n\n\n\n\nBlank instance file tree.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nOutput\n\nERR5000342_sub12.fastq\n\nThe command ls .. moves us up a folder level before we list the contents of nano_fastq.\n\n\n\n\n\n\nNavigational Shortcuts\nThe root directory is the highest level directory in your file system and contains files that are important for your computer to perform its daily work. While you will be using the root (/) at the beginning of your absolute paths, it is important that you avoid working with data in these higher-level directories, as your commands can permanently alter files that the operating system needs to function. In many cases, trying to run commands in root directories will require special permissions which are not discussed here, so it’s best to avoid them and work within your home directory.\nDealing with your home directory is very common. The tilde character, ~, is a shortcut for your home directory. In our case, the root directory is two levels above our home directory, so cd or cd ~ will take you to/home/csuser and cd / will take you to /.\nNavigate to the illumina_fastq directory:\n\n\nCode\n\ncd\ncd cs_course/data/illumina_fastq\n\nThen enter the command:\n\n\nCode\n\nls ~\n\n\n\nOutput\n\nbin  cs_course software\n\nThis prints the contents of your home directory, without you needing to type the full path.\nThe commands cd, and cd ~ are very useful for quickly navigating back to your home directory. We will be using the ~ character in later lessons to specify our home directory.",
    "crumbs": [
      "Home",
      "Using the Command Line",
      "Navigating Files and Directories"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Metagenomics",
    "section": "",
    "text": "This hands-on, online course teaches data analysis for metagenomics projects. It is aimed at those with little or no experience of using high performance computing (HPC) for data analysis. In the course we will cover:\nThe course is taught as a mixture of live coding, online lectures, self-study and drop-in sessions."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Metagenomics",
    "section": "Prerequisites",
    "text": "Prerequisites\n\n\n\n\n\n\nBiological concepts and software setup\n\n\n\nThis course assumes no prior experience with the tools covered in the workshop but learners are expected to have some familiarity with biological concepts, including the concept of genomes and microbiomes. Participants should bring their own laptops and plan to participate actively.\nTo get started, follow the directions in the “Precourse Instructions” tab to get access to the required software and data for this workshop. Windows users need to install Git Bash in their laptop. Mac users may need to configure the terminal program in their laptop to use the Bash shell."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Metagenomics",
    "section": "Data",
    "text": "Data\n\n\n\n\n\n\nAbout the data used in the course\n\n\n\nThis course uses data from a 2022 paper published in BMC Environmental Microbiome titled In-depth characterization of denitrifier communities across different soil ecosystems in the tundra. In this course we will compare data from two of the sites studied.\nYou can read more about the data used in the course here."
  },
  {
    "objectID": "index.html#course-format",
    "href": "index.html#course-format",
    "title": "Metagenomics",
    "section": "Course format",
    "text": "Course format\nThis workshop is designed to be run on pre-imaged Amazon Web Services (AWS) instances. All the software and data used in the workshop are hosted on an Amazon Machine Image (AMI). We will give you details as to how to access these instances after registration.\nThe course will take place over three weeks and will combine live coding and teaching with offline work. We will guide you through the analysis each session but some steps may need to completed offline due to the amount of time they take to complete. There will also be drop-in sessions to offer support and troubleshooting help, and a Slack workspace for questions."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Metagenomics",
    "section": "Course Overview",
    "text": "Course Overview\n\n\n\nLesson\nOverview\n\n\n\n\nFiles and Directories\nLearn about files and directories, log onto a cloud instance and get a basic introduction to the shell.\n\n\nUsing the Command Line\nLearn more about using the shell to navigate directories, manipulate and search files, and combine existing commands to do new things.\n\n\nQC & Assembly\nHow to quality control and assemble a genome.\n\n\nPolishing\nHow to use short reads to polish your metagenome assembly\n\n\nBinning & Functional Annotation\nHow to separate an assembly into MAGs and add functional annotation to these MAGs.\n\n\nTaxonomic Annotations\nHow to add taxonomic annotations onto contigs in an assembly."
  },
  {
    "objectID": "docs/lesson02-using-the-command-line/index.html",
    "href": "docs/lesson02-using-the-command-line/index.html",
    "title": "Using the Command Line",
    "section": "",
    "text": "Welcome to the second lesson of our Metagenomics with High Performance Computing course.\nIn this lesson we will continue learning the foundational command line skills needed for metagenomics analysis. We will recap how to navigate our cloud-based file system, learn how to work with files and discover how combining commands can unlock new possibilities.\n\nBy the end of this lesson you will be able to:\n\nPerform operations on files in directories outside your working directory.\nBe comfortable using absolute and relative paths.\nView, search within, copy, move, and rename files, and create new directories.\nUse wildcards (*) to perform operations on multiple files.\nUse the history command to view and repeat recently used commands.\nEmploy the grep command to search for information within files.\nPrint the results of a command to a file.\n\n\n\n\n\n\n\nGetting Started\n\n\n\nThis lesson assumes no prior experience with the tools covered in the module. However, learners are expected to have some familiarity with biological concepts.\nParticipants should bring their laptops and plan to participate actively.\nBefore starting this lesson, you should make sure you have fully covered the Files and Directories lesson.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Using the Command Line"
    ]
  },
  {
    "objectID": "docs/lesson02-using-the-command-line/02-working-with-file.html",
    "href": "docs/lesson02-using-the-command-line/02-working-with-file.html",
    "title": "Working with Files and Directories",
    "section": "",
    "text": "Now that we know how to navigate around our directory structure, let’s start working with our sequencing files. We are looking at the results from a short-read sequencing experiment, which are stored in our illumina_fastq directory.\n\n\n\nNavigate to your illumina_fastq directory:\n\n\nCode\n\ncd ~/cs_course/data/illumina_fastq\n\nWe are interested in looking at the fastq files in this directory. We can list all files with the .fastq extension using the command:\n\n\nCode\n\nls *.fastq\n\n\n\nOutput\n\nERR4998593_1.fastq  ERR4998593_2.fastq\n\nThe * character is a special type of character called a wildcard, which can be used to represent any number of any type of character. Thus, *.fastq matches every file that ends with .fastq.\nThis command:\n\n\nCode\n\nls ../../\nls *_2.fastq\n\n\n\nOutput\n\nERR4998593_2.fastq\n\nlists only the file that ends with _2.fastq.\nThis command:\n\n\nCode\n\nls /usr/bin/*.sh\n\n\n\nOutput\n\n/usr/bin/amuFormat.sh  /usr/bin/gettext.sh  /usr/bin/gvmap.sh\n\nLists every file in /usr/bin that ends in the characters .sh. Note that the output displays full paths to files, since each result starts with /.\n\n\n\n\n\n\nExercise\n\n\n\nWhat command would you use for each of the following tasks? Start from your current directory using a singlelscommand for each:\n\nList all of the files in /usr/bin that start with the letter ‘c’.\nList all of the files in /usr/bin that contain the letter ‘a’.\nList all of the files in /usr/bin that end with the letter ‘o’.\nList all of the files in /usr/bin that contain the letter ‘a’ or the letter ‘c’.\n\nBonus: What would the output look like if a wildcard could not be matched? Try listing all files that start with ‘missing’.\nHint: Question 4 requires a Unix wildcard that we haven’t talked about yet. Try searching the internet for information about Unix wildcards to find what you need to solve the bonus problem.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nls /usr/bin/c*\nls /usr/bin/*a*\nls /usr/bin/*o\n\nls /usr/bin/*[ac]*\n\nBonus: ls: cannot access 'missing*': No such file or directory",
    "crumbs": [
      "Home",
      "Using the Command Line",
      "Working with Files and Directories"
    ]
  },
  {
    "objectID": "docs/lesson02-using-the-command-line/02-working-with-file.html#working-with-files",
    "href": "docs/lesson02-using-the-command-line/02-working-with-file.html#working-with-files",
    "title": "Working with Files and Directories",
    "section": "",
    "text": "Now that we know how to navigate around our directory structure, let’s start working with our sequencing files. We are looking at the results from a short-read sequencing experiment, which are stored in our illumina_fastq directory.\n\n\n\nNavigate to your illumina_fastq directory:\n\n\nCode\n\ncd ~/cs_course/data/illumina_fastq\n\nWe are interested in looking at the fastq files in this directory. We can list all files with the .fastq extension using the command:\n\n\nCode\n\nls *.fastq\n\n\n\nOutput\n\nERR4998593_1.fastq  ERR4998593_2.fastq\n\nThe * character is a special type of character called a wildcard, which can be used to represent any number of any type of character. Thus, *.fastq matches every file that ends with .fastq.\nThis command:\n\n\nCode\n\nls ../../\nls *_2.fastq\n\n\n\nOutput\n\nERR4998593_2.fastq\n\nlists only the file that ends with _2.fastq.\nThis command:\n\n\nCode\n\nls /usr/bin/*.sh\n\n\n\nOutput\n\n/usr/bin/amuFormat.sh  /usr/bin/gettext.sh  /usr/bin/gvmap.sh\n\nLists every file in /usr/bin that ends in the characters .sh. Note that the output displays full paths to files, since each result starts with /.\n\n\n\n\n\n\nExercise\n\n\n\nWhat command would you use for each of the following tasks? Start from your current directory using a singlelscommand for each:\n\nList all of the files in /usr/bin that start with the letter ‘c’.\nList all of the files in /usr/bin that contain the letter ‘a’.\nList all of the files in /usr/bin that end with the letter ‘o’.\nList all of the files in /usr/bin that contain the letter ‘a’ or the letter ‘c’.\n\nBonus: What would the output look like if a wildcard could not be matched? Try listing all files that start with ‘missing’.\nHint: Question 4 requires a Unix wildcard that we haven’t talked about yet. Try searching the internet for information about Unix wildcards to find what you need to solve the bonus problem.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nls /usr/bin/c*\nls /usr/bin/*a*\nls /usr/bin/*o\n\nls /usr/bin/*[ac]*\n\nBonus: ls: cannot access 'missing*': No such file or directory",
    "crumbs": [
      "Home",
      "Using the Command Line",
      "Working with Files and Directories"
    ]
  },
  {
    "objectID": "docs/lesson02-using-the-command-line/02-working-with-file.html#command-history",
    "href": "docs/lesson02-using-the-command-line/02-working-with-file.html#command-history",
    "title": "Working with Files and Directories",
    "section": "Command History",
    "text": "Command History\nIf you want to repeat a command that you’ve run recently, you can access previous commands using the up arrow on your keyboard to go back to the most recent command. Likewise, the down arrow takes you forward in the command history.\nYou can also review your recent commands with the history command, by entering:\n\n\nCode\n\nhistory\n\nto see a numbered list of recent commands. You can reuse one of these commands directly by referring to the number of that command.\nFor example, if your history looked like this:\n\n\nOutput\n\n259  ls *\n260  ls /usr/bin/*.sh\n261  ls *R1*fastq\n\nthen you could repeat command #260 by entering:\n\n\nCode\n\n!260\n\nType ! (exclamation point) and then the number of the command from your history. You will be glad you learned this when you need to re-run very complicated commands. For more information on advanced usage of history, read section 9.3 of Bash manual.",
    "crumbs": [
      "Home",
      "Using the Command Line",
      "Working with Files and Directories"
    ]
  },
  {
    "objectID": "docs/lesson02-using-the-command-line/02-working-with-file.html#examining-files-the-less-program",
    "href": "docs/lesson02-using-the-command-line/02-working-with-file.html#examining-files-the-less-program",
    "title": "Working with Files and Directories",
    "section": "Examining Files — the less program",
    "text": "Examining Files — the less program\nWe now know how to switch directories, run programs, and look at the contents of directories, but how do we look at the contents of files?\nOne way to examine a file is to open the file in a read-only format and navigate through it using a program called less. The commands for navigating less are the same as the man program:\n\n\n\nkey\naction\n\n\n\n\nSpace\nto go forward\n\n\nb\nto go backward\n\n\ng\nto go to the beginning\n\n\nG\nto go to the end\n\n\nq\nto quit\n\n\n\nEnter the following command from within the illumina_fastq directory:\n\n\nCode\n\ncd ~/cs_course/data/illumina_fastq\nless ERR4998593_1.fastq\n\n\n\n\n\n\n\nFASTQ format\n\n\n\nThe contents might look a bit confusing. That’s because they are in FASTQ format, a popular way to store sequencing data in text-based format. These files contain both sequences and information about each sequence’s read accuracy.\n\n\nEach sequence is described in four lines:\n\n\n\n\n\n\n\nLine\nDescription\n\n\n\n\n1\nAlways begins with ‘@’ and gives the sequence identifier and an optional description\n\n\n2\nThe actual DNA sequence\n\n\n3\nAlways begins with a ‘+’ and sometimes the same info in line 1\n\n\n4\nHas a string of characters which represent the PHRED quality score for each of the bases in line 2; must have same number of characters as line 2\n\n\n\n\n\n\n\n\n\n\n\nPHRED score\n\n\n\nQuality encoding: !\"#$%&'()*+,-./0123456789:;&lt;=&gt;?@ABCDEFGHIJ\n                  |         |         |         |         |\nQuality score:    01........11........21........31........41   \nQuality is interpreted as the probability of an incorrect base call. To make it possible to line up each individual nucleotide with its quality score, the numerical score is encoded by a single character. The quality score represents the probability that the corresponding nucleotide call is incorrect. It is a logarithmic scale so a quality score of 10 reflects a base call accuracy of 90%, but a quality score of 20 reflects a base call accuracy of 99%.\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nOpen the ~/cs_course/data/illumina_fastq/ERR4998593_1.fastq file in less. What is the last line of the file? (Hint: use the shortcuts above to speed things up)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nOutput\n\n1. :FFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFF:FFFFFFFF,FFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFF:FFF:FFFFFFFFFFFFFF:FFFFFF:FF:FFFFFFFFFFFFFFFFFFFFFF,F,FFF,FFFFFF,FFFF\n\n\n\n\n\n\n\nOther programs to look into files: cat, more, head, and tail\nAnother way to look at files is using the command cat. This command prints out the entire contents of the file to the console. In large files, like the ones we’re working with today, this can take a long time and should generally be avoided. For small files, it can be a useful tool.\nThe more command prints to the console only as much content of a file as it fits in the screen, and waits for you to press the space bar to print the following portion of the file likweise, and so on until either the last portion of the file is printed or you press the q key (for quit) to exit more.\nThere’s another final way that we can look at files, and in this case, just look at part of them. This can be particularly useful if we just want to see the beginning or end of the file, or see how it’s formatted.\nThe commands are head and tail and they let you look at the beginning and end of a file, respectively.\n\n\nCode\n\nhead ERR4998593_1.fastq\n\n\n\nOutput\n\n@ERR4998593.40838091 40838091 length=151\nCCACATGCTTTAAGTGCATGTGGTACTGCTCCAGGACCAGCATTGTAGGTCGCCAATGCTTTGGCGTAGGTGCCATCAAACATATTCGTGTAATGAGCCATGAGATGGGCTGCTCCCTTCAATGCATCAACCGGATTCCACGGATCAATGC\n+\n:FFFFFFF:,FFF:FFFFFFFFFFFFFFFFFFFF:FFFF:FFFFFFFFFFF:FFFFFFFFF:FFFF:FFFFFFFFFFFFFFFFFFF:FFFF,FFFFFFF,FFF:FF,FFFFFFF::FF::FFFF:FFFFF:,:FFFFF,,FFFFFFFF,,F\n@ERR4998593.57624042 57624042 length=151\nCCTTACCACACCGGGGCTGTGGCGTTCGACCCCATCGGCAAGGCACTCTGGGTTTCCGATAGCTCGCACCATCGGCTGCTGCGCGTCCGCAATCCGGACGGCTGGGAGAGCAAACTGCTCGTGGACACGGTCATCGGTCAGAAGGACAGGT\n+\nFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFF\n@ERR4998593.3 3 length=151\nGNGGTGCTCGACGGTGGCTCGGCGGATGCGCATGGCGTCGGGCCTGCGGTCCAGCCGCTCCCGCATGGCGTCGATCACCGCCTCATGCTCCCAGCGTTTGATGCGGCGCTCCTTGCCGCTCGTACACCGGCTCTTCAGCGGGCAGCCGGCGta\n\n\n\nCode\n\ntail ERR4998593_1.fastq\n\n\n\nOutput\n\n+\nFFF:F,FF:FFFFFF:F,:FFF,FF:FF,FFF::F:F,FF,FF,FFF,FFFFFF:FFFFFFF,F,,FFF:FFFF:,FFFF:FF::F:FFF,F:FFFF,:FFFFF,F:F:FF,FF:F:FFFF:FFF:FFF::FF:FF:,::FF,FF:,F,FF\n@ERR4998593.55595926 55595926 length=151\nCAGTACAACGTTCGCTCCCTGAATTTCTGTTTCTCGGCCGGCGAAGCAATTGCTGTGGCTATCCAGGAGCGGTTCAAGCGGATGTTCGGCGTCGAAATTACGGAAGGCTGCGGGATGACCGAACTGCAAATTTACTCCATGAATCCGCCAT\n+\nFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFF:FFFFFFF:FF:FFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\n@ERR4998593.34263610 34263610 length=151\nACGCCCCACAGGGCGGCACCGACGCCGCCGCCCGGGCCCGCCGGCCCGCCCCGGTGGGCACCGGTTGCCACTGCGGCTTGCTCGGCCGTCTCACTCACTTGGACACACTTCCGTTCTTCACCGTCTCCACTGGCCGGCTAGACCGGTCCCG\n+\nFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,FFF,FFFF:FFFFFFFF,FFFFFF::F:F,FFFFFF,FFFFFFFFFFFFF,F:F:FF,FFFF\n\nThe -n option to either of these commands can be used to print the first or last n lines of a file.\n\n\nCode\n\nhead -n 1 ERR4998593_1.fastq\n\n\n\nOutput\n\n@ERR4998593.40838091 40838091 length=151\n\n\n\nCode\n\ntail -n 1 ERR4998593_1.fastq\n\n\n\nOutput\n\nF:FFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFF:FFFFFFFF,FFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFF:FFF:FFFFFFFFFFFFFF:FFFFFF:FF:FFFFFFFFFFFFFFFFFFFFFF,F,FFF,FFFFFF,FFFF",
    "crumbs": [
      "Home",
      "Using the Command Line",
      "Working with Files and Directories"
    ]
  },
  {
    "objectID": "docs/lesson02-using-the-command-line/02-working-with-file.html#creating-moving-copying-and-removing-files",
    "href": "docs/lesson02-using-the-command-line/02-working-with-file.html#creating-moving-copying-and-removing-files",
    "title": "Working with Files and Directories",
    "section": "Creating, moving, copying, and removing files",
    "text": "Creating, moving, copying, and removing files\nNow we can move around in the file structure, look at files, and search files. But what if we want to copy files or move them around or get rid of them? Most of the time, you can do these sorts of file manipulations without the command line, but there will be some cases (like when you’re working with a remote computer like we are for this lesson) where it will be impossible. You’ll also find that you may be working with hundreds of files and want to do similar manipulations to all of those files. In cases like this, it’s much faster to do these operations at the command line.\nWe’ll continue looking at our large Illumina sequencing files for the next part of the lesson.\n\nCopying Files\nWhen working with computational data, it’s important to keep a safe copy of that data that can’t be accidentally overwritten or deleted. For this lesson, our raw data is our FASTQ files.\nFirst, let’s make a copy of one of our FASTQ files using the cp command.\nNavigate to the cs_course/data/illumina_fastq directory and enter:\n\n\nCode\n\ncp ERR4998593_1.fastq ERR4998593_1_copy.fastq\nls -F\n\n\n\nOutput\n\nERR4998593_1.fastq  ERR4998593_1_copy.fastq  ERR4998593_2.fastq\n\nThe prompt will disappear for up to two minutes and reappear when the command is completed and the backup is made.\nWe now have two copies of the ERR4998593_1.fastq file, one of them named ERR4998593_1_copy.fastq. We’ll move this file to a new directory called backup where we’ll store our backup data files.\n\n\nCreating Directories\nThe mkdir command is used to make a directory. Enter mkdirfollowed by a space, then the directory name you want to create:\n\n\nCode\n\nmkdir backup\n\n\n\nMoving / Renaming files and directories\nWe can now move our backup file to this directory. We can move files around using the command mv:\n\n\nCode\n\nmv ERR4998593_1_copy.fastq backup\nls backup\n\n\n\nOutput\n\nERR4998593_1_copy.fastq\n\nThe mv command is also how you rename files. Let’s rename this file to make it clear that this is a backup:\n\n\nCode\n\ncd backup\nmv ERR4998593_1_copy.fastq ERR4998593_1_backup.fastq\nls\n\n\n\nOutput\n\nERR4998593_1_backup.fastq\n\n\n\nRemoving files and directories\nYou can delete or remove files with the rm command:\n\n\nCode\n\nrm ERR4998593_1_backup.fastq\n\nImportant: The rm command permanently removes the file. Be careful with this command. It doesn’t just nicely put the files in the recycling. They’re really gone.\nBy default, rm will not delete directories. You can tell rm to delete a directory using the -r (recursive) option. Let’s delete the backup directory we just made:\n\n\nCode\n\ncd ..\nrm -r backup\n\nThis will delete not only the directory, but all files within the directory.\n\n\n\n\n\n\nExercise\n\n\n\nStarting in the illumina_fastq directory, do the following:\n\nMake sure that you have deleted your backup directory and all files it contains.\n\nCreate a backup of each of your FASTQ files using cp. (Note: You’ll need to do this individually for each of the two FASTQ files. We haven’t learned yet how to do this with a wildcard.)\n\nUse a wildcard to move all of your backup files to a new backup directory.\nIt doesn’t make sense to keep our backup directory inside the directory it is backing up. What if we accidentally delete the illumina_fastq directory? To fix this, move your new backup directory out of illumina_fastq and into the parent folder, data.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nrm -r backup\n\ncp ERR4998593_1.fastq ERR4998593_1_backup.fastq and cp ERR4998593_2.fastq ERR4998593_2_backup.fastq\n\nmkdir backup and mv *_backup.fastq backup\nmv backup .. or mv backup ~/cs_course/data/ (note that you do not need to use the -r flag to move directories like you do when deleting them)\n\nIt’s always a good idea to check your work. Move to the data folder with cd .. and then list the contents of backup with ls -l backup. You should see something like:\n\n\nOutput\n\n-rw-rw-r-- 1 csuser csuser 2811886584 Feb 22 11:25 ERR4998593_1_backup.fastq\n-rw-rw-r-- 1 csuser csuser 2302264784 Feb 22 11:29 ERR4998593_2_backup.fastq\n\n\n\n\n\n\nHere is what your file structure should look like at the end of this episode: \n\n\n\nA file hierarchy tree.",
    "crumbs": [
      "Home",
      "Using the Command Line",
      "Working with Files and Directories"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/index.html",
    "href": "docs/lesson03-qc-assembly/index.html",
    "title": "QC & Assembly",
    "section": "",
    "text": "A metagenome is a collection of genomic sequences from various (micro) organisms coexisting in a sample. They are snapshots that tell us about the taxonomic, metabolic or functional composition of the communities that we study.\nIn this lesson we will discuss how to define metagenomics and consider the challenges that this type of analysis can present. We will also discuss a workflow for metagenomics analysis.\nWe will then log into our cloud instance and take a look at some data. Then, we’ll go through the first two steps in our workflow: quality control and metagenome assembly.\n\nBy the end of this lesson you will be able to:\n\nexplain what metagenomics is, and the challenges it presents\ninterpret a FastQC plot summarizing per-base quality across all reads.\ninterpret the NanoPlot output summarizing a Nanopore sequencing run\nfilter Nanopore reads based on quality using the command line tool SeqKit\nrun a metagenomic assembly workflow\nassess the quality of an assembly using SeqKit\n\n\n\n\n\n\n\nIMPORTANT\n\n\n\nYou should be aware that some of the analyses in this lesson can take several hours to run - these will be completed outside of the taught lesson. You will receive guidance about this from your course instructors.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "QC & Assembly"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/01-introduction-meta.html",
    "href": "docs/lesson03-qc-assembly/01-introduction-meta.html",
    "title": "Introduction to Metagenomics",
    "section": "",
    "text": "In genomics, we sequence and analyse the genome of a single species. We often have a known reference genome to which we can align all our reads.\nIn metagenomics we sequence samples composed of many genomes. These might be environmental samples from soil or anaerobic digestors for example, or samples from the skin or digestive tracts of animals. Such samples typically include species that are difficult to culture and thus lack reference genomes. The challenge in metagenomics is to assemble this mix of diverse genomes into its constituent genomes.",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Introduction to Metagenomics"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/01-introduction-meta.html#what-is-the-difference-between-genomics-and-metagenomics",
    "href": "docs/lesson03-qc-assembly/01-introduction-meta.html#what-is-the-difference-between-genomics-and-metagenomics",
    "title": "Introduction to Metagenomics",
    "section": "",
    "text": "In genomics, we sequence and analyse the genome of a single species. We often have a known reference genome to which we can align all our reads.\nIn metagenomics we sequence samples composed of many genomes. These might be environmental samples from soil or anaerobic digestors for example, or samples from the skin or digestive tracts of animals. Such samples typically include species that are difficult to culture and thus lack reference genomes. The challenge in metagenomics is to assemble this mix of diverse genomes into its constituent genomes.",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Introduction to Metagenomics"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/01-introduction-meta.html#metagenomics",
    "href": "docs/lesson03-qc-assembly/01-introduction-meta.html#metagenomics",
    "title": "Introduction to Metagenomics",
    "section": "Metagenomics",
    "text": "Metagenomics\nA metagenome is a collection of genomic sequences from various (micro) organisms coexisting in a given space. They are snapshots that tell us about the taxonomic, metabolic or functional composition of the communities that we study.\nAnalysing multiple genomes rather than individual genomes introduces additional complexity:\n\nTaxonomic assignment: How can we separate the sequences to the different putative organisms or taxonomic units?\nCommunity Composition: How can we quantify the relative abundances of the taxonomic units present?\n\nA typical metagenomic workflow is designed to answer two questions:\n\nWhat species are present in the sample and what are their relative abundances?\nWhat is the functional capacity of the organisms or the community?",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Introduction to Metagenomics"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/01-introduction-meta.html#community-sequencing-approaches",
    "href": "docs/lesson03-qc-assembly/01-introduction-meta.html#community-sequencing-approaches",
    "title": "Introduction to Metagenomics",
    "section": "Community sequencing approaches",
    "text": "Community sequencing approaches\nThere are two technologies used in mixed community sequencing which have different use cases, advantages and disadvantages: Whole genome metagenomics and Amplicon/(16S) sequencing.\n\nWhole metagenome sequencing (WMS)\nRandom parts of all of the genomes present in a sample are sequenced in WMS. We aim to find what organism, or “taxonomic unit”, they come from, what part of their genome they come from, and what functions they encode. This can be done by cross-referencing the sequences obtained with databases of information.\nIf the metagenome is complex and deeply sequenced enough, we may even be able obtain full individual genomes from WMS and a strong understanding of the functional capacity of the microbiome.\nFor abundant organisms in a metagenome sample, there are likely to be enough data to generate reasonable genome coverage. However, this is not the case for low abundance organisms. Often deeper sequencing/ more total sequencing data is required to assemble the genomes of these organisms. If your research question can be addressed by considering the most abundant organisms, you need do less sequencing than if your question requires an understanding of the rarest organisms present.\nThe cost of both preparing the samples and the computational effort required to analyse them can become prohibitively expensive quickly. Before starting you need to think carefully about the question your dataset is trying to answer and how many samples you will need to sequence to get one. This is especially the case when you are trying to include biological or technical replication in your experimental design.\n\n\nAmplicon sequencing\nAn amplicon is a small piece of DNA or RNA that will be amplified through PCR (Polymerase Chain Reaction). Amplicon sequencing is cheaper than WMS because only a small part of the genome is sequenced. This makes it affordable to include additional replicates.\nThe region being amplified needs to be present in all the individuals in the community being characterised, and be highly conserved. 16 rRNA is often used for amplicon sequencing in bacteria for this reason (for eukaryotes 18S rRNA is used instead).\nFor organisms that are well characterised, establishing identity can give you information about functional capacity of the community. For organisms which are not well characterised - and these are common in such samples - we will know little other than relative abundances in the community.\nDespite this, there are workflows such as QIIME2, which are free and community led, which use database annotations of the reference versions of the organisms identified from the amplicon, to suggest what metabolic functions maybe present. The amplicon sequence is also limited because species may have genomic differences, but may be indistinguishable from the amplicon sequence alone. This means that amplicon sequencing can rarely resolve to less than a genus level.\n\n\n\n\n\n\n\n\nAttribute\nAmplicon\nWhole genome metagenomics\n\n\n\n\nCost\nCheap\nExpensive\n\n\nCoverage depth\nHigh\nLower - medium\n\n\nTaxonomy detection\nSpecific to amplicons used\nAll in sample\n\n\nGenome coverage\nOnly region amplified\nAll of genome\n\n\nTaxonomic resolution\nLower\nHigher\n\n\nTurnaround time\nFast\nSlower - more computational time for analysis needed",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Introduction to Metagenomics"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/01-introduction-meta.html#nanopore-long-read-vs-illumina-short-read-data",
    "href": "docs/lesson03-qc-assembly/01-introduction-meta.html#nanopore-long-read-vs-illumina-short-read-data",
    "title": "Introduction to Metagenomics",
    "section": "Nanopore (long-read) vs Illumina (short-read) data",
    "text": "Nanopore (long-read) vs Illumina (short-read) data\nIn our Statistically Useful Experimental Design course we cover how to choose your sequencing platform based on your research question. However, it’s a bit different when doing a whole metagenome assembly experiment.\nFor single genome analyses you can choose between assembling your genome one of two ways: using a reference as a template or de novo (without a reference). Which route you choose depends on whether there is a reasonable reference available for the genome you’re assembling.\nWhen looking at whole metagenome assembly, there will not be a reference available for the metagenome as a whole. You are also unlikely to know what species will be present. All of your assembly stages will therefore be de novo. This makes using long reads preferable as it’s easier to piece them together.\nWe will talk about this more later in this lesson as part of the Genome Assembly section. There are pros and cons to each using both long and short reads, and so using them in combination is usually the best method. These pros and cons are irrespective of the application.\nHowever, for metagenome analysis, if you were to use only short read sequencing for the assembly you would end up with a much more fragmented assembly to work with.\n\n\n\n\n\n\n\n\n\nShort reads\nLong reads\n\n\n\n\nTechnologies\nIllumina\nNanopore and pacbio\n\n\nNumber of reads generated\n800 million paired end*\nDepends on read length, and instrument, but usually only 10s of thousands**\n\n\nBase accuracy\nVery high\nLow\n\n\nEase of assembly\nVery difficult\nEasier\n\n\nFormat output files\nFastq\nFastq, Fast5\n\n\nRead length\n150-300bp\nCommonly 10-30kb***\n\n\n\n* As of July 2022, the NextSeq 550 high-output system runs were capable of generating upto 800 million paired-end reads in one run\n** There are different Nanopore instruments. The smaller instruments, like the minION, will generate far fewer reads. Larger instruments like the promethION will result in ~10-20k reads, but this will vary a lot between samples and their quality. They will never result in millions of reads like the Illumina platforms.\n*** The read length will vary based on the DNA extraction method. If the DNA is very fragmented it will not result in very long reads. In many metagenomes bead beating is required to lyse cells, and so read length will still be longer than Illumina but shorter than non-metagenomic samples sequenced.",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Introduction to Metagenomics"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/01-introduction-meta.html#our-data",
    "href": "docs/lesson03-qc-assembly/01-introduction-meta.html#our-data",
    "title": "Introduction to Metagenomics",
    "section": "Our data",
    "text": "Our data\nThe data we will be using for the rest of the course (and that we’ve been using already!) comes from a 2022 paper titled In-depth characterization of denitrifier communities across different soil ecosystems in the tundra.\nThe paper looks at microbial communities across 43 mountain tundra sites in Northern Finland. The researchers took soil samples from each site and followed a metagenomic analysis workflow to identify the species present in each one. They also measured environmental information such as elevation, soil moisture and N2O fluxes.\nWe will be focusing on data from a single heathland site which has two sets of sequencing data available: one long-read (nanopore) and one short-read (illumina). We’ve already taken a look at these data as part of the previous two lessons - they are saved on our cloud instance under nano_fastq and illumina_fastq respectively.\nTowards the end of the course we’ll compare some of the data from our site with another site to see how they differ in terms of species composition and diversity.",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Introduction to Metagenomics"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/01-introduction-meta.html#bioinformatic-workflows",
    "href": "docs/lesson03-qc-assembly/01-introduction-meta.html#bioinformatic-workflows",
    "title": "Introduction to Metagenomics",
    "section": "Bioinformatic workflows",
    "text": "Bioinformatic workflows\n\n\n\nWhen working with high-throughput sequencing data, the raw reads you get off of the sequencer need to pass through a number of different tools in order to generate your final desired output.\nThe use of this set of tools in a specified order is commonly referred to as a workflow or a pipeline.\nHere is an example of the workflow we will be using for our analysis with a brief description of each step.\n\nSequence reads - obtaining raw reads from a sample via sequencing\nQuality control - assessing quality of the reads and trimming and filtering if necessary.\nMetagenome assembly - piecing together genomes from reads into multiple long “contigs” (overlapping DNA segments)\nBinning - separating out genomes into ‘bins’ containing related contigs\nTaxonomic assignment - assigning taxonomy and functional analysis to sequences/contigs\n\nWorkflows in bioinformatics often adopt a plug-and-play approach so the output of one tool can be easily used as input to another tool. The use of standard data formats in bioinformatics (such as FASTA or FASTQ, which we will be using here) makes this possible. The tools that are used to analyze data at different stages of the workflow are therefore built under the assumption that the data will be provided in a specific format.\nYou can find a more detailed version of the workflow we will be following by going to Extras and selecting Workflow Reference. This diagram contains all of the steps followed over the course alongside program names.",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Introduction to Metagenomics"
    ]
  },
  {
    "objectID": "docs/lesson03-qc-assembly/01-introduction-meta.html#next-steps",
    "href": "docs/lesson03-qc-assembly/01-introduction-meta.html#next-steps",
    "title": "Introduction to Metagenomics",
    "section": "Next steps",
    "text": "Next steps\nHopefully you now feel ready to start following our workflow to analyse our data. We’ll be guiding you through the steps and giving more context for each one as we go along. Let’s go!",
    "crumbs": [
      "Home",
      "QC & Assembly",
      "Introduction to Metagenomics"
    ]
  },
  {
    "objectID": "docs/lesson07-automation-bash-scripts/01-scripting-intro.html",
    "href": "docs/lesson07-automation-bash-scripts/01-scripting-intro.html",
    "title": "Scripting Basics",
    "section": "",
    "text": "Overview\nin construction\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/lesson07-automation-bash-scripts/02-base-automation-script.html",
    "href": "docs/lesson07-automation-bash-scripts/02-base-automation-script.html",
    "title": "Base Environmental Metagenomics Script",
    "section": "",
    "text": "Overview\nin construction\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/01-taxonomic.html",
    "href": "docs/lesson06-taxonomic-annotations/01-taxonomic.html",
    "title": "Taxonomic Assignment",
    "section": "",
    "text": "Taxonomic assignment is the process of assigning a sequence to a specific taxon. In this case we will be assigning our raw short reads but you can also assign metagenome-assembled genomes (MAGs).\nWe are using short reads rather than MAGs because we want to look at the relative differences in abundance between the taxa. This would be much less accurate using MAGs since these are compilations of fragments.\nThese assignments are done by comparing our sequence to a database. These searches can be done in many different ways, and against a variety of databases. There are many programs for doing taxonomic mapping; almost all of them follows one of these strategies:\n\nBLAST: Using BLAST or DIAMOND, these mappers search for the most likely hit for each sequence within a database of genomes (i.e. mapping). This strategy is slow.\nK-mers: A genome database is broken into pieces of length k, so as to be able to search for unique pieces by taxonomic group, from lowest common ancestor (LCA), passing through phylum to species. Then, the algorithm breaks the query sequence (reads, contigs) into pieces of length k, looks for where these are placed within the tree and make the classification with the most probable position.\nMarkers: This method looks for markers of a database made a priori in the sequences to be classified and assigns the taxonomy depending on the hits obtained.\n\n\n\n\n\nFigure 1. Lowest common ancestor assignment example.\n\n\nA key result when you do taxonomic assignment of metagenomes is the abundance of each taxa in your sample. The absolute abundance of a taxon is the number of sequences (reads or contigs within a MAG, depending on how you have performed the searches) assigned to it.\nWe also often use relative abundance, which is the proportion of sequences assigned to it from the total number of sequences rather than absolute abundances. This is because the absolute abundance can be misleading and samples can be sequenced to different depths, and the relative abundance makes it easier to compare between samples accounting for sequencing depth differences.\nIt is important to be aware of the many biases that that can skew the abundances along the metagenomics workflow, shown in the figure, and that because of them we may not be obtaining the real abundance of the organisms in the sample.\n\n\n\n\nFigure 2. Abundance biases during a metagenomics protocol.",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Taxonomic Assignment"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/01-taxonomic.html#what-is-taxonomic-assignment",
    "href": "docs/lesson06-taxonomic-annotations/01-taxonomic.html#what-is-taxonomic-assignment",
    "title": "Taxonomic Assignment",
    "section": "",
    "text": "Taxonomic assignment is the process of assigning a sequence to a specific taxon. In this case we will be assigning our raw short reads but you can also assign metagenome-assembled genomes (MAGs).\nWe are using short reads rather than MAGs because we want to look at the relative differences in abundance between the taxa. This would be much less accurate using MAGs since these are compilations of fragments.\nThese assignments are done by comparing our sequence to a database. These searches can be done in many different ways, and against a variety of databases. There are many programs for doing taxonomic mapping; almost all of them follows one of these strategies:\n\nBLAST: Using BLAST or DIAMOND, these mappers search for the most likely hit for each sequence within a database of genomes (i.e. mapping). This strategy is slow.\nK-mers: A genome database is broken into pieces of length k, so as to be able to search for unique pieces by taxonomic group, from lowest common ancestor (LCA), passing through phylum to species. Then, the algorithm breaks the query sequence (reads, contigs) into pieces of length k, looks for where these are placed within the tree and make the classification with the most probable position.\nMarkers: This method looks for markers of a database made a priori in the sequences to be classified and assigns the taxonomy depending on the hits obtained.\n\n\n\n\n\nFigure 1. Lowest common ancestor assignment example.\n\n\nA key result when you do taxonomic assignment of metagenomes is the abundance of each taxa in your sample. The absolute abundance of a taxon is the number of sequences (reads or contigs within a MAG, depending on how you have performed the searches) assigned to it.\nWe also often use relative abundance, which is the proportion of sequences assigned to it from the total number of sequences rather than absolute abundances. This is because the absolute abundance can be misleading and samples can be sequenced to different depths, and the relative abundance makes it easier to compare between samples accounting for sequencing depth differences.\nIt is important to be aware of the many biases that that can skew the abundances along the metagenomics workflow, shown in the figure, and that because of them we may not be obtaining the real abundance of the organisms in the sample.\n\n\n\n\nFigure 2. Abundance biases during a metagenomics protocol.",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Taxonomic Assignment"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/01-taxonomic.html#using-kraken-2",
    "href": "docs/lesson06-taxonomic-annotations/01-taxonomic.html#using-kraken-2",
    "title": "Taxonomic Assignment",
    "section": "Using Kraken 2",
    "text": "Using Kraken 2\nWe will be using the command line program Kraken2 to do our taxonomic assignment. Kraken 2 is the newest version of Kraken, a taxonomic classification system using exact k-mer matches to achieve high accuracy and fast classification speeds.\nTaxonomic assignment can be done on MAGs however we will be going back to use our raw short reads here.\nkraken2 is already installed on our instance so we can look at the kraken2 help.\n\n\nCode\n\n kraken2  \n\n\n\n\n\n\n\nOutput — Kraken2 help documentation\n\n\n\n\n\n\n\nOutput\n\nUsage: kraken2 [options] &lt;filename(s)&gt;\n\nOptions:\n   --db NAME               Name for Kraken 2 DB\n                           (default: none)\n   --threads NUM           Number of threads (default: 1)\n   --quick                 Quick operation (use first hit or hits)\n   --unclassified-out FILENAME\n                           Print unclassified sequences to filename\n   --classified-out FILENAME\n                           Print classified sequences to filename\n   --output FILENAME       Print output to filename (default: stdout); \"-\" will\n                           suppress normal output\n   --confidence FLOAT      Confidence score threshold (default: 0.0); must be\n                           in [0, 1].\n   --minimum-base-quality NUM\n                           Minimum base quality used in classification (def: 0,\n                           only effective with FASTQ input).\n   --report FILENAME       Print a report with aggregrate counts/clade to file\n   --use-mpa-style         With --report, format report output like Kraken 1's\n                           kraken-mpa-report\n   --report-zero-counts    With --report, report counts for ALL taxa, even if\n                           counts are zero\n   --report-minimizer-data With --report, report minimizer and distinct minimizer\n                           count information in addition to normal Kraken report\n   --memory-mapping        Avoids loading database into RAM\n   --paired                The filenames provided have paired-end reads\n   --use-names             Print scientific names instead of just taxids\n   --gzip-compressed       Input files are compressed with gzip\n   --bzip2-compressed      Input files are compressed with bzip2\n   --minimum-hit-groups NUM\n                           Minimum number of hit groups (overlapping k-mers\n                           sharing the same minimizer) needed to make a call\n                           (default: 2)\n   --help                  Print this message\n   --version               Print version information\n\nIf none of the *-compressed flags are specified, and the filename provided\nis a regular file, automatic format detection is attempted.\n\n\n\n\nIn addition to our input files we will need a database (-db) with which to compare them. There are several different databases available for kraken2. Some of these are larger and much more comprehensive, and some are more specific. There are also instructions on how to generate a database of your own.\n\n\n\n\n\n\nIt’s very important to know your database!\n\n\n\nThe database you use will determine the result you get for your data. Imagine you are searching for a lineage that was recently discovered and it is not part of the available databases. Would you find it? Make sure you keep a note of what database you have used and when you downloaded it or when it was last updated.\n\n\nYou can view and download many of the common Kraken2 databases on this site. We will be using Standard-8 which is already pre installed on the instance.",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Taxonomic Assignment"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/01-taxonomic.html#taxonomic-assignment-of-an-assembly",
    "href": "docs/lesson06-taxonomic-annotations/01-taxonomic.html#taxonomic-assignment-of-an-assembly",
    "title": "Taxonomic Assignment",
    "section": "Taxonomic assignment of an assembly",
    "text": "Taxonomic assignment of an assembly\nFirst, we need to make a directory for the kraken output and then we can run our kraken command.\nWe use the following flags:\n\n--output to specify the location of the .kraken output file\n--report to specify the location of the .report output file\n--threads to specify the number of threads to use\n--minimum-base-quality to exclude bases with a PHRED quality score below a certain threshold (most important if you haven’t filtered your short reads)\n--db to tell Kraken2 where to find the database to compare the reads to\n\n\n\nCode\n\ncd ~/cs_course\nmkdir results/taxonomy\n\nkraken2 --db databases/kraken_20220926/ --output results/taxonomy/ERR4998593.kraken --report results/taxonomy/ERR4998593.report --minimum-base-quality 30 --threads 8 data/illumina_fastq/ERR4998593_1.fastq data/illumina_fastq/ERR4998593_2.fastq\n\nThis should take around 3 - 5 minutes to run so we will run it in the foreground.\nYou should see an output similar to below:\n\n\nOutput\n\nLoading database information... done.\n68527220 sequences (10347.61 Mbp) processed in 109.270s (37628.2 Kseq/m, 5681.86 Mbp/m).\n  616037 sequences classified (0.90%)\n  67911183 sequences unclassified (99.10%)\n\nThis command generates two outputs, a .kraken and a .report file. Let’s look at the top of these files with the following command:\n\n\nCode\n\nhead results/taxonomy/ERR4998593.kraken  \n\n\n\nOutput\n\nU       ERR4998593.40838091     0       151     A:117\nU       ERR4998593.57624042     0       151     0:113 A:4\nU       ERR4998593.3    0       151     A:1 0:39 A:34 0:43\nU       ERR4998593.4    0       151     A:1 0:8 A:34 0:73 A:1\nU       ERR4998593.34339006     0       151     A:20 0:41 A:56\nU       ERR4998593.6    0       151     A:1 0:17 A:99\nU       ERR4998593.59019952     0       151     A:117\nC       ERR4998593.34862640     2686094 151     0:9 A:62 0:6 2686094:5 0:10 28211:1 0:24\nU       ERR4998593.63611176     0       151     0:1 A:42 0:58 A:16\nU       ERR4998593.57807180     0       151     A:5 0:112\n\nThis gives us information about every read in the raw reads. As we can see, the kraken file is not very readable. So let’s look at the report file instead:\n\n\nCode\n\nless results/taxonomy/ERR4998593.report\n\n\n\nOutput\n\n99.10  67911183        67911183        U       0       unclassified\n  0.90  616037  78      R       1       root\n  0.90  615923  1416    R1      131567    cellular organisms\n  0.89  612469  36540   D       2           Bacteria\n  0.53  365022  26922   P       1224          Proteobacteria\n  0.42  289043  21461   C       28211           Alphaproteobacteria\n  0.35  240214  23691   O       356               Hyphomicrobiales\n  0.27  181802  17554   F       41294               Nitrobacteraceae\n  0.23  154769  60679   G       374                   Bradyrhizobium\n  0.06  41382   12771   G1      2631580                 unclassified Bradyrhizobium\n  0.00  2594    2594    S       2782665                   Bradyrhizobium sp. 200\n  0.00  2553    2553    S       2782654                   Bradyrhizobium sp. 186\n  0.00  2520    2520    S       2782641                   Bradyrhizobium sp. 170\n  0.00  1549    1549    S       858422                    Bradyrhizobium sp. CCBAU 051011\n  0.00  1405    1405    S       2840469                   Bradyrhizobium sp. S2-20-1\n...\n\n\n\n\n\n\n\nReading a Kraken report\n\n\n\n\nPercentage of reads covered by the clade rooted at this taxon\nNumber of reads covered by the clade rooted at this taxon\nNumber of reads assigned directly to this taxon\nA rank code, indicating (U)nclassified, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. All other ranks are simply ‘-’.\nNCBI taxonomy ID\nIndented scientific name\n\n\n\nIn our case, 99.1% of the reads are unclassified. These reads either didn’t meet quality threshold or were not identified in the database. That leaves the other 0.9% as our classified reads.\n0.9% of reads classified seems small, but remember that 0.9% of our reads is still over 600,000 reads successfully classified! We can still get a good grasp of the kinds of species present, even if it isn’t a definitive list.\nThis data is real environmental data, and this means the quality of the DNA is likely to be low - it may have degraded in the time between sampling and extraction, or been damaged during the extraction process. In addition, we would see a higher percentage of our reads classified if we ran them through a bigger database - the Standard-8 database we used is fairly small. We [the course writers] saw up to 14% of reads classified when we used Kraken on the same data with a much bigger database and a more powerful computer.\nAs the report is nearly 10,000 lines, we will explore it with Pavian, rather than by hand.",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Taxonomic Assignment"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/01-taxonomic.html#visualisation-of-taxonomic-assignment-results",
    "href": "docs/lesson06-taxonomic-annotations/01-taxonomic.html#visualisation-of-taxonomic-assignment-results",
    "title": "Taxonomic Assignment",
    "section": "Visualisation of taxonomic assignment results",
    "text": "Visualisation of taxonomic assignment results\n\nPavian\nPavian is a tool for the interactive visualisation of metagenomics data and allows the comparison of multiple samples. Pavian can be installed locally but we will use the browser version of Pavian.\nFirst we need to download our ERR4998593.report file from our AWS instance to our local computer. Launch a GitBash window or terminal which is logged into your local computer, from the cloudspan folder. Then use scp to fetch the report.\n\n\nCode\n\nscp -i login-key-instanceNNN.pem csuser@instanceNNN.cloud-span.aws.york.ac.uk.:~/cs_course/results/taxonomy/ERR4998593.report .\n\nCheck you have included the . on the end meaning copy the file ‘to here’.\nGo to the Pavian website, click on Browse and upload the ERR4998593.report file you have just downloaded.\n\n\n\n\nPavian website showing the upload point.\n\n\n\n\n\n\nPavian website once the sample has uploaded.\n\n\nOnce the file is uploaded we can explore the output generated. The column on the left has multiple different options. As we only have one sample and don’t have an alignment to view, only the “Results Overview” and “Sample” tabs are of interest to us.\nAt the bottom of this column there is also an option to “Generate HTML report”. This is a very good option if you want to share your results with others. We have used that option to generate one here for the ERR4998593 data in order to share our results. If you haven’t been able to generate a Pavian output you can view our exported example here: ERR4998593-pavian-report.html (note this looks a bit different to the website version).\nThe Results Overview tab shows us how many reads have been classified in our sample(s). From this we can see what proportion of our reads were classified as being bacterial, viral, fungal etc.\nOn the Sample tab we can see a Sankey diagram which shows us the proportion of our sample that has been classified at each taxa level. If you click on the “Configure Sankey” button you can play with the settings to make the diagram easier to view. Since there are many different species in our Sankey, you might want to try increasing the height of the figure and the number of taxa at each level, to get a broader overview of the species present.\nYou can also view the Sankey diagram of our example here: sankey-ERR4998593.report.html\n\n\n\n\n.\n\n\n\n\n\n\n\n\nExercise 1: Comparison\n\n\n\nHave a look at the Results section of our source paper, where the authors describe some of the genera (subsection 2) and phyla (subsection 3) they documented. Which of our identified taxa match up with the paper’s?\nYou might find it helpful to import the data from ERR4998593.report file we downloaded into a spreadsheet program (as previously described) to take a more in-depth look at the taxa identified. Remember that you can use filtering to see specific taxonomic levels (e.g. only phyla or only genera). You could also use the search function. Here’s one we made earlier.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe subsection titled “Differences in microbial community structure across soils ecosystems” lists examples genera present in heathland soils (like our sample): Acidipila/Silvibacterium, Bryobacter, Granulicella, Acidothermus, Conexibacter, Mycobacterium, Mucilaginibacter, Bradyrhizobium, and Roseiarcus. Here’s how many of our sequences belong to these genera:\n\n\n\nGenus\nSequences assigned\n\n\n\n\nBradyrhizobium\n154769\n\n\nMycobacterium\n10863\n\n\nGranulicella\n1344\n\n\nConexibacter\n2272\n\n\nMucilaginibacter\n197\n\n\nAcidothermus\n122\n\n\nBryobacter\n0\n\n\nAcidipila/Silvibacterium\n0\n\n\nRoseiarcus\n0\n\n\n\nNote that 298 sequences are assigned to Bryobacteraceae, the family to which the genus Bryobacter belongs. However, all 298 of these sequences are assigned to the genus Paludibaculum, another genus in the Bryobacteraceae family, so we can still confidently say that there are no species belonging to the Bryobacter genus present.\nThe subsection titled “A manually curated genomic database from tundra soil metagenomes” describes more generally the most represented phyla across the MAGs generated: Acidobacteriota (n = 172), Actinobacteriota (n = 163), Proteobacteria (Alphaproteobacteria, n = 54; Gammaproteobacteria, n = 39), Chloroflexota (n = 84), and Verrucomicrobiota (n = 43). All of these phyla are present in our sample too, albeit in different proportions:\n\n\n\nPhylum\nSequences assigned\n\n\n\n\nAlphaproteobacteria\n289043\n\n\nActinobacteria\n187653\n\n\nGammaproteobacteria\n18733\n\n\nAcidobacteria\n5500\n\n\nVerrucomicrobia\n778\n\n\nChloroflexi\n122\n\n\n\nThe same subsections states that “in general, barren, heathland, and meadow soils were dominated by the same set of MAGs”: Acidobacteriota, Actinobacteria and Proteobacteria. This holds true for our sample, which is taken from a heathland site. Several of the specific genera mentioned (e.g. unclassified genera in class Acidobacteriae, Mycobacterium, Bradyrhizobium, unclassified Xantherobacteraceae and Steroidobacteraceae) are also present.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Taxonomic level of assignment\n\n\n\nWhat do you think is harder to assign, a species (like E. coli) or a phylum (like Proteobacteria)?\n\n\n\n\n\n\n\n\nOther software\n\n\n\nKrona is a hierarchical data visualization software. Krona allows data to be explored with zooming, multi-layered pie charts and includes support for several bioinformatics tools and raw data formats.\nKrona is used in the MG-RAST analysis pipeline which you can read more about at the end of this course.",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Taxonomic Assignment"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html",
    "href": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html",
    "title": "Diversity Tackled With R",
    "section": "",
    "text": "Once we know the taxonomic composition of our metagenomic sequencing data we can characterise them by their diversity. In this episode we will first define what we mean by diversity and then calculate the α diversity in our sample. We will also calculate the diversity of another sample.",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Diversity Tackled With R"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html#what-is-diversity",
    "href": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html#what-is-diversity",
    "title": "Diversity Tackled With R",
    "section": "What is diversity?",
    "text": "What is diversity?\nSpecies diversity is the number of species in a system and the relative abundance of each of those species. It can be defined on three different scales (Whittaker, 1960).\n\nthe total species diversity in an ecosystem known as gamma (γ) diversity\naverage diversity at a local site, known as alpha (α) diversity\nthe difference in diversity between local sites, known as beta (β) diversity\n\nA metagenome can be considered a local site. In this episode we will calculate α diversity (that within a metagenome) and β diversity (that between metagenomes).\n\nAlpha (α) diversity\nThe simplest measure of α diversity is the number of species, or species richness. However, most indices of α diversity take into account both the number of species and their relative abundances, or species evenness. Different diversity indices weight these two components differently.\n\n\n\n\n\n\n\n\n\nα Diversity Index\nDescription\nCalculation\nWhere\n\n\n\n\nShannon (H)\nEstimation of species richness and species evenness. More weight on richness.\n\\(H = - \\sum_{i=1}^{S} p_{i} \\ln{p_{i}}\\)\n\\(S\\) is the number of OTUs and \\(p_{i}\\) is the proportion of the community represented by OTU\n\n\nSimpson’s (D)\nEstimation of species richness and species evenness. More weight on evenness.\n\\(D = \\frac{1}{\\sum_{i=1}^{S} p_{i}^{2}}\\)\n\\(S\\) is Total number of the species in the community and \\(p_{i}\\) is the proportion of community represented by OTU i\n\n\nChao1\nAbundance based on species represented by a single individual (singletons) and two individuals (doubletons).\n\\(S_{chao1} = S_{Obs} + \\frac{F_{1} \\times (F_{1} - 1)}{2 \\times (F_{2} + 1)}\\)\n\\(F_{1}\\) and \\(F_{2}\\) are the counts of singletons and doubletons respectively and \\(S_{chao1}=S_{Obs}\\) is the number of observed species\n\n\n\n\n\n\n\nFigure 1. Alpha diversity represented by fish in a pond. Here, alpha diversity is measured in the simplest way using species richness.\n\n\n\n\nBeta (β) diversity\nβ diversity measures how different two or more communities are in their richness, evenness or both.\n\n\n\n\n\n\n\nβ Diversity Index\nDescription\n\n\n\n\nBray–Curtis dissimilarity\nThe compositional dissimilarity between two metagenomes, based on counts in each metagenome. Ranges from 0 (the two metagenomes have the same species composition) to 1 (the two metagenomes do not share any species). Bray–Curtis dissimilarity emphasises abundance.\n\n\nJaccard distance\nAlso ranges from 0 (the two metagenomes have the same species) to 1 (the two metagenomes do not share any species) but is based on the presence or absence of species only. This means it emphasises richness.\n\n\nUniFrac\nDiffers from the Bray-Curtis dissimilarity and Jaccard distance by including the relatedness between taxa in a metagenome. Measures the phylogenetic distance between metagenomes as the proportion of unshared phylogenetic tree branches. Weighted-Unifrac takes into account the relative abundance of taxa shared between samples; unweighted-Unifrac only considers presence or absence.\n\n\n\nFigure 2 shows α and the β diversity for three lakes. The most simple way to calculate the β diversity is to calculate the number of species that are unique in two lakes. For example, the number of species in Lake A (the α diversity) is 3 and 1 of these is also found in Lake C; the number of species in Lake C is 2 and 1 of these is also in Lake A. The β diversity between A and C is calculated as (3 - 1) + (2 - 1) = 3\n\n\n\n\nFigure 2. Alpha and Beta diversity represented by fishes in a pond.\n\n\n\n\n\n\n\n\nExercise 1:\n\n\n\nIn the next picture there are two lakes with different fish species: \n\n\n\n.\n\n\nWhich of the options below is true: 1. α diversity of A = 4, α diversity of B = 3, β diversity between A and B = 1 2. α diversity of A = 4, α diversity of B = 3, β diversity between A and B = 5 3. α diversity of A = 9, α diversity of B = 7, β diversity between A and B= 16\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAnswer: 2. α diversity of A = 4, α diversity of B = 3, β diversity between A and B = 5 The number of species in Lake A (the α diversity) is 4 and 1 of these is also found in Lake B; the number of species in Lake B is 3 and 1 of these is also in Lake A. The β diversity between A and C is calculated as (4 - 1) + (3 - 1) = 5.",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Diversity Tackled With R"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html#how-do-we-calculate-diversity-from-metagenomic-samples",
    "href": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html#how-do-we-calculate-diversity-from-metagenomic-samples",
    "title": "Diversity Tackled With R",
    "section": "How do we calculate diversity from metagenomic samples?",
    "text": "How do we calculate diversity from metagenomic samples?\nWe will calculate the diversity in our heathland sample ERR4998593 and compare it with data from another site from the same study: ERR4998600. This is a fenland site (so lower and wetter than our heathland site).\nThere are 2 steps needed to calculate the diversity of our samples.\n\nCreate a Biological Observation Matrix, BIOM table, from the Kraken output. A BIOM table is a matrix of counts with samples in the columns and taxa in the rows. The values in the matrix are the counts of that taxa in that sample.\nAnalyse the BIOM table to generate diversity indices and relative abundance plots.",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Diversity Tackled With R"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html#what-part-of-the-kraken-output-do-we-need",
    "href": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html#what-part-of-the-kraken-output-do-we-need",
    "title": "Diversity Tackled With R",
    "section": "What part of the Kraken output do we need?",
    "text": "What part of the Kraken output do we need?\nWe will use a command-line program called kraken-biom to convert our Kraken output into a BIOM table. kraken-biom takes the .report output of Kraken and creates a BIOM table in .biom format.\nMove in to your taxonomy folder\n\n\nCode\n\n cd ~/cs_course/results/taxonomy\n\nList the files\n\n\nCode\n\n ls -l\n\n\n\nOutput\n\n-rw-rw-r-- 1 csuser csuser 3935007137 Apr  9 09:16 ERR4998593.kraken\n-rw-rw-r-- 1 csuser csuser     424101 Apr  9 09:16 ERR4998593.report\n\nAs we saw in the previous episode, .kraken and .report are the output files generated by Kraken.\nWe will also need the ERR4998600 Kraken report. We have put this in our GitHub repo and it can be copied into your taxonomy directory on the instance using wget. This is a useful command for retrieving files from web servers and simply requires wget followed by the web address the file is stored at.\n\n\nCode\n\nwget https://cloud-span.github.io/nerc-metagenomics06-taxonomic-anno/files/ERR4998600.report\n\nYou should ls to check that this file has been downloaded.",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Diversity Tackled With R"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html#create-the-biom-table",
    "href": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html#create-the-biom-table",
    "title": "Diversity Tackled With R",
    "section": "Create the BIOM table",
    "text": "Create the BIOM table\nkraken-biom has many options which you can see with the help command. However, we only need to specify an output format --fmt of json to use the file in the next step.\n\n\n\n\n\n\nkraken-biom -h output\n\n\n\n\n\n\n\nOutput\n\nusage: kraken-biom [-h] [--max {D,P,C,O,F,G,S}] [--min {D,P,C,O,F,G,S}] [-o OUTPUT_FP] [--otu_fp OTU_FP] [--fmt {hdf5,json,tsv}] [--gzip] [--version] [-v]\n                    kraken_reports [kraken_reports ...]\n                         Set the output format of the BIOM table. Default is HDF5.\nCreate BIOM-format tables (http://biom-format.org) from Kraken output  BIOM (v2.x) files are internally compressed by default, so this option is not needed when\n(http://ccb.jhu.edu/software/kraken/).mt hdf5.\n   --version             show program's version number and exit\nThe program takes as input, one or more files output from the kraken-report\ntool. Each file is parsed and the counts for each OTU (operational taxonomic\nunit) are recorded, along with database ID (e.g. NCBI), and lineage. The\nextracted data are then stored in a BIOM table where each count is linked\nto the Sample and OTU it belongs to. Sample IDs are extracted from the input\nfilenames (everything up to the '.').\n\nOTUs are defined by the --max and --min arguments. By default these are\nset to Order and Species respectively. This means that counts assigned\ndirectly to an Order, Family, or Genus are recorded under the associated\nOTU ID, and counts assigned at or below the Species level are assigned to\nthe OTU ID for the species. Setting a minimum rank below Species is not yet\navailable.\n\nThe BIOM format currently has two major versions. Version 1.0 uses the\nJSON (JavaScript Object Notation) format as a base. Version 2.x uses the\nHDF5 (Hierarchical Data Format v5) as a base. The output format can be\nspecified with the --fmt option. Note that a tab-separated (tsv) output\nformat is also available. The resulting file will not contain most of the\nmetadata, but can be opened by spreadsheet programs.\n\nVersion 2 of the BIOM format is used by default for output, but requires the\nPython library 'h5py'. If the library is not installed, kraken-biom will\nautomatically switch to using version 1.0. Note that the output can\noptionally be compressed with gzip (--gzip) for version 1.0 and TSV files.\nVersion 2 files are automatically compressed.\n\nUsage examples\n--------------\n1. Basic usage with default parameters:\n\n$ kraken-biom.py S1.txt S2.txt\n\n   This produces a compressed BIOM 2.1 file: table.biom\n\n2. BIOM v1.0 output:\n\n$ kraken-biom.py S1.txt S2.txt --fmt json\n\n   Produces a BIOM 1.0 file: table.biom\n\n3. Compressed TSV output:\n\n$ kraken-biom.py S1.txt S2.txt --fmt tsv --gzip -o table.tsv\n\n   Produces a TSV file: table.tsv.gz\n\n4. Change the max and min OTU levels to Class and Genus:\n\n$ kraken-biom.py S1.txt S2.txt --max C --min G\n\nProgram arguments\n-----------------\n\npositional arguments:\n   kraken_reports        Results files from the kraken-report tool.\n\noptional arguments:\n   -h, --help            show this help message and exit\n   --max {D,P,C,O,F,G,S}\n                         Assigned reads will be recorded only if they are at or below max rank.\n                         Default: O.\n   --min {D,P,C,O,F,G,S}\n                         Reads assigned at and below min rank will be recorded as being assigned\n                         to the min rank level. Default: S.\n   -o OUTPUT_FP, --output_fp OUTPUT_FP\n                         Path to the BIOM-format file. By default, the table will be in the HDF5 \n                         BIOM 2.x format. Users can output to a different format using the --fmt \n                         option. The output can also be gzipped using the --gzip option. Default \n                         path is: ./table.biom\n   --otu_fp OTU_FP       Create a file containing just the (NCBI) OTU IDs for use with a service\n                         such as phyloT (http://phylot.biobyte.de/) to generate a phylogenetic \n                         tree for use in downstream analysis such as UniFrac, iTol (itol.embl.\n                         de), or PhyloToAST (phylotoast.org).\n   --fmt {hdf5,json,tsv} Set the output format of the BIOM table. Default is HDF5.\n   --gzip                Compress the output BIOM table with gzip. HDF5 BIOM (v2.x) files are \n                         internally compressed by default, so this option is not needed when \n                         specifying --fmt hdf5.\n   --version             show program's version number and exit\n   -v, --verbose         Prints status messages during program execution.\n\n\n\n\nWith the next command, we are going to create a table in Biom format from our two Kraken reports: ERR4998593.report and ERR4998600.report.\nWe customise the command with a couple of flags:\n\n--fmt json tells kraken-biom that we want the output table to be in JSON format as opposed to the default HDF5 BIOM2.x format\n-o metagenome.biom means our output table will be named metagenome.biom\n\n\n\nCode\n\ncd ~/cs_course\nkraken-biom results/taxonomy/ERR4998593.report results/taxonomy/ERR4998600.report --fmt json -o results/taxonomy/metagenome.biom\n\nkraken-biom uses both reports to generate the BIOM table. The BIOM table is in a file called metagenome.biom.\n\n\nCode\n\n ls -l results/taxonomy\n\n\n\nOutput\n\n-rw-rw-r-- 1 csuser csuser 3935007137 Apr  9 09:16 ERR4998593.kraken\n-rw-rw-r-- 1 csuser csuser     424101 Apr  9 09:16 ERR4998593.report\n-rw-rw-r-- 1 csuser csuser     404232 Apr  9 09:30 ERR4998600.report\n-rw-rw-r-- 1 csuser csuser     741259 Apr  9 10:22 metagenome.biom",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Diversity Tackled With R"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html#analyse-the-biom-table-using-r",
    "href": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html#analyse-the-biom-table-using-r",
    "title": "Diversity Tackled With R",
    "section": "Analyse the BIOM table using R",
    "text": "Analyse the BIOM table using R\nWe will be using an R package called phyloseq to analyse our biom file. Other software for analyses of diversity include Qiime2, MEGAN and the R package Vegan\nIf you are very familiar with R and RStudio and already have them on your machine, you may want to install the packages needed and download the metagenome.biom file to do the analysis on your own computer. However, you do not need prior experience with R and RStudio for this part of the course: we have set up the analysis in RStudio Cloud, an online version of RStudio which has everything you need, including the code. We have given instructions for both options.\n\nOption A: I know R and RStudio.\nIf you know R and RStudio and already have them on your machine you may want to use this option.\n\nOpen RStudio\n\nInstall the packages\nYou will need phyloseq, a Bioconductor package, and the tidyverse and ggvenn packages. Bioconductor packages are installed using the install() function from the BioManager package so we first install that, then phyloseq, tidyverse and ggvenn:\n\n\nCode (R)\n\n&gt; install.packages(\"BiocManager\")\n&gt; BiocManager::install(\"phyloseq\")\n&gt; install.packages(\"tidyverse\")\n&gt; install.packages(\"ggvenn\")\n\nMake an RStudio project\nMake an RStudio project workshop by clicking on the drop-down menu on top right where it says Project: (None) and choosing New Project and then New Directory, then New Project. In the “Create project as a subdirectory” box, use Browse to navigate to the “cloudspan” folder. Name the RStudio Project ‘diversity’.\nDownload the metagenome.biom file to the project folder\nDownload the file to the project folder using scp. Use a terminal that is not logged into the cloud instance and ensure you are in your cloudspan directory. Use scp to copy the file to the diversity folder - the command will look something like:\n\n\nCode\n\nscp -i login-key-instanceNNN.pem csuser@instanceNNN.cloud-span.aws.york.ac.uk:~/cs_course/results/taxonomy/metagenome.biom diversity\n\nRemember to replace NNN with the instance number specific to you.\nOpen a new script.\n\nNow go to Start the analysis.\n\n\nOption B: I don’t know R and RStudio.\nIf you don’t know R and RStudio we suggest you to use this option. We have installed the packages needed and put the data, metagenome.biom, and a script, analysis.R, in the RStudio Cloud Project.\n\nMake an RStudio Cloud account by visiting https://rstudio.cloud/ and click Get Started for Free. We recommend signing up with your Google account if you use one.\n\nOpen the project we have set up: cloud-span-metagenomics. You’ll get a message saying “Deploying project”. This will take a few seconds.\nOpen analysis.R from the Files pane on the bottom right of the display.\n\n\n\nStart the Analysis\nIf you are using RStudio Cloud, we will run through the code in analysis.R line by line. If you are using RStudio on your own machine you can type in the commands or copy them from analysis.R\nFirst load the packages we need. Put your cursor on the line you want to run and press CTRLENTER\n\n\nCode (R)\n\nlibrary(\"phyloseq\")\nlibrary(\"tidyverse\")\nlibrary(\"ggvenn\")\n\nNow import the data in metagenome.biom into R using the import_biom() function from phyloseq\n\n\nCode (R)\n\nbiom_metagenome &lt;- import_biom(\"metagenome.biom\")\n\nThis command produces no output in the console but created a special class of R object which is defined by the phyloseq package and called it biom_metagenome. Click on the object name, biom_metagenome, in the Environment pane (top right), see the screenshot below. This will open a view of the object in the same pane as your script.\nA phyloseq object is a special object in R. It has five parts, called ‘slots’ which you can see listed in the object view. These are otu_table, tax_table, sam_data, phy_tree and refseq. In our case, sam_data, phy_tree and refseq are empty. The useful data are in otu_tableandtax_table.\n\n\n  \n\nReturn to your script (click on the tab). Typing biom_metagenome will give some summary information about the biom_metagenome object:\n\n\nCode (R)\n\nbiom_metagenome\n\n\n\nOutput\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 7637 taxa and 2 samples ]\ntax_table()   Taxonomy Table:    [ 7637 taxa by 7 taxonomic ranks ]\n\nThe line starting otu_table tells us we have two samples - these are ERR4998593 and ERR4998600 - with a total of 7637 taxa. The tax_table again tells us how many taxa we have. The seven ranks indicates that we have some identifications down to species level. The taxonomic ranks are from the classification system of taxa from the most general (kingdom) to the most specific (species): kingdom/domain, phylum, class, order, family, genus, species.\nWe can view the tax_table with:\n\n\nCode (R)\n\nView(biom_metagenome@tax_table)\n\nThis table has the OTU identity in the row names and the samples in the columns. The values in the columns are the abundance of that OTU in that sample.\n\n\n\n\nFigure 3. Table of the OTU data from our biom_metagenome object.\n\n\nTo make downstream analysis easier for us we will remove the prefix (e.g. f__) on each item. This contains information about the rank of the assigned taxonomy, we don’t want to lose this information so will and instead rename the header of each column of the DataFrame to contain this information.\nTo remove unnecessary characters we are going to use command substring().\nThis command is useful to extract or replace characters in a vector. To use the command, we have to indicate the vector (x) followed by the first element to replace or extract (first) and the last element to be replaced (last). For instance: substring (x, first, last). If a last position is not used it will be set to the end of the string.\nThe prefix for each item in biom_metagenome is made up of a letter and two underscores, for example: o__Bacillales. In this case “Bacillales” starts at position 4 with a B. So to remove the unnecessary characters we will use the following code:\n\n\nCode (R)\n\nbiom_metagenome@tax_table &lt;- substring(biom_metagenome@tax_table, 4)\n\nLet’s change the names of the columns too:\n\n\nCode (R)\n\ncolnames(biom_metagenome@tax_table) &lt;- c(\"Kingdom\",\n           \"Phylum\",\n           \"Class\",\n           \"Order\",\n           \"Family\",\n           \"Genus\",\n           \"Species\")\n\nCheck it worked:\n\n\nCode (R)\n\nView(biom_metagenome@tax_table)\n\n\n\n\n\n\nFigure 4. Table of the OTU data from our biom_metagenome object. With corrections.\n\n\nHow many OTUs are in each kingdom? We can find out by combining some commands. We need to:\n\nturn the tax_table into a data frame (a useful data structure in R)\ngroup by the Kingdom column\nsummarise by counting the number of rows for each Kingdom\n\nThis can be achieved with the following command:\n\n\nCode (R)\n\nbiom_metagenome@tax_table %&gt;% \n  data.frame() %&gt;% \n  group_by(Kingdom) %&gt;% \n  summarise(n = length(Kingdom)) \n\n\n\nOutput\n\n# A tibble: 4 × 2\n  Kingdom       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Archaea     339\n2 Bacteria   7231\n3 Eukaryota     1\n4 Viruses      66\n\nMost things are bacteria!\nWe can explore how many phlya we have and how many OTU there are in each phlya in a similar way. This time we will use View() to see the whole table because it won’t all print to the console\n\nturn the tax_table into a data frame (a useful data structure in R)\ngroup by the Phylum column\nsummarise by counting the number of rows for each phylum\nviewing the result\n\nThis can be achieved with the following command:\n\n\nCode (R)\n\nbiom_metagenome@tax_table %&gt;% \n data.frame() %&gt;% \n group_by(Phylum) %&gt;% \n summarise(n = length(Phylum)) %&gt;% \n View()\n\nThis shows us a table with a phylum, and the number times it appeared, in each row. The number of phyla is given by the number of rows in this table. By defualt, the table is sorted alphabetically by phylum. We can sort by frequency by clicking on the ‘n’ column. There are 3471 Proteobacteria and 1571 Actinobacteria for example.\n\n\n\n\n\n\n\n\nExercise 2: Explore the Orders\n\n\n\nAdapt the code to explore how many Orders we have and how many OTU there are in each order.\na) How many orders are there?\nb) What is the most common order?\nc) How many OTUs did not get down to order level?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou should the use the column name ‘Order’ instead of ‘Phylum’ in the code\n\n\nCode (R)\n\nbiom_metagenome@tax_table %&gt;% \n   data.frame() %&gt;% \n   group_by(Order) %&gt;% \n   summarise(n = length(Order)) %&gt;% \n   View()\n\n\n\nThis is the number of rows in the table\n\n\nHyphomicrobiales. Sorting the column n will bring this to the top. Hyphomicrobiales appears 480 times\n\n\nIf an OTU has not been identified to order level the order column will be blank. The table shows there were 66 such cases (you can find this more easily by sorting the column Order).",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Diversity Tackled With R"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html#plot-alpha-diversity",
    "href": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html#plot-alpha-diversity",
    "title": "Diversity Tackled With R",
    "section": "Plot alpha diversity",
    "text": "Plot alpha diversity\nWe want to explore the bacterial diversity of our samples, so we will remove all of the non-bacterial organisms. To do this we will generate a subset of all bacterial groups and save them.\n\n\nCode (R)\n\nbac_biom_metagenome &lt;- subset_taxa(biom_metagenome, Kingdom == \"Bacteria\")\n\nNow let’s look at some statistics of our bacterial metagenomes:\n\n\nCode (R)\n\nbac_biom_metagenome\n\n\n\nOutput\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 7231 taxa and 2 samples ]\ntax_table()   Taxonomy Table:    [ 7231 taxa by 7 taxonomic ranks ]\n\nphyloseq includes a function to find the sample names and one to count the number of reads in each sample.\nFind the sample names with sample_names():\n\n\nCode (R)\n\nsample_names(bac_biom_metagenome)\n\n\n\nOutput\n\n\"ERR4998593\" \"ERR4998600\"  \n\nCount the number of reads with sample_sums():\n\n\nCode (R)\n\nsample_sums(bac_biom_metagenome)\n\n\n\nOutput\n\nERR4998593 ERR4998600 \n    442490     305135\n\nThe summary() function can give us an indication of species evenness\n\n\nCode (R)\n\nsummary(bac_biom_metagenome@otu_table)\n\n\n\nOutput\n\n ERR4998593         ERR4998600     \nMin.   :    0.00   Min.   :    0.0  \n1st Qu.:    1.00   1st Qu.:    2.0  \nMedian :    6.00   Median :   11.0  \nMean   :   61.19   Mean   :   42.2  \n3rd Qu.:   31.50   3rd Qu.:   38.0  \nMax.   :60679.00   Max.   :38768.0   \n\nThe median in sample ERR4998593 is 6, meaning many of OTU occur six times. The maximum is very high so at least one OTU is very abundant.\nThe plot_richness() command will give us a visual representation of the diversity inside the samples (i.e. α diversity):\n\n\nCode (R)\n\nplot_richness(physeq = biom_metagenome,\n              measures = c(\"Observed\",\"Chao1\",\"Shannon\"))\n\n\n  \nEach of these metrics can give insight of the distribution of the OTUs inside our samples. For example Chao1 diversity index gives more weight to singletons and doubletons observed in our samples, while the Shannon is a measure of species richness and species evenness with more weigth on richness.\nUse the following to open the manual page for plot_richness\n\n\nCode (R)\n\n?plot_richness\n\n\n\n\n\n\n\nExercise 3:\n\n\n\nWhile using the help provided explore these options available for the function in plot_richness():\n\nnrow\nsortby\ntitle\n\nUse these options to generate new figures that show you other ways to present the data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe code and the plot using the three options will look as follows: The “title” option adds a title to the figure.\n\n\nCode (R)\n\nplot_richness(physeq = biom_metagenome,\n              title = \"Alpha diversity indexes for both metagenomic samples\",\n              measures = c(\"Observed\",\"Chao1\",\"Shannon\"))\n\n   \nThe “nrow” option arranges the graphics horizontally.\n\n\nCode (R)\n\nplot_richness(physeq = biom_metagenome,\n              title = \"Alpha diversity indexes for both metagenomic samples\",\n              measures = c(\"Observed\",\"Chao1\",\"Shannon\"),\n              nrow=3)\n\n   \nThe “sortby” option orders the samples from least to greatest diversity depending on the parameter. In this case, it is ordered by “Shannon” and tells us that the ERR4998593 sample has the lowest diversity and the ERR4998600 sample the highest.\n\n\nCode (R)\n\nplot_richness(physeq = bac_biom_metagenome,\n              title = \"Alpha diversity indexes for both metagenomic samples\",\n              measures = c(\"Observed\",\"Chao1\",\"Shannon\"),\n              sortby = \"Shannon\")",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Diversity Tackled With R"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html#beta-diversity",
    "href": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html#beta-diversity",
    "title": "Diversity Tackled With R",
    "section": "Beta diversity",
    "text": "Beta diversity\nThe β diversity between ERR4998593 and ERR4998600 can be calculated with the distance() function. For example, we can find the Bray–Curtis dissimilarity with:\n\n\nCode (R)\n\ndistance(bac_biom_metagenome, method=\"bray\")\n\n\n\nOutput\n\n           ERR4998593\nERR4998600  0.5438328\n\nThere are other methods of determining distance and we can view our options with:\n\n\nCode (R)\n\ndistanceMethodList$vegdist\n\n\n\nOutput\n\n[1] \"manhattan\"  \"euclidean\"  \"canberra\"   \"bray\"       \"kulczynski\" \"jaccard\"    \"gower\"     \n[8] \"altGower\"   \"morisita\"   \"horn\"       \"mountford\"  \"raup\"       \"binomial\"   \"chao\"      \n[15] \"cao\"\n\nAny of these methods can be used in the same way used above, e.g.\n\n\nCode (R)\n\ndistance(bac_biom_metagenome, method=\"jaccard\")\n\n\n\nOutput\n\n           ERR4998593\nERR4998600  0.7045229\n\nThe output of this function is a distance matrix. When we have just two samples there is only one distance to calculate. If we had many samples, the output would have the pairwise distances between all of them",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Diversity Tackled With R"
    ]
  },
  {
    "objectID": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html#reading",
    "href": "docs/lesson06-taxonomic-annotations/02-Diversity-tackled-with-R.html#reading",
    "title": "Diversity Tackled With R",
    "section": "Reading",
    "text": "Reading\nTuomisto, H. A consistent terminology for quantifying species diversity? Yes, it does exist. Oecologia 164, 853–860 (2010). https://doi.org/10.1007/s00442-010-1812-0\nWhittaker, R. H. (1960) Vegetation of the Siskiyou Mountains, Oregon and California. Ecological Monographs, 30, 279–338",
    "crumbs": [
      "Home",
      "Taxonomic Annotations",
      "Diversity Tackled With R"
    ]
  },
  {
    "objectID": "docs/miscellanea/precourse-instructions.html",
    "href": "docs/miscellanea/precourse-instructions.html",
    "title": "Metagenomics: Precourse Instructions",
    "section": "",
    "text": "The software and data used for analysis during the course are hosted on an Amazon Web Services (AWS) virtual machine (VM) instance. A copy of such an instance that requires no previous setup by you will be made available to you at no cost by the Cloud-SPAN team.\nTo access and use the resources in your AWS instance from your personal computer, you will use a command-line interface (CLI) program that is widely known as the shell or terminal. The shell is available by default for Linux and Mac users (so they don’t need to install any software).\nWindows users will need to install Git for Windows on their computer as described below prior to the course. Git includes Git Bash which is a Windows version of the Unix Bash shell, the most widely used shell and the default shell in Linux systems.\nYou will need to use a laptop or desktop to take this course. Due to the need to both follow the instructor in Zoom and perform analyses, tablets and iPads are not suitable for using during this course. Having both an up to date browser and a stable internet connection is important.\nBefore the course you will receive via email the information that you will need to log in to your AWS instance. Your instructor will show you how to log in."
  },
  {
    "objectID": "docs/miscellanea/precourse-instructions.html#overview",
    "href": "docs/miscellanea/precourse-instructions.html#overview",
    "title": "Metagenomics: Precourse Instructions",
    "section": "",
    "text": "The software and data used for analysis during the course are hosted on an Amazon Web Services (AWS) virtual machine (VM) instance. A copy of such an instance that requires no previous setup by you will be made available to you at no cost by the Cloud-SPAN team.\nTo access and use the resources in your AWS instance from your personal computer, you will use a command-line interface (CLI) program that is widely known as the shell or terminal. The shell is available by default for Linux and Mac users (so they don’t need to install any software).\nWindows users will need to install Git for Windows on their computer as described below prior to the course. Git includes Git Bash which is a Windows version of the Unix Bash shell, the most widely used shell and the default shell in Linux systems.\nYou will need to use a laptop or desktop to take this course. Due to the need to both follow the instructor in Zoom and perform analyses, tablets and iPads are not suitable for using during this course. Having both an up to date browser and a stable internet connection is important.\nBefore the course you will receive via email the information that you will need to log in to your AWS instance. Your instructor will show you how to log in."
  },
  {
    "objectID": "docs/miscellanea/precourse-instructions.html#install-git-bash-windows-users",
    "href": "docs/miscellanea/precourse-instructions.html#install-git-bash-windows-users",
    "title": "Metagenomics: Precourse Instructions",
    "section": "Install Git Bash — Windows users",
    "text": "Install Git Bash — Windows users\n\n\n\n\n\n\nInstall Git Bash (Git for Windows)\n\n\n\n\n\nThe steps below correspond to the installation of Git for Windows version 2.33.1 from scratch. The installation of a more recent version, or updating a previously installed version, may show different wording in the screen messages mentioned below or may vary slightly in the number of steps to follow. Choose as many of the options below as possible.\n\nClick on this link: Git for Windows download page\nOnce in that page, click on Download to download the installer.\nOnce the installer is downloaded,\n\ndouble click on it\nyou will then be asked some questions and to select an option for each question.\neach question is shown below in Italics, and the selection to be made is shown in bold\n\nThe app you’re trying to install isn’t a Microsoft-verified app ..?\n\nClick on Install anyway\n\nDo you want to allow this app to make changes to your device?\n\nClick on Yes\n\nGNU General Public License\n\nclick on Next\n\nSelect Destination Location\n\nclick on Next (don’t change the location shown).\n\nSelect Components\n\nclick on Additional Icons (it will also select “On the Desktop” option)\nthen click on Next\n\nSelect Start Menu Folder\n\nclick on Next (don’t change the folder name shown)\n\nChoosing the default editor used by Git\n\nselect Use the nano editor by default and click on Next.\nNB: you may need to click on the dropdown menu and to scroll up with the mouse to see this option – see the figure:\n\n\n\n\n.\n\n\n\nAdjusting the name of the initial branch in new repositories\n\nkeep the selected (or select the) option Let Git decide and click on Next.\n\nAdjusting your PATH environment\n\nkeep the selected, Recommended option Git from the command line and also from 3rd-party software\nor selec it, and click on Next.\nNB: if this option is not selected, some programs that you need for the course will not work properly. If this happens rerun the installer and select the appropriate option.\n\nChoosing the SSH executable\n\nkeep the selected (or select the) option Use bundled OpenSSH and click on Next.\n\nChoosing HTTPS transport backend\n\nkeep the selected (or select the) option Use the OpenSSL library and click on Next.\n\nConfiguring the line ending conversions\n\nkeep the selected (or select the) option Checkout Windows-style, commit Unix-style line endings and click on Next.\n\nConfiguring the terminal emulator to use with Git Bash\n\nkeep the selected (or select the) option Use MinTTy (the default terminal of MSYS2) and click on Next.\n\nChoose the default behaviour of git pull\n\nkeep the selected (or select the) option Default (fast-forward or merge) and click on Next.\n\nChoose a credential helper\n\nkeep the selected (or select the) option Git Credential Manager Core and click on Next.\n\nConfiguring extra options\n\nkeep the selected option (Enable File System Caching) and click on Next.\n\nConfiguring experimental options\n\nclick on Install without selecting any option\n\nClick on Finish\n\nRun Git Bash by double clicking on the Git Bash icon in your Desktop screen.\n\n\n\n.\n\n\nExit Git Bash by pressing the keys Ctrl and d (Ctrl-d) simultaneously."
  },
  {
    "objectID": "docs/miscellanea/extras/glossary.html",
    "href": "docs/miscellanea/extras/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Glossary\n\nabsolute path\nA path that refers to a particular location in a file system. Absolute paths are usually written with respect to the file system’s root directory, and begin with either “/” (on Unix) or “\\” (on Microsoft Windows). See also: relative path.\n\n\naccession\na unique identifier assigned to each sequence or set of sequences\n\n\nargument\nA value given to a function or program when it runs. The term is often used interchangeably (and inconsistently) with parameter.\n\n\ncategorical variable\nVariables can be classified as categorical (aka, qualitative) or quantitative (aka, numerical). Categorical variables take on a fixed number of values that are names or labels.\n\n\ncleaned data\ndata that has been manipulated post-collection to remove errors or inaccuracies, introduce desired formatting changes, or otherwise prepare the data for analysis\n\n\ncommand shell\nSee shell\n\n\ncommand-line interface\nA user interface based on typing commands, usually at a REPL. See also: graphical user interface.\n\n\nconditional formatting\nformatting that is applied to a specific cell or range of cells depending on a set of criteria\n\n\nCSV (comma separated values) format\na plain text file format in which values are separated by commas\n\n\ncurrent working directory\nThe directory that relative paths are calculated from; equivalently, the place where files referenced by name only are searched for. Every process has a current working directory. The current working directory is usually referred to using the shorthand notation . (pronounced “dot”).\n\n\nfactor\na variable that takes on a limited number of possible values (i.e. categorical data)\n\n\nfile system\nA set of files, directories, and I/O devices (such as keyboards and screens). A file system may be spread across many physical devices, or many file systems may be stored on a single physical device; the operating system manages access.\n\n\nfilename extension\nThe portion of a file’s name that comes after the final “.” character. By convention this identifies the file’s type: .txt means “text file”, .png means “Portable Network Graphics file”, and so on. These conventions are not enforced by most operating systems: it is perfectly possible (but confusing!) to name an MP3 sound file homepage.html. Since many applications use filename extensions to identify the MIME type of the file, misnaming files may cause those applications to fail.\n\n\nflag\nA terse way to specify an option or setting to a command-line program. By convention Unix applications use a dash followed by a single letter, such as -v, or two dashes followed by a word, such as --verbose, while DOS applications use a slash, such as /V. Depending on the application, a flag may be followed by a single argument, as in -o /tmp/output.txt.\n\n\nGB\ngigabyte of file storage or file size\n\n\nGbase\na gigabase represents one billion nucleic acid bases (Gbp may indicate one billion base pairs of nucleic acid)\n\n\ngraphical user interface\nA user interface based on selecting items and actions from a graphical display, usually controlled by using a mouse. See also: command-line interface.\n\n\nheaders\nnames at tops of columns that are descriptive about the column contents (sometimes optional)\n\n\nhome directory\nThe default directory associated with an account on a computer system. By convention, all of a user’s files are stored in or below their home directory.\n\n\nmetadata\ndata which describes other data\n\n\nNGS\ncommon acronym for “Next Generation Sequencing” currently being replaced by “High Throughput Sequencing”\n\n\nnull value\na value used to record observations missing from a dataset\n\n\nobservation\na single measurement or record of the object being recorded (e.g. the weight of a particular mouse)\n\n\noperating system\nSoftware that manages interactions between users, hardware, and software processes. Common examples are Linux, OS X, and Windows.\n\n\nparent directory\nThe directory that “contains” the one in question. Every directory in a file system except the root directory has a parent. A directory’s parent is usually referred to using the shorthand notation .. (pronounced “dot dot”).\n\n\n\n\n\npath\nA description that specifies the location of a file or directory within a file system. See also: absolute path, relative path.\n\n\npipe\nA connection from the output of one program to the input of another. When two or more programs are connected in this way, they are called a “pipeline”.\n\n\nplain text\nunformatted text\n\n\nprocess\nA running instance of a program, containing code, variable values, open files and network connections, and so on. Processes are the “actors” that the operating system manages; it typically runs each process for a few milliseconds at a time to give the impression that they are executing simultaneously.\n\n\nprompt\nA character or characters displayed by a REPL to show that it is waiting for its next command.\n\n\nquality assurance\nany process which checks data for validity during entry\n\n\nquality control\nany process which removes problematic data from a dataset\n\n\nraw data\ndata that has not been manipulated and represents actual recorded values\n\n\nread-evaluate-print loop\n(REPL): A command-line interface that reads a command from the user, executes it, prints the result, and waits for another command.\n\n\nredirect\nTo send a command’s output to a file, or another command, rather than to the screen, or equivalently to read a command’s input from a file.\n\n\nregular expression\nA pattern that specifies a set of character strings. REs are most often used to find sequences of characters in strings.\n\n\nrelative path\nA path that specifies the location of a file or directory with respect to the current working directory. Any path that does not begin with a separator character (“/” or “\\”) is a relative path. See also: absolute path.\n\n\nrich text\nformatted text (e.g. text that appears bolded, colored or italicized)\n\n\nroot directory\nThe top-most directory in a file system. Its name is “/” on Unix (including Linux and Mac OS X) and “\\” on Microsoft Windows.\n\n\nshell\nA command-line interface such as Bash (the Bourne-Again Shell) or the Microsoft Windows DOS shell that allows a user to interact with the operating system.\n\n\nshell script\nA set of shell commands stored in a file for re-use. A shell script is a program executed by the shell; the name “script” is used for historical reasons.\n\n\nstandard input\nA process’s default input stream. In interactive command-line applications, it is typically connected to the keyboard; in a pipe, it receives data from the standard output of the preceding process.\n\n\nstandard output\nA process’s default output stream. In interactive command-line applications, data sent to standard output is displayed on the screen; in a pipe, it is passed to the standard input of the next process.\n\n\nstring\na collection of characters (e.g. “thisisastring”)\n\n\nsub-directory\nA directory contained within another directory.\n\n\ntab completion\nA feature provided by many interactive systems in which pressing the Tab key triggers automatic completion of the current word or command.\n\n\nTSV (tab separated values) format\na plain text file format in which values are separated by tabs\n\n\nvariable\nA name in a program that is associated with a value or a collection of values. Also: a category of data being collected on the object being recorded (e.g. a mouse’s weight)\n\n\nwildcard\nA character used in pattern matching. In the Unix shell, the wildcard * matches zero or more characters, so that *.txt matches all files whose names end in .txt.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Extras",
      "Glossary"
    ]
  },
  {
    "objectID": "docs/miscellanea/about.html",
    "href": "docs/miscellanea/about.html",
    "title": "About",
    "section": "",
    "text": "Cloud-SPAN is a collaboration between the University of York and The Software Sustainability Institute funded by the UKRI Innovation Scholars award. Project Reference: MR/V038680/1.\nCloud-SPAN trains researchers, and the research software engineers that support them, to run specialised analyses on cloud-based high-performance computing infrastructure. We are developing highly accessible resources which integrate with existing Carpentries courses.\nThis set of lessons is based on the Data Carpentries Genomics Workshop."
  },
  {
    "objectID": "docs/miscellanea/about.html#our-handbook",
    "href": "docs/miscellanea/about.html#our-handbook",
    "title": "About",
    "section": "Our Handbook",
    "text": "Our Handbook\nThe Cloud-SPAN team are dedicated to providing a welcoming and supportive environment for all people, regardless of background or identity. We hope to develop a community of practice around our materials. We have a Handbook that gives:\n⭐ An introduction to the Cloud-SPAN project\n🤝 Our Code of Conduct\n🎓 More information on our Courses\n👪 An open invitation to the Cloud-SPAN Community\n📌 Information about the FAIR Principles"
  },
  {
    "objectID": "docs/miscellanea/about.html#licences",
    "href": "docs/miscellanea/about.html#licences",
    "title": "About",
    "section": "Licences",
    "text": "Licences\nThis instructional material is made available for reuse and remixing under the Creative Commons Attribution license.\nThe Cloud-SPAN Genomics course consists of materials derived from Data Carpentry’s Genomics Workshop. This material is not endorsed by Data Carpentry or the Carpentries in general.\nCloud-SPAN is a collaboration between the University of York and The Software Sustainability Institute funded by the UKRI Innovation Scholars award. Project Reference: MR/V038680/1."
  },
  {
    "objectID": "docs/lesson01-files-and-directories/index.html",
    "href": "docs/lesson01-files-and-directories/index.html",
    "title": "Files and Directories",
    "section": "",
    "text": ".\n\n\nWelcome to the first lesson in Cloud-SPAN’s Metagenomics with High Performance Computing course!\nOver the next two lessons we will cover the foundational skills and knowledge needed for the rest of the course. Once you are comfortable with these skills, we can move on to applying them to a metagenomics analysis workflow.\nIn this lesson we will learn how the files and directories on your computer are structured, as well as logging onto the cloud and using the command line (also known as the shell and the terminal) for the first time. It is heavily based on the first lesson of our Prenomics course.\n\nBy the end of this lesson you will be able to:\n\nExplain the hierarchical structure of a file system.\nUnderstand what working directories, paths and relative paths are.\nLog onto to a running instance.\nNavigate a file system using the command line.\nAccess and read help files for bash programs and use help files to identify useful command options.\nWork with hidden directories and hidden files.\n\n\n\n\n\n\n\nGetting Started\n\n\n\nThis lesson assumes no prior experience with the tools covered in the module. It is designed for absolute beginners.\nThis lesson uses an Amazon Web Services (AWS) instance, which is a Linux virtual machine. Your AWS instance will be created for you and you will be sent the log in information you will need for this lesson.\nBefore starting you should read the Precourse Instructions page.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Files and Directories"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/01-understanding-file-systems.html",
    "href": "docs/lesson01-files-and-directories/01-understanding-file-systems.html",
    "title": "Understanding your file system",
    "section": "",
    "text": "A file system is exactly what it sounds like - a way for your computer to store your data in an organised way using files.\nYou will definitely have come across files before. All the data stored on your computer is split into separate files, making it much easier to keep track of than if it was all in one big blob.\nThere are lots of different file types; we can often find out something about what kind of data a file contains by looking at its filename extension. For example:\n\n.txt tells us that the file contains text\n.exe tells us that the file contains a program to be run\n.html tells us that the file contains a webpage, and should be run inside a web browser\n\nWe will come across various different file types during this course, some of which you may not have seen before. Do not worry. We will introduce them to you and explain how to use them when necessary.\nIn a file system, files are organised into directories, which can also be called folders. Hopefully you will have used folders to organise your files before! Folders can contain sub-folders, which can contain their own sub-folders, and so on almost without limit.\nIt is easiest to picture a file system, or part of it, as a tree that starts at a directory and branches out from there. This is called a hierarchical structure. The figure below shows an example of a hierarchical file structure that starts at the “home directory” of the user named user1:\n\n\n\nA file hierarchy containing 4 levels of folders and files.\n\n\nThe directory you are working inside is called your working directory. For example, if you were editing doc2.txt in the diagram above, your working directory would be the folder called docs.\n\n\n\n\n\n\nExercise\n\n\n\nThink about your own computer and how your files and directories are organised. Sketch a tree diagram like the one above for your file system.\nHint: remind yourself of your file system’s layout using a file manager application such as:\n\n\n\n\n\n\n\nOS\nIcon\n\n\n\n\nFile Explorer (Windows)\n\n\n\nFinder (Mac)\n\n\n\n\nYou probably already use these applications regularly to find, open and organise files.",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Understanding your file system"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/01-understanding-file-systems.html#what-is-a-file-system",
    "href": "docs/lesson01-files-and-directories/01-understanding-file-systems.html#what-is-a-file-system",
    "title": "Understanding your file system",
    "section": "",
    "text": "A file system is exactly what it sounds like - a way for your computer to store your data in an organised way using files.\nYou will definitely have come across files before. All the data stored on your computer is split into separate files, making it much easier to keep track of than if it was all in one big blob.\nThere are lots of different file types; we can often find out something about what kind of data a file contains by looking at its filename extension. For example:\n\n.txt tells us that the file contains text\n.exe tells us that the file contains a program to be run\n.html tells us that the file contains a webpage, and should be run inside a web browser\n\nWe will come across various different file types during this course, some of which you may not have seen before. Do not worry. We will introduce them to you and explain how to use them when necessary.\nIn a file system, files are organised into directories, which can also be called folders. Hopefully you will have used folders to organise your files before! Folders can contain sub-folders, which can contain their own sub-folders, and so on almost without limit.\nIt is easiest to picture a file system, or part of it, as a tree that starts at a directory and branches out from there. This is called a hierarchical structure. The figure below shows an example of a hierarchical file structure that starts at the “home directory” of the user named user1:\n\n\n\nA file hierarchy containing 4 levels of folders and files.\n\n\nThe directory you are working inside is called your working directory. For example, if you were editing doc2.txt in the diagram above, your working directory would be the folder called docs.\n\n\n\n\n\n\nExercise\n\n\n\nThink about your own computer and how your files and directories are organised. Sketch a tree diagram like the one above for your file system.\nHint: remind yourself of your file system’s layout using a file manager application such as:\n\n\n\n\n\n\n\nOS\nIcon\n\n\n\n\nFile Explorer (Windows)\n\n\n\nFinder (Mac)\n\n\n\n\nYou probably already use these applications regularly to find, open and organise files.",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Understanding your file system"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/01-understanding-file-systems.html#file-paths",
    "href": "docs/lesson01-files-and-directories/01-understanding-file-systems.html#file-paths",
    "title": "Understanding your file system",
    "section": "File paths",
    "text": "File paths\nIt is not practical to draw out a tree diagram every time we want to refer to a file’s location. Instead, we can represent the information as a file path.\nIn a file path, each directory is represented as a separate component separated by a character such as \\ or /. It is like writing an address or set of instructions for someone to follow if they want to find a specific file.\nFor example, the path for the file called doc3.txt in the file system above looks like this: user1/docs/data/doc3.txt in a Unix or Linux computer.\nIt is useful to note that Windows uses backslashes (\\) to separate path components, while Unix, Linux and Mac use forward slashes (/).",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Understanding your file system"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/01-understanding-file-systems.html#root-and-home-directories",
    "href": "docs/lesson01-files-and-directories/01-understanding-file-systems.html#root-and-home-directories",
    "title": "Understanding your file system",
    "section": "Root and home directories",
    "text": "Root and home directories\nThe root is the top-level of directories, which contains all other directories further down the tree.\nThe root is represented as a / in Unix, Linux, and Mac operating systems.\nIn the Windows operating system, the root directory is also known as a drive. In most cases, this will be the C:\\ drive.\nEven though the root directory is at the base of the file tree (or the top, depending on how you view it), it is not necessarily where our journey through the file system starts when we launch a new session on our computer. Instead our journey begins in the so called “home directory”.\nIn Windows, Mac, Unix, and Linux, the “home directory” is a folder named with your username. Your personal files and directories can be found inside this folder. This is where your computer assumes you want to start when you open your file manager.\nOn Windows and Mac your home directory is a directory inside directory called Users and named with your username. On Unix/Linux systems it is called home.\nFrom the root, the file system is:\n\n\n\nA file hierarchy containing with root and home directories labelled.\n\n\n\nYour home directory is:\n\nC:\\Users\\user1\\ on Windows\n/Users/user1/ on Mac\n/home/user1/ on Linux\n\nIn Linux (the operating system we will use later in the course), a tilde symbol (~) is used as a shortcut for your home directory. So, for example, the path ~/docs/doc2.txt is equivalent to /home/user1/docs/doc2.txt.",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Understanding your file system"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/01-understanding-file-systems.html#absolute-vs-relative-paths",
    "href": "docs/lesson01-files-and-directories/01-understanding-file-systems.html#absolute-vs-relative-paths",
    "title": "Understanding your file system",
    "section": "Absolute vs relative paths",
    "text": "Absolute vs relative paths\nThere are two ways of writing a file path - absolute paths and relative paths.\nAn absolute path contains the complete list of directories needed to locate a file on your computer. This allows you to reach the file no matter where you are.\nA relative path describes the location of a file relative to your current working directory. For example, if you were already in the folder called docs, the relative path for doc3.txt would be data/doc3.txt. There is no need to give instructions to navigate a route you have already taken.\nIf, however, you were in the folder called docs and you wanted to open one of the .exe files, you would need to give the path to .exe relative to the docs folder or give the absolute path.\n\n\n\n\n\n\nChallenge\n\n\n\nUse the file system above to answer these questions.\n\nWhat is the absolute path for the document doc4.txt on a Linux computer?\nAssuming you are currently in the directory called docs, what is the relative path for the document doc2.txt?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe absolute path is /home/user1/docs/data/doc4.txt or ~/docs/data/doc4.txt.\nThe relative path is doc2.txt (as you are already in the directory where doc2.txt is stored).",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Understanding your file system"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/01-understanding-file-systems.html#create-a-folder-for-the-course",
    "href": "docs/lesson01-files-and-directories/01-understanding-file-systems.html#create-a-folder-for-the-course",
    "title": "Understanding your file system",
    "section": "Create a folder for the course",
    "text": "Create a folder for the course\nTo keep things tidy and easily accessible, we will create a folder (directory) to keep everything related to this course: the key you will need to log in, your notes, data etc.\nIn theory you can make this folder anywhere in your file system but we recommend making it inside your Desktop folder, to make it easy to access.\n\nCreate the folder cloudspan in your Desktop.\nMinimise all windows until you can see your desktop background. Right click and select New, then Folder. Name the folder cloudspan.\nYou should see a folder icon appear on your desktop with the label cloudspan.\nAdditionally, if you enter your file explorer application you should be able to click on the Desktop directory at the side and see the cloudspan folder.\nWrite down the absolute path to your cloudspan folder.\nFind out what the absolute path is using your file manager application. Right click on the folder, or in any blank space inside the folder, and then:\n\nWindows users: select Properties, then look at the field called Location. Your path will be separated with backslashes (\\) but you should write it down using forward slashes (/).\nMac Users: select Get info, then look at the field called Where. Your path should start with the word Users/ - ignore anything that comes before this. Separate your path components using forward slashes (/).\n\nNow add the folder name (/cloudspan) to the end of this path. This is your absolute path. Once you have this written down, do not lose it! Now you can find your way back to the cloudspan folder whenever you need to, no matter where you are in your file structure.",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Understanding your file system"
    ]
  },
  {
    "objectID": "docs/lesson01-files-and-directories/01-understanding-file-systems.html#download-your-login-key-file",
    "href": "docs/lesson01-files-and-directories/01-understanding-file-systems.html#download-your-login-key-file",
    "title": "Understanding your file system",
    "section": "Download your login key file",
    "text": "Download your login key file\nNext we will download your unique login key file from the email you received from the Cloud-SPAN team. This type of file is called a .pem file. It contains a certificate which allows you to communicate with the Cloud securely. Without the .pem file you cannot access the Cloud.\nFor now we will use the file explorer to move the .pem file around.\n\nFind out where downloads are saved on your computer.\nHow you do this will depend on which browser you use. You can find instructions for changing your default download location in Chrome, Edge or Safari.\nIf you already know which folder your downloads go to, then you can skip this step.\nDownload your login key file to the folder you created a few minutes ago.\nClick on the link embedded in the email you received from the Cloud-SPAN team.\nMac users may need to Click on ‘download’ when the file says it can’t be opened.\nIf your browser asks you “where do you want to download the file?”, choose the cloudspan directory.\nOtherwise, once downloading is finished, copy and paste/drag and drop your login key file from wherever it was downloaded to your cloudspan folder.\n\nNow you are ready to log into the cloud!",
    "crumbs": [
      "Home",
      "Files and Directories",
      "Understanding your file system"
    ]
  },
  {
    "objectID": "docs/lesson04-polishing/02-QC-polished-assembly.html",
    "href": "docs/lesson04-polishing/02-QC-polished-assembly.html",
    "title": "QC polished assembly",
    "section": "",
    "text": "We now have a polished assembly (pilon.fasta). The next thing to do is to perform quality control (QC) checks to make sure we can be confident in our results and conclusions about the taxonomic, metabolic or functional composition of the communities that we are studying. This also allows us to check whether our efforts to improve the assembly with polishing were successful.\nThe quality of metagenome assemblies is typically lower than it is for single genome assemblies due to the use of long reads.\nIn this episode we explain what is meant by assembly quality and how to examine and compare the quality of your metagenome assemblies using seqkit stats and MetaQUAST.",
    "crumbs": [
      "Home",
      "Polishing",
      "QC polished assembly"
    ]
  },
  {
    "objectID": "docs/lesson04-polishing/02-QC-polished-assembly.html#why-qc-a-metagenome-assembly",
    "href": "docs/lesson04-polishing/02-QC-polished-assembly.html#why-qc-a-metagenome-assembly",
    "title": "QC polished assembly",
    "section": "",
    "text": "We now have a polished assembly (pilon.fasta). The next thing to do is to perform quality control (QC) checks to make sure we can be confident in our results and conclusions about the taxonomic, metabolic or functional composition of the communities that we are studying. This also allows us to check whether our efforts to improve the assembly with polishing were successful.\nThe quality of metagenome assemblies is typically lower than it is for single genome assemblies due to the use of long reads.\nIn this episode we explain what is meant by assembly quality and how to examine and compare the quality of your metagenome assemblies using seqkit stats and MetaQUAST.",
    "crumbs": [
      "Home",
      "Polishing",
      "QC polished assembly"
    ]
  },
  {
    "objectID": "docs/lesson04-polishing/02-QC-polished-assembly.html#what-do-we-mean-by-assembly-quality",
    "href": "docs/lesson04-polishing/02-QC-polished-assembly.html#what-do-we-mean-by-assembly-quality",
    "title": "QC polished assembly",
    "section": "What do we mean by assembly quality?",
    "text": "What do we mean by assembly quality?\nThere are several variables that affect assembly quality. Let’s go over them in more detail.\n\nContiguity\nA high quality assembly is highly contiguous meaning there are long stretches of the genome that have been successfully pieced together. The opposite of contiguous is fragmented.\nContiguity is strongly correlated with both the technology used and the quality of the original DNA used. “Short read”-only assemblies are often very fragmented as it is much more difficult to assemble the short reads into a contiguous assembly. With long reads it is easier to span bits of a genome that are tricky to reassemble, like repeats. However, some preparation methods, such as bead-beating, result in long-reads which are relatively short.\nThe extent to which contiguity matters depends on your research question. If you need to identify a large structural difference, high contiguity is important.\n\n\nDegree of duplication\nA duplication is when there is more than one copy of a particular region of the genome in the assembly. If a genome is much bigger than expected then there may have been duplication. Removing duplicated regions from assemblies is difficult but is made easier after the process of binning.\n\n\nDegree of completeness\nThe more complete an assembly, the higher quality it is. Sometimes there are regions of the assembly that are unexpectedly missing, meaning the metagenome is incomplete.\n\n\nChimeric Contigs\nChimeric contigs are when contigs belonging to different genomes get stuck together as one continuous piece. This is caused during assembly, and can be controlled by the parameters used to generate the assembly. While it is difficult to identify chimeras, it is worth considering the parameters we use for the polishing and assembly steps because inappropriate parameters can result in problems such as these.\n\n\nLow base quality\nLow base quality happens when mutations are present in reads that do not reflect actual biological variation. This happens more in long reads due to a higher error rate. However, this is outweighed by the fact that using long reads for metagenome assemblies results in higher overall quality due to higher contiguity. This is why we ‘polished’ our genome in the last episode by comparing the draft assembly to raw short reads.",
    "crumbs": [
      "Home",
      "Polishing",
      "QC polished assembly"
    ]
  },
  {
    "objectID": "docs/lesson04-polishing/02-QC-polished-assembly.html#using-seqkit-to-generate-summary-statistics-of-an-assembly",
    "href": "docs/lesson04-polishing/02-QC-polished-assembly.html#using-seqkit-to-generate-summary-statistics-of-an-assembly",
    "title": "QC polished assembly",
    "section": "Using seqkit to generate summary statistics of an assembly",
    "text": "Using seqkit to generate summary statistics of an assembly\nAfter finishing the draft assembly we used seqkit stats to see some basic statistics about the assembly. We will be using it again to get summary statistics for all three of the assemblies (unpolished, long read polished and short read polished). We can then use those statistics to examine the polishing process.\nLet’s review the help documentation for seqkit stats.\n\n\nCode\n\nseqkit stats --help\n\n\n\n\n\n\n\nOutput — seqkit stats help documentation\n\n\n\n\n\nsimple statistics of FASTA/Q files\n\nTips:\n  1. For lots of small files (especially on SDD), use big value of '-j' to\n     parallelize counting.\n\nUsage:\n  seqkit stats [flags]\n\nAliases:\n  stats, stat\n\nFlags:\n  -a, --all                  all statistics, including quartiles of seq length, sum_gap, N50\n  -b, --basename             only output basename of files\n  -E, --fq-encoding string   fastq quality encoding. available values: 'sanger', 'solexa', 'illumina-1.3+', 'illumina-1.5+', 'illumina-1.8+'. (default \"sanger\")\n  -G, --gap-letters string   gap letters (default \"- .\")\n  -h, --help                 help for stats\n  -e, --skip-err             skip error, only show warning message\n  -i, --stdin-label string   label for replacing default \"-\" for stdin (default \"-\")\n  -T, --tabular              output in machine-friendly tabular format\n\nGlobal Flags:\n      --alphabet-guess-seq-length int   length of sequence prefix of the first FASTA record based on which seqkit guesses the sequence type (0 for whole seq) (default 10000)\n      --id-ncbi                         FASTA head is NCBI-style, e.g. &gt;gi|110645304|ref|NC_002516.2| Pseud...\n      --id-regexp string                regular expression for parsing ID (default \"^(\\\\S+)\\\\s?\")\n      --infile-list string              file of input files list (one file per line), if given, they are appended to files from cli arguments\n  -w, --line-width int                  line width when outputting FASTA format (0 for no wrap) (default 60)\n  -o, --out-file string                 out file (\"-\" for stdout, suffix .gz for gzipped out) (default \"-\")\n      --quiet                           be quiet and do not show extra information\n  -t, --seq-type string                 sequence type (dna|rna|protein|unlimit|auto) (for auto, it automatically detect by the first sequence) (default \"auto\")\n  -j, --threads int                     number of CPUs. can also set with environment variable SEQKIT_THREADS) (default 4)\n\n\n\n\nThe N50 length\nN50 is a metric indicating the distribution of contig lengths in an assembly. The value of N50 indicates that 50% of the total sequence is in contigs that are that size or larger. For a more thorough introduction to N50, read What’s N50? By The Molecular Ecologist.\nIt indicates the average size of the contigs the assembly software has produced. A higher N50 length means that more of the assembly is in longer fragments. That means the chunks of sequence produced by the assembler are, on average, larger.\nseqkit stats has an option to calculate the N50 length. Use the seqkit stats help documentation to answer the exercise below.\n\n\n\n\n\n\nExercise 1: Flag to get the N50 length\n\n\n\n\nUsing the help documentation, what flag can we add to get the N50 length for this assembly?\n\nWhat would the new command be if we added this flag?\n\nBonus exercise: What flag would enable us to save the output table in a tabular (i.e. tsv) format?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWe can see from the help documentation that the flag -a or --all will calculate all statistics, including quartiles of seq length, sum_gap, N50.\n\nThe new command would be seqkit stats -a results/assembly/assembly.fasta or seqkit stats --all results/assembly/assembly.fasta\n\nBonus: The flag -T allows us to save it in a tabular output - this makes the table easier to use in other command line programs or programming languages such as R and Python. The command could be either seqkit stats -a -T assembly/assembly.fasta or we can combine the two flags seqkit stats -aT assembly/assembly.fasta\n\n\n\n\n\nNext, run the command on the original draft assembly (~/cs_course/results/assembly/assembly.fasta) to calculate the N50 length and answer the questions below about the output.\n\n\n\n\n\n\nHint: Seqkit stats N50 command\n\n\n\n\n\n\n\nCode\n\ncd ~/cs_course/\nseqkit stats -a results/assembly/assembly.fasta\n\n\n\n\n\n\n\n\n\n\nExercise 2: Calculating the N50 length\n\n\n\n\nWhat is the output if we run the new command from the above exercise?\n\nWhat is the N50 length of this assembly?\n\nBonus exercise: Looking at the information available online for Seqkit stats, can you work out what the extra statistics other than N50 tell us?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\nOutput\n\nfile            format  type  num_seqs     sum_len  min_len   avg_len  max_len     Q1      Q2      Q3  sum_gap     N50  Q20(%)  Q30(%)  GC(%)\nassembly.fasta  FASTA   DNA      1,161  18,872,828      528  16,255.7  118,427  7,513  11,854  19,634        0  20,921       0       0  66.26\n\n(Your numbers will probably be slightly different to the solution given, as the assembly algorithm runs differently each time. As long as they are in the same ballpark there’s no need to worry!)\n\nThe N50 length for this assembly is 20,921 bp. This tells us that 50% of the assembly is in fragments that are nearly 21,000 bases long or longer!\n\nBonus: Q1, Q2 and Q3 are the quartile ranges of sequence length. sum_gap is the total number of ambiguous bases (N’s) in the sequence. Q20(%) is the percentage of bases with a PHRED score over 20 and similarly Q30(%) is the percentage of bases with a PHRED score over 30. GC(%) is the guanine-cytosine content of the sequence.\n\n\n\n\n\n\n\nGenerating statistics for all three assemblies\nInstead of passing just one FASTA file to seqkit stats we can use all three FASTA files at once.\nFirst we need to make sure we’re in the cs_course directory.\n\n\nCode\n\ncd ~/cs_course/\n\nThe three files we want to generate statistics for are:\n\nDraft assembly generated by Flye in results/assembly/assembly.fasta\nLong-read polished assembly generated by Medaka in results/medaka/consensus.fasta\nShort-read polished assembly generated by Pilon in results/pilon/pilon.fasta\n\nThis makes our command:\n\n\nCode\n\nseqkit stats -a results/assembly/assembly.fasta results/medaka/consensus.fasta results/pilon/pilon.fasta\n\n\n\nOutput\n\nfile                    format type num_seqs sum_len    min_len avg_len  max_len Q1      Q2     Q3       sum_gap N50    Q20(%) Q30(%) GC(%)\nassembly/assembly.fasta FASTA  DNA  791      11,979,728 662     15,145   95,860  7,148   11,019 18,874   0       19,835 0.00   0.00   66.58\nmedaka/consensus.fasta  FASTA  DNA  791      12,019,945 662     15,195.9 96,516  7,127.5 10,960 18,914.5 0       19,831 0.00   0.00   65.16\npilon/pilon.fasta       FASTA  DNA  791      11,950,064 662     15,107.5 95,826  7,108.5 10,902 18,830   0       19,748 0.00   0.00   66.13\n\n\n\n\n\n\n\nExercise 3: Comparing the Assemblies\n\n\n\nUsing the seqkit output for all three assemblies, compare the statistics for each of the three assemblies. What has changed across the two rounds of polishing? (From assembly&gt;medaka&gt;pilon)\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBetween the original assembly and the medaka polished assembly:\n\nTotal length, maximum length and average length have all increased. The N50 and GC content have decreased.\n\nBetween the medaka polished assembly and the pilon polished assembly:\n\nAll variables have either decreased in length or stayed the same, except the maximum length which is higher (as is GC content by a very small margin).\n\n\n\n\n\n\nWe can compare these basic assembly statistics in this way. However, these may not tell the full story as there will also have been changes to the overall sequence (e.g. correcting individual base errors).",
    "crumbs": [
      "Home",
      "Polishing",
      "QC polished assembly"
    ]
  },
  {
    "objectID": "docs/lesson04-polishing/02-QC-polished-assembly.html#using-metaquast-to-further-assess-assembly-quality",
    "href": "docs/lesson04-polishing/02-QC-polished-assembly.html#using-metaquast-to-further-assess-assembly-quality",
    "title": "QC polished assembly",
    "section": "Using MetaQUAST to further assess assembly quality",
    "text": "Using MetaQUAST to further assess assembly quality\nWe will use MetaQUAST to further evaluate our metagenomic assemblies. MetaQUAST is based on the QUAST genome quality tool but accounts for high species diversity and misassemblies.\nMetaQUAST assesses the quality of assemblies using alignments to close references. This means we need to determine which references are appropriate for our data. MetaQUAST can automatically select reference genomes to align the assembly to, but it does not always pick the most appropriate references. Instead we will provide it with a list of references. For now you can just use our list, but next lesson we will show you how we generated it so you know how to do it for your own data in future.\nSo far we’ve been able to do a lot of work with files that already exist, but now we need to write our own file using the text editor Nano.\n\n\n\n\n\n\nText editors\n\n\n\nText editors, like Nano, “notepad” on Windows or “TextEdit” on Mac are used to edit any plain text files. Plain text files are those that contain only characters, not images or formatting.\nWe are using Nano because it is one of the least complex Unix text editors. However, many programmers use Emacs or Vim (both of which require more time to learn), or a graphical editor such as Gedit.\nNo matter what editor you use, you will need to know where it searches for and saves files. If you start it from the shell, it will (probably) use your current working directory as its default location.",
    "crumbs": [
      "Home",
      "Polishing",
      "QC polished assembly"
    ]
  },
  {
    "objectID": "docs/lesson04-polishing/02-QC-polished-assembly.html#writing-files",
    "href": "docs/lesson04-polishing/02-QC-polished-assembly.html#writing-files",
    "title": "QC polished assembly",
    "section": "Writing files",
    "text": "Writing files\nFirst, we need to create a directory to put the reference genomes list in. We’ll put it in a new folder in the data directory, since it’s an input not an output.\n\n\nCode\n\ncd ~/cs_course/data\nmkdir ref_genomes\ncd ref_genomes\n\nTo open Nano type the command nano followed by the name of the text file we want to generate.\n\n\nCode\n\nnano reference_genomes.txt\n\nWhen you press enter your terminal should change. You will see a white bar at the top with GNU nano 4.8 and some suggested commands at the bottom of the page. There should also be a white box or cursor which indicates where your cursor is. It will look something like this:\n\n\n\nnano202405.png\n\n\nThe text at the bottom of the screen shows the keyboard shortcuts for performing various tasks in Nano. We will talk more about how to interpret this information soon.\nCopy and paste the following list of organism names into this file (don’t forget that Ctrl/Cmd + v won’t work in Linux (Unix). Try Shift+Insert instead, or right click and select paste from the drop-down menu - see the note below).\n\n\nCode\n\nBradyrhizobium erythrophlei\nBradyrhizobium lablabi\nBradyrhizobium canariense\nBradyrhizobium sp. 200\nBradyrhizobium sp. 170\nBradyrhizobium diazoefficiens\nBradyrhizobium sp. 186\nRhodopseudomonas palustris\nAfipia sp. GAS231\nBradyrhizobium arachidis\nBradyrhizobium icense\nBradyrhizobium sp. CCBAU 051011\nRhodoplanes sp. Z2-YC6860\nBradyrhizobium sp. S2-20-1\nBradyrhizobium sp. S2-11-2\nBradyrhizobium sp. CCBAU 51753\nBradyrhizobium genosp. L\nBradyrhizobium paxllaeri\nFrigoriglobus tundricola\nBradyrhizobium sp. A19\n\nOnce we’re happy with our text, we can press Ctrl-o (press the Ctrl or Control key and, while holding it down, press the o key) to write our data to disk. You will then be prompted with File Name to Write: reference_genomes.txt at the bottom of the screen. Pressing Enter will confirm and save your changes. Once the file is saved, we can use Ctrl-x to quit the nano editor and return to the shell.\n\n\n\n\n\n\nControl, Ctrl, or ^ Key\n\n\n\nThe Control key is also called the “Ctrl” key. There are various ways in which using the Control key may be described. For example, you may see an instruction to press the Ctrl key and, while holding it down, press the X key, described as any of:\n\nControl-X\nControl+X\nCtrl-X\nCtrl+X\n^X\nC-x\n\n\n\nIn nano, along the bottom of the screen you’ll see ^G Get Help ^O WriteOut. This means that you can use Ctrl-G to get help and Ctrl-O to save your file.\nIf you are using a Mac, you might be more familiar with the Command key, which is labelled with a ⌘ . But you will often use the the Ctrl key when working in a Terminal. :\n\n\n\n\n\n\nCopying and pasting in Git bash\n\n\n\nMost people will want to use Ctrl+C and Ctrl+V to copy and paste. However in GitBash these shortcuts have other functions. Ctrl+C interrupts the currently running command and Ctrl+V tells the terminal to treat every keystroke as a literal character, so will add shortcuts like Ctrl+C as characters. Instead you can copy and paste using the mouse:\n\nLeft click and drag to highlight text, then right click to copy. Move the cursor to where you want to paste and right click to paste.\n\n\n\nYou should then be able to see this file when you ls and view it using less.\n\n\nCode\n\nls\nless reference_genomes.txt\n\n\n\nOutput\n\nreference_genomes.txt\n\n\n\nOutput\n\nBradyrhizobium erythrophlei\nBradyrhizobium lablabi\nBradyrhizobium canariense\nBradyrhizobium sp. 200\nBradyrhizobium sp. 170\nBradyrhizobium diazoefficiens\nBradyrhizobium sp. 186\nRhodopseudomonas palustris\nAfipia sp. GAS231\nBradyrhizobium arachidis\nBradyrhizobium icense\nBradyrhizobium sp. CCBAU 051011\nRhodoplanes sp. Z2-YC6860\nBradyrhizobium sp. S2-20-1\nBradyrhizobium sp. S2-11-2\nBradyrhizobium sp. CCBAU 51753\nBradyrhizobium genosp. L\nBradyrhizobium paxllaeri\nFrigoriglobus tundricola\nBradyrhizobium sp. A19\n(END)\n\n\nRunning MetaQUAST\nOnce we have our list of reference genomes we can run MetaQUAST on the original assembly and the two polished assemblies.\nFirst we should make a directory in results where we’ll store the MetaQUAST output.\n\n\nCode\n\ncd ~/cs_course\nmkdir results/metaquast\n\nNext let’s look at the help documentation to work out which commands are right for us.\n\n\nCode\n\nmetaquast.py -h\n\n\n\n\n\n\n\nOutput — MetaQUAST help documentation\n\n\n\n\n\nMetaQUAST: Quality Assessment Tool for Metagenome Assemblies\nVersion: 5.2.0\n\nUsage: python metaquast.py [options] &lt;files_with_contigs&gt;\n\nOptions:\n-o  --output-dir  &lt;dirname&gt;       Directory to store all result files [default: quast_results/results_&lt;datetime&gt;]\n-r   &lt;filename,filename,...&gt;      Comma-separated list of reference genomes or directory with reference genomes\n--references-list &lt;filename&gt;      Text file with list of reference genome names for downloading from NCBI\n-g  --features [type:]&lt;filename&gt;  File with genomic feature coordinates in the references (GFF, BED, NCBI or TXT)\n                                   Optional 'type' can be specified for extracting only a specific feature type from GFF\n-m  --min-contig  &lt;int&gt;           Lower threshold for contig length [default: 500]\n-t  --threads     &lt;int&gt;           Maximum number of threads [default: 25% of CPUs]\n\nAdvanced options:\n-s  --split-scaffolds                 Split assemblies by continuous fragments of N's and add such \"contigs\" to the comparison\n-l  --labels \"label, label, ...\"      Names of assemblies to use in reports, comma-separated. If contain spaces, use quotes\n-L                                    Take assembly names from their parent directory names\n-e  --eukaryote                       Genome is eukaryotic (primarily affects gene prediction)\n    --fungus                          Genome is fungal (primarily affects gene prediction)\n    --large                           Use optimal parameters for evaluation of large genomes\n                                      In particular, imposes '-e -m 3000 -i 500 -x 7000' (can be overridden manually)\n-k  --k-mer-stats                     Compute k-mer-based quality metrics (recommended for large genomes)\n                                      This may significantly increase memory and time consumption on large genomes\n    --k-mer-size                      Size of k used in --k-mer-stats [default: 101]\n    --circos                          Draw Circos plot\n-f  --gene-finding                    Predict genes using MetaGeneMark\n    --glimmer                         Use GlimmerHMM for gene prediction (instead of the default finder, see above)\n    --gene-thresholds &lt;int,int,...&gt;   Comma-separated list of threshold lengths of genes to search with Gene Finding module\n                                      [default: 0,300,1500,3000]\n    --rna-finding                     Predict ribosomal RNA genes using Barrnap\n-b  --conserved-genes-finding         Count conserved orthologs using BUSCO (only on Linux)\n    --operons  &lt;filename&gt;             File with operon coordinates in the reference (GFF, BED, NCBI or TXT)\n    --max-ref-number &lt;int&gt;            Maximum number of references (per each assembly) to download after looking in SILVA database.\n                                      Set 0 for not looking in SILVA at all [default: 50]\n    --blast-db &lt;filename&gt;             Custom BLAST database (.nsq file). By default, MetaQUAST searches references in SILVA database\n    --use-input-ref-order             Use provided order of references in MetaQUAST summary plots (default order: by the best average value)\n    --contig-thresholds &lt;int,int,...&gt; Comma-separated list of contig length thresholds [default: 0,1000,5000,10000,25000,50000]\n    --x-for-Nx &lt;int&gt;                  Value of 'x' for Nx, Lx, etc metrics reported in addition to N50, L50, etc (0, 100) [default: 90]\n    --reuse-combined-alignments       Reuse the alignments from the combined_reference stage on runs_per_reference stages.\n-u  --use-all-alignments              Compute genome fraction, # genes, # operons in QUAST v1.* style.\n                                      By default, QUAST filters Minimap's alignments to keep only best ones\n-i  --min-alignment &lt;int&gt;             The minimum alignment length [default: 65]\n    --min-identity &lt;float&gt;            The minimum alignment identity (80.0, 100.0) [default: 90.0]\n-a  --ambiguity-usage &lt;none|one|all&gt;  Use none, one, or all alignments of a contig when all of them\n                                      are almost equally good (see --ambiguity-score) [default: one]\n    --ambiguity-score &lt;float&gt;         Score S for defining equally good alignments of a single contig. All alignments are sorted\n                                      by decreasing LEN * IDY% value. All alignments with LEN * IDY% &lt; S * best(LEN * IDY%) are\n                                      discarded. S should be between 0.8 and 1.0 [default: 0.99]\n    --unique-mapping                  Disable --ambiguity-usage=all for the combined reference run,\n                                      i.e. use user-specified or default ('one') value of --ambiguity-usage\n    --strict-NA                       Break contigs in any misassembly event when compute NAx and NGAx.\n                                      By default, QUAST breaks contigs only by extensive misassemblies (not local ones)\n-x  --extensive-mis-size  &lt;int&gt;       Lower threshold for extensive misassembly size. All relocations with inconsistency\n                                      less than extensive-mis-size are counted as local misassemblies [default: 1000]\n    --local-mis-size  &lt;int&gt;           Lower threshold on local misassembly size. Local misassemblies with inconsistency\n                                      less than local-mis-size are counted as (long) indels [default: 200]\n    --scaffold-gap-max-size  &lt;int&gt;    Max allowed scaffold gap length difference. All relocations with inconsistency\n                                      less than scaffold-gap-size are counted as scaffold gap misassemblies [default: 10000]\n    --unaligned-part-size  &lt;int&gt;      Lower threshold for detecting partially unaligned contigs. Such contig should have\n                                      at least one unaligned fragment &gt;= the threshold [default: 500]\n    --skip-unaligned-mis-contigs      Do not distinguish contigs with &gt;= 50% unaligned bases as a separate group\n                                      By default, QUAST does not count misassemblies in them\n    --fragmented                      Reference genome may be fragmented into small pieces (e.g. scaffolded reference)\n    --fragmented-max-indent  &lt;int&gt;    Mark translocation as fake if both alignments are located no further than N bases\n                                      from the ends of the reference fragments [default: 200]\n                                      Requires --fragmented option\n    --upper-bound-assembly            Simulate upper bound assembly based on the reference genome and reads\n    --upper-bound-min-con  &lt;int&gt;      Minimal number of 'connecting reads' needed for joining upper bound contigs into a scaffold\n                                      [default: 2 for mate-pairs and 1 for long reads]\n    --est-insert-size  &lt;int&gt;          Use provided insert size in upper bound assembly simulation [default: auto detect from reads or 255]\n    --report-all-metrics              Keep all quality metrics in the main report even if their values are '-' for all assemblies or\n                                      if they are not applicable (e.g., reference-based metrics in the no-reference mode)\n    --plots-format  &lt;str&gt;             Save plots in specified format [default: pdf].\n                                      Supported formats: emf, eps, pdf, png, ps, raw, rgba, svg, svgz\n    --memory-efficient                Run everything using one thread, separately per each assembly.\n                                      This may significantly reduce memory consumption on large genomes\n    --space-efficient                 Create only reports and plots files. Aux files including .stdout, .stderr, .coords will not be created.\n                                      This may significantly reduce space consumption on large genomes. Icarus viewers also will not be built\n-1  --pe1     &lt;filename&gt;              File with forward paired-end reads (in FASTQ format, may be gzipped)\n-2  --pe2     &lt;filename&gt;              File with reverse paired-end reads (in FASTQ format, may be gzipped)\n    --pe12    &lt;filename&gt;              File with interlaced forward and reverse paired-end reads. (in FASTQ format, may be gzipped)\n    --mp1     &lt;filename&gt;              File with forward mate-pair reads (in FASTQ format, may be gzipped)\n    --mp2     &lt;filename&gt;              File with reverse mate-pair reads (in FASTQ format, may be gzipped)\n    --mp12    &lt;filename&gt;              File with interlaced forward and reverse mate-pair reads (in FASTQ format, may be gzipped)\n    --single  &lt;filename&gt;              File with unpaired short reads (in FASTQ format, may be gzipped)\n    --pacbio     &lt;filename&gt;           File with PacBio reads (in FASTQ format, may be gzipped)\n    --nanopore   &lt;filename&gt;           File with Oxford Nanopore reads (in FASTQ format, may be gzipped)\n    --ref-sam &lt;filename&gt;              SAM alignment file obtained by aligning reads to reference genome file\n    --ref-bam &lt;filename&gt;              BAM alignment file obtained by aligning reads to reference genome file\n    --sam     &lt;filename,filename,...&gt; Comma-separated list of SAM alignment files obtained by aligning reads to assemblies\n                                      (use the same order as for files with contigs)\n    --bam     &lt;filename,filename,...&gt; Comma-separated list of BAM alignment files obtained by aligning reads to assemblies\n                                      (use the same order as for files with contigs)\n                                      Reads (or SAM/BAM file) are used for structural variation detection and\n                                      coverage histogram building in Icarus\n    --sv-bedpe  &lt;filename&gt;            File with structural variations (in BEDPE format)\n\nSpeedup options:\n    --no-check                        Do not check and correct input fasta files. Use at your own risk (see manual)\n    --no-plots                        Do not draw plots\n    --no-html                         Do not build html reports and Icarus viewers\n    --no-icarus                       Do not build Icarus viewers\n    --no-snps                         Do not report SNPs (may significantly reduce memory consumption on large genomes)\n    --no-gc                           Do not compute GC% and GC-distribution\n    --no-sv                           Do not run structural variation detection (make sense only if reads are specified)\n    --no-read-stats                   Do not align reads to assemblies\n                                      Reads will be aligned to reference and used for coverage analysis,\n                                      upper bound assembly simulation, and structural variation detection.\n                                      Use this option if you do not need read statistics for assemblies.\n    --fast                            A combination of all speedup options except --no-check\n\nOther:\n    --silent                          Do not print detailed information about each step to stdout (log file is not affected)\n    --test                            Run MetaQUAST on the data from the test_data folder, output to quast_test_output\n    --test-no-ref                     Run MetaQUAST without references on the data from the test_data folder, output to quast_test_output.\n                                      MetaQUAST will download SILVA 16S rRNA database (~170 Mb) for searching reference genomes\n                                      Internet connection is required\n-h  --help                            Print full usage message\n-v  --version                         Print version\n\nOnline QUAST manual is available at http://quast.sf.net/manual\n\n\n\nThe documentation gives the usage as python metaquast.py [options] &lt;files_with_contigs&gt;.\n\nWe can disregard the python portion of this\n\nThe main “option” we want to include is --references-list, which supplies a text file containing our list of reference genomes (this is the file we just made).\nThe &lt;files_with_contigs&gt; part just means that this is where we put the assemblies to be evaluated. We’ll list them all one after the other so we can compare their outputs easily. We’ll also use the --output or -o flag to specify where the output should be stored.\nOur command therefore looks like:\n\n\nCode\n\nmetaquast.py --references-list data/ref_genomes/reference_genomes.txt results/assembly/assembly.fasta results/medaka/consensus.fasta results/pilon/pilon.fasta --output results/metaquast &&gt; results/metaquast/metaquast.out &\n\nNote that once again we are running the command in the background using the & symbol and redirecting the output to a file (metaquast.out) using &gt;. This is because it takes around 8-10 minutes to run.\nIf you open up metaquast.out after running the command you should see something like this as MetaQUAST starts downloading the reference species we specified in our files:\n\n\nCode\n\ncd results/metaquast\nless metaquast.out\n\n\n\nOutput\n\nVersion: 5.2.0\n\nSystem information:\n  OS: Linux-5.4.0-131-generic-x86_64-with-glibc2.31 (linux_64)\n  Python version: 3.9.7\n  CPUs number: 8\n\nStarted: 2023-03-16 16:32:09\n\nLogging to /home/csuser/cs_course/results/metaquast/quast_results/results_YYYY_MM_DD_HH_MM_SS/metaquast.log\nNOTICE: Maximum number of threads is set to 2 (use --threads option to set it manually)\n\nContigs:\n  Pre-processing...\n  1  results/assembly/assembly.fasta ==&gt; assembly\n  2  results/medaka/consensus.fasta==&gt; consensus\n  3  results/pilon/pilon.fasta ==&gt; pilon\n\nList of references was provided, starting to download reference genomes from NCBI...\n\n2023-03-16 16:32:11\n\n2023-03-16 16:32:11\nTrying to download found references from NCBI. Totally 20 organisms to try.\n  Bradyrhizobium_erythrophlei     | successfully downloaded (total 1, 19 more to go)\n  Bradyrhizobium_lablabi          | successfully downloaded (total 2, 18 more to go)\n  Bradyrhizobium_canariense       | successfully downloaded (total 3, 17 more to go)\n  Bradyrhizobium_sp._200          | successfully downloaded (total 4, 16 more to go)\n  Bradyrhizobium_sp._170          | successfully downloaded (total 5, 15 more to go)\n  Bradyrhizobium_diazoefficiens   | successfully downloaded (total 6, 14 more to go)\n  Bradyrhizobium_sp._186          | successfully downloaded (total 7, 13 more to go)\n  Rhodopseudomonas_palustris      | successfully downloaded (total 8, 12 more to go)\n  Afipia_sp._GAS231               | successfully downloaded (total 9, 11 more to go)\n  Bradyrhizobium_arachidis        | successfully downloaded (total 10, 10 more to go)\n  Bradyrhizobium_icense           | successfully downloaded (total 11, 9 more to go)\n  Bradyrhizobium_sp._CCBAU_051011 | successfully downloaded (total 12, 8 more to go)\n  Rhodoplanes_sp._Z2-YC6860       | successfully downloaded (total 13, 7 more to go)\n  Bradyrhizobium_sp._S2-20-1      | successfully downloaded (total 14, 6 more to go)\n  Bradyrhizobium_sp._S2-11-2      | successfully downloaded (total 15, 5 more to go)\n  Bradyrhizobium_sp._CCBAU_51753  | successfully downloaded (total 16, 4 more to go)\n  Bradyrhizobium_genosp._L        | successfully downloaded (total 17, 3 more to go)\n  Bradyrhizobium_paxllaeri        | successfully downloaded (total 18, 2 more to go)\n  Frigoriglobus_tundricola        | successfully downloaded (total 19, 1 more to go)\n  Bradyrhizobium_sp._A19          | successfully downloaded (total 20, 0 more to go)\n\nOnce MetaQUAST has finished you should see an output like:\n\n\nOutput\n\n[2]+  Done   metaquast.py --references-list data/ref_genomes/reference_genomes.txt results/assembly/assembly.fasta results/medaka/consensus.fasta results/pilon/pilon.fasta --output results/metaquast &&gt; results/metaquast/metaquast.out & \n\nYou can also look in your metaquast.out and jump to the end using Shift+g.\n\n\nOutput\n\nMetaQUAST finished.\n  Log is saved to /home/csuser/cs_course/results/metaquast/metaquast.log\n\nFinished: 2023-03-16 13:12:43\nElapsed time: 0:10:51.624140\nTotal NOTICEs: 45; WARNINGs: 0; non-fatal ERRORs: 0\n\nThank you for using QUAST!\n\nWe can now navigate into the metaquast directory to see our output files.\n\n\nCode\n\ncd results/metaquast\nls\n\n\n\nOutut\n\ncombined_reference  icarus_viewers  metaquast.out  quast_downloaded_references  runs_per_reference\nicarus.html         metaquast.log   not_aligned    report.html                  summary\n\nWe can see that MetaQUAST has generated multiple different files.\nIf you want to explore all the files you can download this whole directory using scp, with -r flag to download all directories and what they contain. This will require ~500MB of space.\nHowever, most of this information is in the report.html file so we can download just that one instead. As this is a HTML file we will first need to download it to our local computer in order to open it.\n\n\nDownloading MetaQUAST report.html\n\n\nCode\n\nscp -i login-key-instanceNNN.pem csuser@instanceNNN.cloud-span.aws.york.ac.uk:~/cs_course/results/metaquast/report.html .\n\nMake sure you replace both the NNNs with your instance number. The results.html file relies on some of the other files generated by MetaQUAST so with only the one file you won’t have full functionality but we can still view the information we want.\nIf you haven’t managed to download the file you can view our example report here.\nYou should take a few minutes to explore the file before answering the following exercise.\n\n\n\n\nMetaQUAST statistics output.\n\n\n\n\nUnderstanding MetaQUAST output\nThe output is organised into several sections, with a column for each assembly. The worst scoring column is shown in red, the best in blue.\n\nGenome statistics\nThis section determines the quality of the assembly based on the size of the total assembly. The duplication ratio is the amount of the total assembly that is represented more than once. The more fragmented the assembly, the higher this value will be. See What makes an assembly bad section above to see further details.\n\n\nMisassemblies\nMissassemblies are when pieces in an assembly are overlapped in the incorrect way. In the MetaQUAST manual, they define missassemblies in the four following ways (taken from the manual):\nMisassemblies is the number of positions in the contigs (breakpoints) that satisfy one of the following criteria.\n\nthe left flanking sequence aligns over 1 kbp away from the right flanking sequence on the reference;\nflanking sequences overlap on more than 1 kbp;\nflanking sequences align to different strands or different chromosomes;\nflanking sequences align on different reference genomes (MetaQUAST only)\n\nThese can be caused by errors in the assembly, or they can be caused by structural variation in the sample, such as inversions, relocations and translocation.\n\n\nMismatches\nThis is where there are incorrect bases in the contigs making up the assembly. The summary gives you the total number of mismatches per 100kbp of sequence and short insertions and deletions (indels) per 100kbp.\n\n\nStatistics without reference\nThe statistics without a reference are based on the full assembly rather than comparing the assembly to the species that you have asked it to compare to. The largest contig is an indicator of how good the overall assembly is. If the largest contig is small, it means that the rest of the assembly is of a poor quality.\nThere is a detailed description of each of the outputs in the MetaQUAST manual which you can use to understand the output more fully.\n\n\n\n\n\n\nComparing assemblies using MetaQUAST\n\n\n\nUsing the above image how has the iterative polishing from assembly &gt; consensus &gt; pilon improved the assembly?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn all three assemblies there is a low duplication ratio and this decreases with each round of polishing. Completeness of the assembly increases with polishing, as does largest alignment. In these areas polishing has improved the assembly. However, the number of mismatches and misassemblies increased with polishing and the size of the largest contig decreased. These conflicting statistics illustrate the difficulty in assessing the quality of an assembly.",
    "crumbs": [
      "Home",
      "Polishing",
      "QC polished assembly"
    ]
  },
  {
    "objectID": "docs/lesson05-binning-functional-annotation/03-Functional-annotation.html",
    "href": "docs/lesson05-binning-functional-annotation/03-Functional-annotation.html",
    "title": "Functional annotation",
    "section": "",
    "text": "Now we have our binned MAGs, we can start to think about what functions genes contained within their genomes do. We can do this via functional annotation - a way to collect information about and describe a DNA sequence.\nNext lesson we will talk about taxonomic annotation, which tells us which organisms are present in the metagenome assembly. This lesson, however, we will do some brief functional annotation to get more information about the potential metabolic capacity of the organism we are annotating. This is possible because there is software available which uses features in DNA sequences to predict where genes start and end, allowing us to predict which genes are in our MAGs.\nA high quality functional annotation is important because it is very useful for lots of downstream analyses. For instance, if we were looking for genes that have a particular function, we would only be able to do that if we were able to predict the location of the genes in these assemblies.\nFor example, the paper this data is pulled from uses functional annotation of MAGs to look for genes associated with denitrification pathways. The abundance of these genes is then linked to N2O flux rates at different sites.\nIn this lesson we will only be doing a very small amount of functional annotation using the tool Prokka for rapid prokaryotic genome annotation. This is intended as a taster to give you an idea what you can use your MAGs for. There are many other routes to be taken regarding functional annotation, some of which will be discussed briefly at the end of this episode.\nAs with taxonomic annotation, effectiveness is determined by the database that the MAG sequence is being compared to. If you do not use the appropriate database you may not end up with many annotated sequences. In particular, Prokka (the tool we will use in this episode) annotates archaea and bacterial genomes. If you are trying to annotate a fungal genome or a eukaryote, you will need to use something different.",
    "crumbs": [
      "Home",
      "Binning & Functional Annotation",
      "Functional annotation"
    ]
  },
  {
    "objectID": "docs/lesson05-binning-functional-annotation/03-Functional-annotation.html#what-is-functional-annotation",
    "href": "docs/lesson05-binning-functional-annotation/03-Functional-annotation.html#what-is-functional-annotation",
    "title": "Functional annotation",
    "section": "",
    "text": "Now we have our binned MAGs, we can start to think about what functions genes contained within their genomes do. We can do this via functional annotation - a way to collect information about and describe a DNA sequence.\nNext lesson we will talk about taxonomic annotation, which tells us which organisms are present in the metagenome assembly. This lesson, however, we will do some brief functional annotation to get more information about the potential metabolic capacity of the organism we are annotating. This is possible because there is software available which uses features in DNA sequences to predict where genes start and end, allowing us to predict which genes are in our MAGs.\nA high quality functional annotation is important because it is very useful for lots of downstream analyses. For instance, if we were looking for genes that have a particular function, we would only be able to do that if we were able to predict the location of the genes in these assemblies.\nFor example, the paper this data is pulled from uses functional annotation of MAGs to look for genes associated with denitrification pathways. The abundance of these genes is then linked to N2O flux rates at different sites.\nIn this lesson we will only be doing a very small amount of functional annotation using the tool Prokka for rapid prokaryotic genome annotation. This is intended as a taster to give you an idea what you can use your MAGs for. There are many other routes to be taken regarding functional annotation, some of which will be discussed briefly at the end of this episode.\nAs with taxonomic annotation, effectiveness is determined by the database that the MAG sequence is being compared to. If you do not use the appropriate database you may not end up with many annotated sequences. In particular, Prokka (the tool we will use in this episode) annotates archaea and bacterial genomes. If you are trying to annotate a fungal genome or a eukaryote, you will need to use something different.",
    "crumbs": [
      "Home",
      "Binning & Functional Annotation",
      "Functional annotation"
    ]
  },
  {
    "objectID": "docs/lesson05-binning-functional-annotation/03-Functional-annotation.html#using-prokka-for-functional-annotation",
    "href": "docs/lesson05-binning-functional-annotation/03-Functional-annotation.html#using-prokka-for-functional-annotation",
    "title": "Functional annotation",
    "section": "Using Prokka for functional annotation",
    "text": "Using Prokka for functional annotation\n\n\n\n\n\n\nSoftware choices\n\n\n\nWe are using Prokka here as it is still the software most commonly used. However, the program is no longer being updated. One recent alternative that is being actively developed is Bakta.\n\n\nProkka identifies candidate genes in a iterative process. First it uses Prodigal (another command line tool) to find candidate genes.These are then compared against databases of known protein sequences in order to determine their function. If you like, you can read more about Prokka in this 2014 paper.\nProkka has been pre-installed on our instance. First, let’s create a directory inside results where we can store our outputs from Prokka.\n\n\nCode\n\ncd ~/cs_course\nmkdir results/prokka\n\nFor now we will annotate just one MAG at a time with Prokka. In the previous episode we produced 90 MAGs of varying quality. In this example, we will start with the MAG bin.45.fa, as this MAG had fairly high completeness (57.76%) and only 1.72% contamination. You should choose your own “best” bin to use (one with good completeness and low contamination).\nBefore we start we’ll need to activate a conda environment to run the software.\n\nActivating an environment\nEnvironments are a way of installing a piece of software so that it is isolated, so that things installed within an environment, do not affect other software installed at system wide level. For some pieces of software, the requirements for different dependency versions, such different versions of python mean this is an easy way to have multiple pieces of software installed without conflicts. One popular way to manage environments is to use conda which is a popular environment manager. We will not discuss using conda in detail, so for further information of how to use it, here is a Carpentries course that covers how to use conda in more detail.\nFor this course we have created a conda environment containing prokka. In order to use this we will need to use the conda activate command:\n\n\nCode\n\nconda activate prokka\n\nYou will be able to tell you have activated your environment because your prompt should go from looking like this, with (base) at the beginning…\n\n\nCode\n\n(base) csuser@instance001:~ $\n\n…to having (prokka) at the beginning. If you forget whether you are in an the prokka environment, look back to see what the prompt looks like.\n\n\nCode\n\n(prokka) csuser@instance001:~ $\n\nNow let’s take a look at the help page for Prokka using the -h flag.\n\n\nCode\n\nprokka -h\n\n\n\n\n\n\n\nOutput — Prokka Help documentation\n\n\n\n\n\nName:\n  Prokka 1.12 by Torsten Seemann &lt;torsten.seemann@gmail.com&gt;\nSynopsis:\n  rapid bacterial genome annotation\nUsage:\n  prokka [options] &lt;contigs.fasta&gt;General:\n  --help            This help\n  --version         Print version and exit\n  --docs            Show full manual/documentation\n  --citation        Print citation for referencing Prokka\n  --quiet           No screen output (default OFF)\n  --debug           Debug mode: keep all temporary files (default OFF)\nSetup:\n  --listdb          List all configured databases\n  --setupdb         Index all installed databases\n  --cleandb         Remove all database indices\n  --depends         List all software dependencies\nOutputs:\n --outdir [X]      Output folder [auto] (default '')\n --force           Force overwriting existing output folder (default OFF)\n --prefix [X]      Filename output prefix [auto] (default '')\n --addgenes        Add 'gene' features for each 'CDS' feature (default OFF)\n --addmrna         Add 'mRNA' features for each 'CDS' feature (default OFF)\n --locustag [X]    Locus tag prefix [auto] (default '')\n --increment [N]   Locus tag counter increment (default '1')\n --gffver [N]      GFF version (default '3')\n --compliant       Force Genbank/ENA/DDJB compliance: --addgenes --mincontiglen 200 --centre XXX (default OFF)\n --centre [X]      Sequencing centre ID. (default '')\n --accver [N]      Version to put in Genbank file (default '1')\nOrganism details:\n  --genus [X]       Genus name (default 'Genus')\n  --species [X]     Species name (default 'species')\n  --strain [X]      Strain name (default 'strain')\n  --plasmid [X]     Plasmid name or identifier (default '')\n Annotations:\n  --kingdom [X]     Annotation mode: Archaea|Bacteria|Mitochondria|Viruses (default 'Bacteria')\n  --gcode [N]       Genetic code / Translation table (set if --kingdom is set) (default '0')\n  --gram [X]        Gram: -/neg +/pos (default '')\n  --usegenus        Use genus-specific BLAST databases (needs --genus) (default OFF)\n  --proteins [X]    FASTA or GBK file to use as 1st priority (default '')\n  --hmms [X]        Trusted HMM to first annotate from (default '')\n  --metagenome      Improve gene predictions for highly fragmented genomes (default OFF)\n  --rawproduct      Do not clean up /product annotation (default OFF)\n  --cdsrnaolap      Allow [tr]RNA to overlap CDS (default OFF)\nComputation:\n  --cpus [N]        Number of CPUs to use [0=all] (default '8')\n  --fast            Fast mode - only use basic BLASTP databases (default OFF)\n  --noanno          For CDS just set /product=\"unannotated protein\" (default OFF)\n  --mincontiglen [N] Minimum contig size [NCBI needs 200] (default '1')\n  --evalue [n.n]    Similarity e-value cut-off (default '1e-06')\n  --rfam            Enable searching for ncRNAs with Infernal+Rfam (SLOW!) (default '0')\n  --norrna          Don't run rRNA search (default OFF)\n  --notrna          Don't run tRNA search (default OFF)\n  --rnammer         Prefer RNAmmer over Barrnap for rRNA prediction (default OFF)\n\n\n\nLooking at the help page tells us how to construct our basic command, which has these arguments: - --outdir tells Prokka where to put the output - --prefix tells Prokka what to call the outputs (in this case, the name of the bin will suffice) - finally we need to provide the file to be annotated\nProkka produces multiple different file types, which you can see in the table below. We are mainly interested in .faa, .ffn and .tsv but many of the other files are useful for submission to different databases.\n\n\n\n\n\n\n\nSuffix\nDescription of file contents\n\n\n\n\n.fna\nFASTA file of original input contigs (nucleotide)\n\n\n.faa\nFASTA file of translated coding genes (protein)\n\n\n.ffn\nFASTA file of all genomic features (nucleotide)\n\n\n.fsa\nContig sequences for submission (nucleotide)\n\n\n.tbl\nFeature table for submission\n\n\n.sqn\nSequin editable file for submission\n\n\n.gbk\nGenbank file containing sequences and annotations\n\n\n.gff\nGFF v3 file containing sequences and annotations\n\n\n.log\nLog file of Prokka processing output\n\n\n.txt\nAnnotation summary statistics\n\n\n.tsv\nTab-separated file of all features: locus_tag,ftype,len_bp,gene,EC_number,COG,product\n\n\n\n\n\nCode\n\nprokka --outdir results/prokka/bin.45 --prefix bin.45 results/binning/assembly_ERR5000342.fasta.metabat-bins1500-YYYMMDD_HHMMSS/bin.45.fa\n\nThis should take around 1-2 minutes on the instance so we will not be running the command in the background.\n\n\n\n\n\n\nExercise 1: Recap of Prokka command\n\n\n\nTest yourself! What do each of these parts of the command signal?\n\n--outdir bin.45\n--prefix bin.45\nresults/binning/assembly_ERR5000342.fasta.metabat-bins1500-YYYMMDD_HHMMSS/bin.45.fa\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbin.45 is the name of the directory where Prokka will place its output files\nbin.45 will be the name of each output file e.g. bin.45.tsv or bin.45.faa\nThis is the file path for the file we want Prokka to annotate\n\n\n\n\n\n\nWhen you initially run the command you should see similar to the following.\n\n\nOutput\n\n[11:58:55] This is prokka 1.12\n[11:58:55] Written by Torsten Seemann &lt;torsten.seemann@gmail.com&gt;\n[11:58:55] Homepage is https://github.com/tseemann/prokka\n[11:58:55] Local time is Wed Mar 22 11:58:55 2023\n[11:58:55] You are csuser\n[11:58:55] Operating system is linux\n[11:58:55] You have BioPerl 1.006924\n[11:58:55] System has 8 cores.\n[11:58:55] Will use maximum of 8 cores.\n[11:58:55] Annotating as &gt;&gt;&gt; Bacteria &lt;&lt;&lt;\n[11:58:55] Generating locus_tag from 'results/binning/assembly_ERR5000342.fasta.metabat-bins1500-YYYMMDD_HHMMSS/bin.45.fa' contents.\n\nAnd you should see the following when the command has finished:\n\n\nOutput\n\n[12:00:28] Output files:\n[12:00:28] bin.45/bin.45.fna\n[12:00:28] bin.45/bin.45.faa\n[12:00:28] bin.45/bin.45.ffn\n[12:00:28] bin.45/bin.45.fsa\n[12:00:28] bin.45/bin.45.err\n[12:00:28] bin.45/bin.45.sqn\n[12:00:28] bin.45/bin.45.txt\n[12:00:28] bin.45/bin.45.gbk\n[12:00:28] bin.45/bin.45.tsv\n[12:00:28] bin.45/bin.45.gff\n[12:00:28] bin.45/bin.45.log\n[12:00:28] bin.45/bin.45.tbl\n[12:00:28] Annotation finished successfully.\n[12:00:28] Walltime used: 1.55 minutes\n[12:00:28] If you use this result please cite the Prokka paper:\n[12:00:28] Seemann T (2014) Prokka: rapid prokaryotic genome annotation. Bioinformatics. 30(14):2068-9.\n[12:00:28] Type 'prokka --citation' for more details.\n[12:00:28] Share and enjoy!\n\nNow prokka has finished running, we can exit the conda environment using the conda deactivate command.\n\n\nCode\n\nconda deactivate\n\nYour prompt should return from something like this:\n\n\nCode\n\n(prokka) csuser@metagenomicsT3instance04:~ $ conda deactivate\n\nto this:\n\n\nCode\n\n(base) csuser@metagenomicsT3instance04:~ $\n\nIf we navigate into the bin.45 output file we can use ls to see that Prokka has generated many files.\n\n\nCode\n\ncd results/prokka/bin.45\nls\n\n\n\nOutput\n\nbin.45.err  bin.45.faa  bin.45.ffn  bin.45.fna  bin.45.fsa  bin.45.gbk  bin.45.gff  bin.45.log  bin.45.sqn  bin.45.tbl  bin.45.tsv  bin.45.txt\n\nAs mentioned previously, the two files we are most interested in are those with the extension .tsv and .faa:\n\nthe .tsv file contains information about every gene identified by Prokka\nthe .faa file is a FASTA file containing the amino acid sequence of every gene that has been identified.\n\nWe can take a look at the .tsv file using head.\n\n\nCode\n\nhead bin.45.tsv\n\n\n\nOutput\n\nlocus_tag       ftype   gene    EC_number       product\nDDJNKIGN_00001  CDS     hypothetical protein\nDDJNKIGN_00002  CDS     hypothetical protein\nDDJNKIGN_00003  CDS     macA    Macrolide export protein MacA\nDDJNKIGN_00004  CDS     hypothetical protein\nDDJNKIGN_00005  CDS     hypothetical protein\nDDJNKIGN_00006  CDS     hypothetical protein\nDDJNKIGN_00007  CDS     hypothetical protein\nDDJNKIGN_00008  CDS     pstA_1  Phosphate transport system permease protein PstA\nDDJNKIGN_00009  CDS     pstA_2  Phosphate transport system permease protein PstA\n\nThis file gives us a list of all the sequences that Prokka has identified as being protein-coding, along with the gene name (if there is one) and the protein product (again, if there is one).\nYou will notice that some of the output are labelled simply “hypothetical protein”. This means the locus in questions looks like a protein-coding gene, but there isn’t a match for it in any of the databases used by Prokka to label genes.\nOthers have a gene and product name, meaning Prokka was able to successfully identify them as a specific gene. The product column tells you the name of the protein this gene codes for.\nWe can then look at the .faa file to see the sequences of these proteins.\n\n\nCode\n\nhead bin.45.faa\n\n\n\nOutput\n\n&gt;DDJNKIGN_00001 hypothetical protein\nMTSSTVINTLVAAQTPILKQNLRPVSVWLHHCGLGGVQASWIQFRDSLRQAIIDALSAAG\nMTDCMNELKYRWGL\n&gt;DDJNKIGN_00002 hypothetical protein\nMQPRPGIPFAGALVPLSTFNKTALRSNSIDLTNPPQLEPFTRREQYRIVVSGDEPDCDDT\nLELPVWDCDLIRKCYEVSYHKARLDYYGPAAPFSPKDMTSFRGSSRQCWERTERLRSAGC\nTTSRPINCLRQILNVSWTKNMSAVLAGGLLQGLRPEPQLDPAWAAFFALPDIEITSLRST\nGTSSPDRTRSRKRTPSAESRRPWRCHGPQPVLPG\n&gt;DDJNKIGN_00003 Macrolide export protein MacA\nMTSKHIGMVAGAMAFIAAGVGCARSRTAAAGDERPAVSVVKIARGDLSQGLTLAAEFRPF",
    "crumbs": [
      "Home",
      "Binning & Functional Annotation",
      "Functional annotation"
    ]
  },
  {
    "objectID": "docs/lesson05-binning-functional-annotation/03-Functional-annotation.html#what-next",
    "href": "docs/lesson05-binning-functional-annotation/03-Functional-annotation.html#what-next",
    "title": "Functional annotation",
    "section": "What next?",
    "text": "What next?\nNow we have information about the various genes (and the proteins they code for) present in one of our bins. What can we do with this information?\n\nRelating genes to an online database\nThere are tools available which allow you to visualise the proteins in your bin and how they fit into different metabolic pathways. Some of these are available through your browser.\nOne such tool is BlastKOALA, where you can upload the .faa file we just looked at and get back a breakdown of the proteins mapped to the KEGG database (a database of molecular interaction maps). Start by downloading the .faa file to your local maching using scp:\n\n\nCode\n\nscp -i login-key-instanceNNN.pem csuser@instanceNNN.cloud-span.aws.york.ac.uk:~/cs_course/results/prokka/bin.45/bin.45.faa\n\nYou can then upload this file onto BlastKOALA. BlastKOALA is a tool which can annotate the sequences with K numbers. These then relate back to the KEGG database.\n\n\n\n.\n\n\nYou should click on the “Choose file” button and navigate to where your *.faa file is located on your computer.\nChoose the Prokaryotes database as the one to search - you can run it again with eukaryotes and/or viruses if you like later. However, we’re mostly expecting to see prokaryotes so this will be the most useful output.\nOnce you have pressed submit you should be re-directed to a screen that says “Request accepted”. You will also be sent an email with two links, one to submit the job and one to cancel. Make sure you press the submit link as your job will not be running without it! If you haven’t received an email, check your spam.\nOnce you have pressed the submit link in the email you should be redirected to a BlastKOALA page that says “Job submitted”. This is an online server shared by lots of people, so your job has to queue with other jobs before it can be executed. This may take a while. You will recieve an email when the job has completed.\nOnce the job has been completed you will receive a link by email. From this you can explore the annotated MAG. You can view/download the data and use the KEGG mapper reconstruct pathway option to see how these genes interact with each other.\n\n\n\n.\n\n\nUsing an annotation tool like this can help you understand more about the genes and pathways present in your sample(s). For example, as previously described, the paper this data is pulled from uses functional annotation of MAGs to look for genes associated with denitrification pathways.\n\n\nBuilding a tree from the 16S sequence\nProkka is able to identify 16S sequences present in our MAGs. This can be used to build a quick taxonomic tree to see what organisms our MAG is related to.\nFirst we will search for the presence of 16S sequences in the Prokka output.\nWhile still logged into the instance, navigate to the Prokka output directory you generated earlier (~/cs_course/results/prokka/bin.45). Once in that directory we can search for sequenced identified as being 16S in the .tsv file using grep:\n\n\nCode\n\ngrep 16S *.tsv\n\nYou should get a result similar to the below. Yours may differ slightly depending on the MAG you ran.\n\n\nOutput\n\nNBEAANKK_00310  CDS     yrrK    3.1.-.- Putative pre-16S rRNA nuclease\nNBEAANKK_02181  rRNA    16S ribosomal RNA\n\n\n\n\n\n\n\nNote\n\n\n\nIf you don’t get an output here it may be that your MAG doesn’t have any 16S sequences present. In the case of this lesson, that means you have run Prokka on a less complete MAG than the one we used. You should double check your output from CheckM and pick a MAG that is highly complete to run through Prokka instead.\n\n\nOur output shows that there is 1 full size 16S ribosomal RNA genes present in our data, and a putative rRNA nuclease (which we can discount as it isn’t an rRNA).\nThe next step is to pull out the sequence of one of these 16S rRNA genes and run it through a BLAST database. This is possible using the .ffn file which gives the sequences in nucleotide format. We’ll need to search the .ffn file for the tag associated with our gene of interest (the first column in the output above).\nWe do this using seqkit with a grep option, which you can read more about here). Here’s the format of this command:\n\n\nCode\n\nseqkit grep -p &lt;prokka_id&gt; &lt;prokka.ffn&gt;\n\nSo in our case this command would be:\n\n\nCode\n\n$ seqkit grep -p NBEAANKK_02181 bin.45.ffn\n\nYou can see the output of the command below:\n\n\nOutput\n\n&gt;NBEAANKK_02181 16S ribosomal RNA\nACGAGAGTTTGATCCTGGCTCAGAGCGAACGCTGGCGGCGTGCTTAACACATGCAAGTCG\nAGTGCGCGCCTGTAGCAATACAGGTGGCGCACGGCGCACGGGTGCGTAACACGTGGGTAA\nTCTGCCCTTCGATGGGGAATAACTCGCCGAAAGGCGAGCTAATTCCGCATAACATTCCGA\nGAACTTTGGTTTTTGGATTCAAAGGCGTAAGTCGTCGGAGGAGGAGCCCGCGCACGATTA\nGCTAGTTGGTGAGGTAACGGCTCACCAAGGCTATGATCGTTAGCTGGTCTGAGAGGATGG\nCCAGCCACACTGGAACTGAGACACGGTCCAGACTCCTACGGGAGGCAGCAGTGGGGAATC\nTTGCACAATGGGCGAAAGCCTGATGCAGCGACGCCGCGTGGGGGATGAAGCTTTTCGGAG\nTGTAAACCCCTTTCGACCCGGACGAATGCCTCGCAAGAGGACTGACGGTACGGGTATAAG\nAAGCCCCGGCTAACTACGTGCCAGCAGCCGCGGTAAGACGTAGGGGGCCAGCGTTGCTCG\nGAATTACTGGGTGTAAAGGGTTCGTAGGCGGTGTGGCAAGTCGGGAGTGAAATCTCTGGG\nCTCAACCCAGAGGCTGCTTCCGAAACTGCTGTGCTTGAGTGTGGGAGAGGCGCGTGGAAT\nTGCAGGTGTAGCGGTGAAATGCGTAGATATCTGCAGGAACACCCGTGGCGAAAGCGGCGC\nGCTGGACCACAACTGACGCTGAGGAACGAAAGCTAGGGGAGCAAACAGGATTAGATACCC\nTGGTAGTCCTAGCCCTAAACGATCAGGACTTGGGGTGCCGCCCGTTCGGGCGTCGTCCCG\nGAGCTAACGCGTTAAGTCCTGCACCTGGGGAGTACGGTCGCAAGACTGAAACTCAAAGGA\nATTGACGGGGGCCCGCACAAGCGGTGGAACATGTGGTTCAATTCGACGCTACGCGAGGAA\nCCTTACCTGGGCTCGAAATGCTTATGACCAGCTGTAGAAATACGGCCTTCCCGCAAGGGA\nCAGGAGTATAGGCGCTGCATGGCTGTCGTCAGCTCGTGCCGTGAGGTGTTGGGTTAAGTC\nCCGCAACGAGCGCAACCCCTGCACGTAGTTGCCACTCCGCAAGGAGGGAACTCTACGTGG\nACTGCTCCGGATAACGGAGAGGAAGGTGGGGATGACGTCAAGTCCGCATGGCCTTTATGT\nCCAGGGCTACACACGTGTTACAATGCAGGGTACAAACCGTTGCCAACCCGCGAGGGGGAG\nCTAATCGGAAAAAACTCTGCTCAGTTCGGATTGCAGTCTGCAACTCGACTGCATGAAGCC\nGGAATCGCTAGTAATGGCGTATCAGATCGACGCCGTGAATACGTTCCCGGGCCTTGTACA\nCACCGCCCGTCACATCACGAAAGTGAGTTGTACTAGAAGTCGTCACGCTGACCGCAAGGA\nGGCAGACGCCCAAGGTATGACCCATGATTGGGGTGAAGTCGTAACAAGGTAGCCGTAGGA\nGAACCTGCGGCTGGATCACCTCCTTT\n\nNow we have the 16S rRNA sequence we can upload this to BLAST and search the 16S database to see what organisms this MAG relates to.\n\nBLAST\nWe will be using BLAST (Basic Local Alignment Search Tool) which is an algorithm to find regions of similarity between biological sequences. BLAST is a very popular program in bioinformatics so you may be familiar with the online BLAST server run by NCBI.\nWe will be using the online server, available at BLAST.\n\n\n\n.\n\n\nSelect the button that says “Nucleotide BLAST”.\nUnder “Choose Search Set” set the database to “rRNA/ITS database”. Then, copy and paste the 16S sequence from your instance into the box at the top titled “Enter accession number(s), gi(s), or FASTA sequence(s)”\nYour screen should look something like the below:\n\n\n\n.\n\n\nNow, click the blue BLAST button!\nYour job will then be added to a queue of other jobs until there is space for it to run. Usually this is only a couple of minutes, especially as the sequence length and database size we are using are small. Make sure you leave the tab open while you wait so you can see your results when they arrive.\nYour ouput should look like this:\n\n\n\n.\n\n\nFrom here you can explore the sequences that were aligned to your 16S sequence using the “Descriptions”, “Graphic Summary”, “Alignments” and “Taxonomy” tabs. You can also browse the “Distance tree of results” to see where your 16S sequence lies in relation to other species.\n\n\n\n\n\n\nExercise 2: Understand the blast output\n\n\n\nYou will now have a 16S sequence from the MAG that you have chosen. Use the output from your BLAST search to answer these questions.\n\nWhat do you think is the most likely annotation for your MAG? ?\nWhich columns in the BLAST output do you think are the most important for selecting which is the best hit?\nNow you have identified this MAG, try repeating the process for the other bins and see which organisms they belong to. Which are the best hits for these MAGs?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThis will vary depending on the MAG you have picked but it will be one of the first hits in the output. The “closest” match will probably be the one with the highest total score. However these are not the only columns worth using to identify the best hit.\nOther columns worth looking at (because the top hit may not be the best) are the query cover, percent identity and the E-value.\n\npercent identity is how similar the query sequence (your input) is to the target AKA how many characters are identical. Higher percent identity = more similar sequences\nquery coverage is the percentage of the query sequence that overlaps the target. If there is only a small overlap then the match is less significant, even if it has a very high percent identity. We want as much of the two sequences to be identical as possible.\nE-value is the number of matches you would expect to see by chance. This is dependent on the size of the database. Lower E-value = less likely to be by chance = a better match.",
    "crumbs": [
      "Home",
      "Binning & Functional Annotation",
      "Functional annotation"
    ]
  },
  {
    "objectID": "docs/lesson05-binning-functional-annotation/index.html",
    "href": "docs/lesson05-binning-functional-annotation/index.html",
    "title": "Binning and Functional Annotation",
    "section": "",
    "text": "In this lesson we will finish off our analysis by separating out the individual genomes into metagenome-assembled genomes (MAGs) using a process called binning. Binning can be done in lots of different ways but the general idea is to put all the “similar” contigs together into one bin. By the end we should have several bins each containing the genome of one organism - this is what we call a MAG.\n\n\n\n\n.\n\n\nOnce we have our MAGs we can annotate them to predict where genes are and what function they have. This is useful for predicting the metabolic capacity of each organism. We can also look up these predicted gene sequences in a database and build taxonomic trees to make an educated guess about what species we are looking at.\nBy the end of this lesson you will be able to:\n\nprepare your polished assembly for binning using BWA and Samtools\ngenerate bins using MetaBAT2\nevaluate the quality of your generated bins/MAGs using CheckM\nuse Prokka for functional annotation of your MAGs\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Binning & Functional Annotation"
    ]
  }
]